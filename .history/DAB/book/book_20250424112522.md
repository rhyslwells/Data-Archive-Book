# Preface {#preface}

Remember the Data Archive is non‑linear — use internal links for navigation.
If you need a specific page see the **Table of Contents** to jump to a topic.



# 1 On 1 Template {#1-on-1-template}

- Decisions
    - [Your name] _add decisions that need to be made_
    - [Other person's name] _add decisions that need to be made_
        
- Action items
    - [Your name] _add next steps from the discussion_
    - [Other person's name] _add next steps from the discussion_
        
- Topics to discuss (bi-directional)
    - [Your name] _add topics or questions to discuss together_
    - [Other person's name] _add topics or questions to discuss together_
        
- Updates (uni-directional - no action needed)
    - [Your name] _add updates with no action needed_
    - [Other person's name] _add updates with no action needed_

# Ab Testing {#ab-testing}

A/B testing is a method of performance testing two versions of a product like an app.

# Accessing Gen Ai Generated Content {#accessing-gen-ai-generated-content}


To assess whether the content generated by a [Generative AI](#generative-ai) is truthful and faithful, several methods and frameworks can be employed. Truthfulness refers to whether the generated content <mark>is factually correct</mark>, while faithfulness refers to whether it <mark>accurately</mark> reflects the input data or prompt.

[interpretability](#interpretability)
### 1. Frameworks for Truthfulness and Faithfulness

   - Subject Matter Expert (SME) Reviews: One of the most reliable methods for verifying truthfulness and faithfulness is through SME validation. SMEs can manually check the content to ensure it aligns with domain-specific knowledge and is factually accurate.
   
   - [Knowledge Graph](#knowledge-graph) and External Data: Generative AI models can be linked to external sources of truth, such as knowledge graphs, databases, or other verified resources. This allows the system to cross-check facts and improve the truthfulness of the content.
   
   - Retrieval-Augmented Generation ([RAG](#rag)): This framework involves retrieving relevant information from trusted sources before generating content. It helps ensure that the AI is providing up-to-date, reliable, and contextually accurate responses.
   
   - Evaluation Metrics: Some metrics can be used to evaluate faithfulness:
     - Factual Consistency Metrics: Tools such as [BERTScore](#bertscore) or FactCC can compare generated text with reference text or factual databases to check for consistency.
     - Human Evaluation: In certain contexts, human evaluators rate the content on aspects of truthfulness and faithfulness. This can be part of quality assurance processes.
     

   - Cross-Referencing Data: AI-generated content should be cross-referenced with existing, credible sources to confirm its accuracy. For example, if the AI makes a historical claim or provides statistical data, those facts should be verifiable through known data repositories.

   - Fact-Checking Tools: Using automated fact-checking tools or models trained to detect false information can provide another layer of defence against untruthful content.

# Accuracy {#accuracy}


## Definition

- Accuracy Score is the proportion of correct predictions out of all predictions made. In other words, it is the percentage of correct predictions.
- Accuracy can have issues with [Imbalanced Datasets](#imbalanced-datasets)where there is more of one class than another.

## Formula

- The formula for accuracy is:
  $$\text{Accuracy} = \frac{TN + TP}{\text{Total}}$$
In the context of [Classification](#classification) problems, particularly binary classification, TN and TP are components of the confusion matrix:

- TP (True Positive): The number of instances that are correctly predicted as the positive class. For example, if the model predicts a positive outcome and it is indeed positive, it counts as a true positive.
- TN (True Negative): The number of instances that are correctly predicted as the negative class. For example, if the model predicts a negative outcome and it is indeed negative, it counts as a true negative.

The [Confusion Matrix](#confusion-matrix) also includes:

- FP (False Positive): The number of instances that are incorrectly predicted as the positive class. This is also known as a "Type I error."
- FN (False Negative): The number of instances that are incorrectly predicted as the negative class. This is also known as a "Type II error."

These metrics are used to evaluate the performance of a classification model, providing insights into not just accuracy but also precision, recall, and other performance measures.
## Exploring Accuracy in Python

To explore accuracy in Python, you can use libraries such as `scikit-learn`, which provides the `accuracy_score` function. This function compares the predicted labels with the true labels and calculates the accuracy.

### Example Usage

```python
from sklearn.metrics import accuracy_score
# Assuming pred and y_test are defined
accuracy = accuracy_score(y_test, pred)
print("Prediction accuracy: {:.2f}%".format(accuracy  100.0))
```

- Make sure to replace `pred` and `y_test` with your actual prediction and test data variables.

# Acid Transaction {#acid-transaction}


An ACID [Transaction](#transaction) ensures that either all changes are successfully committed or rolled back, preventing the database from ending up in an inconsistent state. This guarantees the integrity of the data throughout the transaction process.

### Key Properties of ACID Transactions

1. Atomicity: This property ensures that transactions are treated as a single, indivisible unit. If any part of the transaction fails, the entire transaction is rolled back, and none of the changes are applied. Users do not see intermediate states of the transaction.

2. Consistency: Transactions must leave the database in a valid state, adhering to all defined constraints. If a transaction violates a constraint, it is rolled back to maintain the database's stable state.

3. Isolation: This property ensures that concurrent transactions do not interfere with each other. Each transaction operates independently, and the results of one transaction are not visible to others until it is committed.

4. Durability: Once a transaction has been committed, the changes are permanent, even in the event of a system failure. The data remains intact and recoverable.



# Activation Atlases {#activation-atlases}


is a viewing method for high dimensional space that AI system use for predictions.

Example AlexNet (cofounder of OpenAI)

# Activation Function {#activation-function}


Activation functions play a role in [Neural network](#neural-network) by introducing non-linearity, allowing models to learn from complex patterns and relationships in the data.

[How do we choose the right Activation Function](#how-do-we-choose-the-right-activation-function)
### Key Uses of Activation Functions:

1. Non-linearity: Without activation functions, neural networks would behave as linear models, unable to capture complex, non-linear patterns in the data
2. [Data transformation](#data-transformation): Activation functions modify input signals from one layer to another, helping the model focus on important information while ignoring irrelevant data,
3. [Backpropagation](#backpropagation): They enable gradient-based optimization by making the network differentiable, essential for efficient learning.

### Purpose of Typical Activation Functions

Linear: Outputs a continuous value, suitable for regression.

ReLU (Rectified Linear Unit): 
  - Purpose: ReLU is used to introduce non-linearity by turning neurons "on" or "off." It outputs the input directly if it is positive; otherwise, it outputs zero. This helps in efficiently training deep networks by mitigating the vanishing gradient problem.
  - Function:$f(x) = \max(0, x)$

Sigmoid:
  - Purpose: Sigmoid is used primarily in [Binary Classification](#binary-classification) tasks. It squashes input values to a range between 0 and 1, making it suitable for representing probabilities.
  - Function:$f(x) = \frac{1}{1 + e^{-x}}$

 Tanh:
  - Purpose: Tanh is similar to the sigmoid function but outputs values in the range of -1 to 1. This zero-centered output can be beneficial for optimization in certain scenarios.
  - Function:$f(x) = \tanh(x)$

Softmax:
  - Purpose: Softmax is used in multi-class classification tasks. It converts a vector of raw scores (logits) into a probability distribution, where each value is between 0 and 1, and the sum of all values is 1. This allows the outputs to be interpreted as probabilities, with larger inputs corresponding to larger output probabilities.
  - Application: In both softmax regression and neural networks with softmax outputs, a vector$\mathbf{z}$is generated by a linear function and then passed through the softmax function to produce a probability distribution. This enables the selection of one output as the predicted category.

The softmax function converts a vector of raw scores (logits) into a probability distribution. The formula for the softmax function for a vector $\mathbf{z} = [z_1, z_2, \ldots, z_N]$is given by:

$$\sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{N} e^{z_j}}$$

This ensures that the output values are between 0 and 1 and that they sum to 1, making them [interpretability|interpretable](#interpretabilityinterpretable) as probabilities.






# Active Learning {#active-learning}


Think captchas for training.
  
To help the [Supervised Learning](#supervised-learning) models when they are less confident.  
  
Reducing labelling time or need for it.


# Ada Boosting {#ada-boosting}


Resources:
[LINK](https://www.youtube.com/watch?v=LsK-xG1cLYA)
# Overview:

Ada Boosting short for <mark>Adaptive Boosting</mark>, is a specific type of [Boosting](#boosting) algorithm that focuses on improving the accuracy of predictions by <mark>combining multiple weak learners</mark> into a strong learner. It is particularly known for its <mark>simplicity</mark> and effectiveness in classification tasks.

### How AdaBoost Works:

1. **Base Learners**: In AdaBoost, the base learners are typically low-depth trees, also known as <mark>stumps</mark>. These are simple models that perform slightly better than random guessing.

2. **Sequential Training**: AdaBoost trains these stumps sequentially. Each stump is trained to correct the errors made by the previous stumps. This sequential approach ensures that each new model focuses on the data points that were misclassified by earlier models.

3. **Weighting**: After each stump is trained, <mark>AdaBoost assigns a weight to it based on its accuracy</mark>. More accurate stumps receive higher weights, giving them more influence in the final prediction.

4. **Error Focus**: The algorithm <mark>increases the weights of the misclassified data points</mark>, making them more prominent in the training of the next stump. This ensures that subsequent models pay more attention to the difficult-to-classify instances.

5. **Final Prediction**: The final prediction is a weighted sum of the predictions from all the stumps. The stumps with higher accuracy have more say in the final classification.

# Further Understanding

### Creating a Forest with AdaBoost:

To create a forest using AdaBoost, you start with a [Decision Tree](#decision-tree) or [Random Forests](#random-forests) approach, but instead of using full-sized trees, you use stumps. 

These stumps are trained sequentially, with each one focusing on the errors of the previous stumps. 

The final prediction is a weighted sum of the predictions from all the stumps, where more accurate stumps have more influence on the final outcome.

### Key Differences from [Random Forests](#random-forests):

- **Tree Depth**: In [Random Forests](#random-forests), full-sized trees are used, and each tree gets an equal say in the final prediction. In contrast, AdaBoost uses low-depth trees (stumps) and assigns different weights to each stump based on its accuracy.

- **Order and Sequence**: In AdaBoost, the order of the stumps is important because errors are passed on in sequence. In [Random Forests](#random-forests), trees are built independently and simultaneously.

### Advantages of AdaBoost:

- **Increased Accuracy**: By focusing on the errors of previous models, AdaBoost can significantly improve the accuracy of predictions.
- **Simplicity**: AdaBoost is relatively simple to implement and understand compared to other ensemble methods.
- **Flexibility**: It can be applied to various types of base models and is not limited to a specific algorithm.

### Challenges of AdaBoost:

- **Sensitivity to Noisy Data**: AdaBoost can be sensitive to noisy data and outliers, as it focuses heavily on correcting errors.
- **Complexity**: While simpler than some other boosting methods, AdaBoost can still be computationally intensive due to its sequential nature.

# Adam Optimizer {#adam-optimizer}


Adam (Adaptive Moment Estimation) is an advanced optimization algorithm that combines the benefits of both [Momentum](#momentum) and adaptive learning rates. It is widely used due to its efficiency and effectiveness in training [deep learning](#deep-learning) models.

Adam is particularly effective for large datasets and complex models, as it provides robust convergence and requires minimal tuning compared to other optimization algorithms. Its ability to <mark>dynamically adjust learning rates</mark> makes it a popular choice in the deep learning community.
#### Key Features of Adam:

**Adaptive Learning Rates:** Adam adjusts the [learning rate](#learning-rate) for each parameter individually, based on the first and second moments of the gradients. This allows for more precise updates and better convergence.

**Momentum and RMSProp Combination:** Adam incorporates the concept of momentum by using moving averages of the gradients (first moment) and the squared gradients <mark>(second moment)</mark>, similar to RMSProp.

**Parameter Update Rule:** The update rule involves computing biased estimates of the first and second moments, which are then corrected to provide unbiased estimates. These are used to update the parameters.

**[Hyperparameter](#hyperparameter):**
  - **Learning Rate (\($\alpha$\)):** Typically set to 0.001, but can be tuned for specific tasks.
  - **Beta1 and Beta2:** Control the decay rates for the moving averages of the first and second moments. Common values are 0.9 and 0.999, respectively.
  - **Epsilon (\($\epsilon$\)):** A small constant added for numerical stability, usually set to \(1 \times 10^{-8}\).

**Implementation Challenges:**
  - **Parameter Tuning:** Careful tuning of learning rate, beta1, and beta2 is essential for optimal performance.
  - **Numerical Stability:** Adjusting epsilon can help prevent division by zero and other numerical issues.  
## Related concepts
- [Gradient Descent](#gradient-descent)
- [Why does the Adam Optimizer converge](#why-does-the-adam-optimizer-converge)
- 

# Adaptive Learning Rates {#adaptive-learning-rates}

 [Adam Optimizer](#adam-optimizer)

Adaptive [learning rate](#learning-rate) adjust the learning rate for each parameter based on the estimates of the first and second moments of the gradients. Adam (short for Adaptive Moment Estimation) combines ideas from [Momentum](#momentum) and adaptive learning rates to help the optimization process.

### How to Add a Database to PostgreSQL  

### Using pgAdmin (GUI)  

1. Open pgAdmin and log in.  
2. In the Object Explorer, right-click Databases → Create → Database.  
3. Enter Database Name (e.g., `mydatabase`).  
4. Choose an Owner (optional).  
5. Click Save.  
### Using Python (`psycopg2`)  

If you're using Python (e.g., in a Jupyter Notebook), install the `psycopg2` package if needed:  

```python
!pip install psycopg2-binary
```

Then, run this script to create a PostgreSQL database:  

```python
import psycopg2

# Connect to the PostgreSQL server (default 'postgres' database) {#connect-to-the-postgresql-server-default-postgres-database}
conn = psycopg2.connect(
    dbname="postgres",  # Default DB to connect before creating a new one
    user="postgres",
    password="your_password",
    host="localhost"
)
conn.autocommit = True  # Required for CREATE DATABASE
cursor = conn.cursor()

# Create a new database
cursor.execute("CREATE DATABASE mydatabase;")

# Close connection
cursor.close()
conn.close()
print("Database 'mydatabase' created successfully!")
```

# Addressing Multicollinearity {#addressing-multicollinearity}

In [ML_Tools](#ml_tools) see: [Addressing_Multicollinearity.py](#addressing_multicollinearitypy)

Multicollinearity can impact the performance and [interpretability](#interpretability) of regression models by causing instability in coefficient estimates and complicating the analysis of variable significance. Techniques like PCA can help by transforming correlated variables into uncorrelated principal components, thereby improving model stability and interpretability.

[Principal Component Analysis](#principal-component-analysis) (PCA) is a [dimensionality reduction](#dimensionality-reduction) technique that can help address [multicollinearity](#multicollinearity) in regression models.

1. **Combining Correlated Variables**: PCA transforms the correlated independent variables into a set of uncorrelated variables called principal components. These components capture the majority of the variance in the data while reducing redundancy.

2. **Reducing Dimensionality**: By selecting a smaller number of principal components that explain most of the variance, PCA can simplify the model. This reduces the complexity and potential overfitting associated with having too many correlated predictors.

3. **Improving Model Stability**: By using principal components instead of the original correlated variables, the regression model can achieve greater stability and reliability in coefficient estimates, as the issues caused by multicollinearity are mitigated.

4. **Enhanced Interpretability**: While the principal components may not have a direct interpretation in terms of the original variables, they can still provide insights into the underlying structure of the data and the relationships among variables.
### Example Code

```python

# edit this to explore how to address multi collinusing pca 
from statsmodels.stats.outliers_influence import variance_inflation_factor
import pandas as pd

variables = data_cleaned['var1', 'var2', 'var3'](#var1-var2-var3)
vif = pd.DataFrame()
vif["VIF"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]
vif["features"] = variables.columns

# Drop feature with VIF > 10
data_no_multicollinearity = data_cleaned.drop(['Year'], axis=1)
```



# Addressing_Multicollinearity.Py {#addressing_multicollinearitypy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Regression/Addressing_Multicollinearity.py

# Adjusted R Squared {#adjusted-r-squared}


Adjusted R-squared is a [Regression Metrics](#regression-metrics)or assessing the quality of a regression model, <mark>especially when multiple predictors</mark> are involved. It helps ensure that the model remains [parsimonious](#parsimonious) while still providing a good fit to the data.

When evaluating a regression model, if you notice a <mark>large difference</mark> between [R squared](#r-squared) and adjusted R², it indicates that the additional predictors may not be improving the model's performance.

In such cases, it may be beneficial to drop those extra variables to simplify the model without sacrificing predictive power.

Key features:

1. **Penalty for Number of Predictors**:
   - Adjusted R² adjusts the R² value by penalizing the addition of <mark>unnecessary predictors</mark>. This means that if you add a variable that does not improve the model significantly, the adjusted R² will decrease, reflecting that the model may be overfitting.

2. **Comparison with R-squared**:
   - Adjusted R² is always less than or equal to R². While R² can artificially inflate with the addition of more predictors (even if they are not useful), adjusted R² provides a <mark>more reliable</mark> assessment of model fit by considering the number of predictors relative to the number of observations.

3. **Interpretation**:
   - Like R², adjusted R² values range from 0 to 1, where values closer to 1 indicate a better fit. However, a significant difference between R² and adjusted R² suggests that the model may be penalized for including extra variables that do not contribute meaningfully to the prediction.

4. **Formula**:
   $$R^2_{adj.} = 1 - (1 - R^2) \cdot \frac{n - 1}{n - p - 1}$$
   - Where:
     - $R^2$ = R-squared value
     - $n$ = number of observations
     - $p$ = number of predictors (independent variables)




# Agent Based Modelling {#agent-based-modelling}


(ABM) is a computational approach that simulates the interactions of individual agents within a defined environment to observe complex phenomena and [emergent behavior](#emergent-behavior) at a system level. 

Agent-based modeling provides a robust framework for understanding and analyzing complex systems, particularly in the energy sector. 

By simulating individual agents and their interactions, researchers and practitioners can gain insights into system dynamics, evaluate [Policy|policies](#policypolicies), and optimize strategies for energy production and consumption.
### Principles of Agent-Based Modelling

1. **Agents**: The primary components of ABM, agents can represent individuals, groups, or entities with defined behaviors and attributes. Each agent operates based on its rules and interactions with other agents and the environment.

2. **Environment**: The space in which agents operate, which can be a physical or abstract setting. The environment influences agent behavior and can include various elements like resources, obstacles, or rules governing interactions.

3. **Interactions**: Agents communicate and interact with each other and their environment. These interactions can be cooperative, competitive, or neutral, leading to complex system dynamics.

4. **Emergence**: ABM focuses on emergent phenomena, where the collective behavior of agents leads to unexpected outcomes not evident from examining individual agents alone. This principle helps understand complex systems' dynamics and behaviors.

### Techniques in Agent-Based Modeling

1. **Model Development**: 
   - **Define Agents**: Specify agent types, behaviors, attributes, and decision-making processes.
   - **Environment Design**: Create a representation of the environment, including spatial aspects and available resources.
   - **Interaction Rules**: Establish rules governing how agents interact with each other and their environment.

2. **Simulation**: 
   - Execute the model over time, allowing agents to make decisions, interact, and adapt based on predefined rules.
   - Collect data on agents' behaviors and system-wide outcomes during the simulation.

3. **Analysis**: 
   - Analyze the results to understand emergent patterns and behaviors. This can involve statistical analysis, visualization of agent interactions, and evaluating how different parameters influence outcomes.

4. **Validation**: 
   - Compare model outputs with real-world data to validate the model's accuracy. Calibration may be necessary to ensure the model reflects observed behaviors accurately.

### Applications of Agent-Based Modeling

1. **Energy Systems**: 
   - **Demand Response**: ABM can simulate consumer behavior in response to dynamic pricing or demand response programs, providing insights into how to encourage energy conservation during peak demand.
   - **Renewable Energy Integration**: It helps model the interactions between different energy producers (e.g., solar, wind) and consumers, examining how they adapt to changes in supply and demand.

2. **Market Dynamics**: 
   - Simulate interactions between different energy providers and consumers to understand competitive behavior, pricing strategies, and market outcomes.
   - Evaluate the impact of regulatory changes on market behavior and investment decisions.

3. **Resource Management**: 
   - Model interactions among agents in managing shared resources, such as water or electricity, to study the effects of cooperation and competition on resource depletion or sustainability.

### Example Frameworks and Tools

Several tools and frameworks are available for building and simulating agent-based models, including:

- **NetLogo**: A user-friendly platform for creating agent-based models, particularly in education and research.
- **AnyLogic**: A powerful commercial tool that supports agent-based, system dynamics, and discrete event modeling.
- **Repast**: An open-source framework for building ABMs, widely used in academic research.
- **MASON**: A fast and flexible discrete-event simulation library for Java, suitable for developing complex ABMs.

---

[Agent-Based Modelling](#agent-based-modelling)
- **Multi-Agent Systems** in [LLM|LLMs](#llmllms)
  - The need for shared context across different agents, ensuring consistency and coherence in interactions.

## **Agent Interactions**

### How do Agents Interact?
- **Horizontal Collaboration:** Agents with local goals coordinate to achieve a shared objective.
- **Hierarchical Collaboration:** Primary agents oversee specialized agents to manage complex tasks effectively.

## **Understanding Agents**

### Core Components:
1. **Tools:** Resources such as APIs, databases, or GitHub.
2. **Strategy:** Techniques like self-criticism, [chain of thought](#chain-of-thought) (CoT), and planning to improve reasoning.
3. **States:** Memory, context tracking, and microservices for modularity.
4. **Goals:** Specific objectives defined for each agent.

## **Agent Planning and Interaction**
1. **Planning:** Agents plan operations, such as managing workflows in a support center.
2. **Agent Collaboration:** Agents align their individual goals with shared objectives to enhance system performance.
3. Multi-step

## **Compounding Systems**

### Multi-Agent Systems
These systems reduce reliance on extensive [prompt engineering](#prompt-engineering) by compartmentalizing tasks across specialized agents.  
**Example:** A writer agent drafts content, while a reviewer agent ensures quality, both operating within defined scopes.

# Agentic Solutions {#agentic-solutions}


Agentic solutions leverage multiple autonomous agents (usually [Small Language Models|SLM](#small-language-modelsslm)) to achieve goals collaboratively. These systems distribute tasks across agents that operate individually or collectively to solve complex problems.

Agents can model specific business functions. Role clarity enhances the effectiveness of these systems.

Related terms:
- [GraphRAG](#graphrag)
- [Agent-Based Modelling](#agent-based-modelling)
## **Types of Agentic Solutions**

**Reactive Solutions (Ask Approach):**  
    Systems like chatbots and retrieval-augmented generation (RAG) tools respond to user queries.
    
**Autonomous Solutions (Do Approach):**  
    Agents perform tasks proactively, e.g., drafting documents or scheduling meetings.
## **Business Process Integration**

### Workflow:
1. Identify a business problem.
2. Define personas (agents) required.
3. Develop an agentic workflow.

### Agentic Architectures:
1. **Vertical:** Hierarchical structures for task delegation.
2. **Horizontal:** Collaborative structures with high feedback loops.
3. **Mixed:** Combines vertical delegation with horizontal collaboration.

**Vertical Example:** A primary agent delegates tasks to lower-level agents for execution.

## **The Orleans Framework**

A framework for building distributed applications in .NET.
- **Grains:** Individual agents performing specific tasks.
- **Silos:** Distributed nodes managing grains.
- **Clusters:** Collections of silos for scalability.

## **Benefits of Using Agents**

1. **Performance Gains:** Task parallelization enhances throughput.
2. **Developer Abstraction:** Modular design simplifies system understanding and debugging.
3. **Workflow Integration:** Aligns AI agents with organizational processes.

## **Example Use Cases**

1. **IT Helpdesk Agent:** Automates troubleshooting and network access requests.
2. **Device Refresh Agent:** Manages hardware upgrades and approvals.
3. **Lead Generation Agent:** Identifies and researches potential leads.
4. **Budget Management Agent:** Reviews financial data and aids in planning.
5. **Customer Support Agent:** Triage support issues for faster resolution.
6. **Project Tracker Agent:** Tracks project milestones and budget compliance.





# Aggregation {#aggregation}

Aggregation

Summarizing data for analysis ([Pandas Pivot Table](#pandas-pivot-table) and [Groupby](#groupby)).

In [DE_Tools](#de_tools) see:
	- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/group_by.ipynb
	- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/pivot_table.ipynb

# Ai Engineer {#ai-engineer}


They know what
- [LSTM](#lstm) means
- [Attention mechanism](#attention-mechanism)
- [Prompting](#prompting) optimisation
- [Neural network](#neural-network)



# Ai Governance {#ai-governance}

AI Governance

[Data Governance](#data-governance)

Used in regulated sectors.  

Constraints to using ai: 
- legal,
- transparency, 
- security, 
- historical bias  

AI acts and standards: 
- eu and AI acts
- NIST standards framework  
- Security OWASP standards for LLM  

how will beaurcracy keeps up with ai innovation.  

Governance can stifle innovation.

# Algorithms {#algorithms}


###  [Recursive Algorithm](#recursive-algorithm)

### Backtracking

Backtracking is a method for solving problems incrementally, by trying partial solutions and then abandoning them if they are not valid.

Example: Graph coloring with the 4-color theorem.

### Divide and Conquer

Divide and conquer is a strategy that involves breaking a problem into smaller subproblems of the same type, solving these subproblems recursively, and then combining their solutions to solve the original problem.

Example: Merge sort, where the array is split in half, and each smaller part is sorted.

Note: Subproblems do not generally overlap.

### Dynamic Programming

Dynamic programming is used for optimization problems and involves storing past results to find new results efficiently.

- [Memoization](#memoization): This technique "remembers" past results to avoid redundant calculations.
- It is characterized by overlapping substructure and overlapping subproblems.

Examples: Fibonacci numbers, Towers of Hanoi, etc.

### Greedy Algorithms

A greedy algorithm builds up a solution piece by piece, always choosing the next piece that offers the most immediate benefit without regard for future consequences. The hope is that by choosing a local optimum at each step, a global optimum will be reached.

Examples: Dijkstra's shortest path algorithm, Knapsack problem.

### Branch and Bound

Branch and bound algorithms are typically used for optimization problems and are similar to backtracking.

- As the algorithm progresses, a tree of subproblems is formed, with the original problem as the "root problem."
- A method is used to construct upper and lower bounds for a given problem.
- At each node, bounding methods are applied:
    - If the bounds match, it is deemed a feasible solution for that subproblem.
    - If the bounds do not match, the problem represented by that node is partitioned into two subproblems, which become child nodes.
- The process continues, using the best known feasible solution to trim sections of the tree until all nodes have been solved or trimmed.

Example: Traveling salesman problem.

### Brute Force

Brute force algorithms try all possible solutions until they find an optimized or satisfactory answer. Heuristics can be used to assist in this process.

### Randomized Algorithms

Randomized algorithms use random numbers to influence their behaviour.

Example: [K-means](#k-means) clustering initialization.

# Alternatives To Batch Processing {#alternatives-to-batch-processing}

If you’re working with a streaming dataset ([Data Streaming](#data-streaming)), why might [batch processing](#batch-processing) not be suitable, and what alternatives would you consider?  

**Latency**: Batch processing involves collecting data over a period and processing it in large chunks. This can introduce significant delays, making it unsuitable for applications that require real-time or near-real-time insights.

**Timeliness**: Streaming datasets often require immediate processing to respond to events as they occur. Batch processing cannot meet the demand for timely [data analysis](#data-analysis).

**Data Freshness**: In streaming scenarios, data is continuously generated, and waiting for a batch interval can result in outdated information being analyzed.

### Alternatives to Batch Processing

1. **Stream Processing**: This approach processes data in real-time as it arrives. Tools like [Apache Kafka](#apache-kafka), [Apache Flink](#apache-flink), and [Apache Spark Streaming](#apache-spark-streaming) are designed for handling streaming data efficiently.

2. **Event-Driven Architectures**: Implementing an [event-driven architecture](#event-driven-architecture) allows systems to react to data changes or events immediately, ensuring timely processing and response.

3. **Micro-batching**: This technique processes small batches of data at very short intervals, striking a balance between batch and stream processing. Tools like [Apache Spark Streaming](#apache-spark-streaming) can utilize micro-batching to handle streaming data more effectively.

4. **Complex Event Processing (CEP)**: CEP systems analyze and process streams of events in real-time, allowing for the detection of patterns and trends as they happen.

# Amazon S3 {#amazon-s3}


# Amazon S3
## Amazon S3 buckets (onedrive essentially)

Amazon Simple Storage Service (S3) is a versatile <mark>object storage solution</mark> known for its scalability, data availability, security, and performance.

Stands for Simple Storage Service.

**What is Amazon S3?**

It's an object storage service, allowing you to upload and store various types of objects, including images, text, videos, and more. S3's structure resembles that of a typical file system, with folders, subfolders, and files. Notably, it's extremely cost-effective, with storage costs starting at only 0.023 cents per GB, and it offers high durability with data replicated across three availability zones.

**Why is Amazon S3 Useful?**

1. **Cost-Effective Storage**: S3 provides cheap, reliable storage for objects of any type.
2. **Low Latency, High Throughput**: It offers fast access to your data, making it suitable for hosting static websites.
3. **Integration with AWS Services**: S3 can be integrated with other AWS services like SNS, SQS, and Lambda for powerful event-driven applications.
4. **Lifecycle Management**: S3 offers lifecycle management to automatically transition data to lower-cost storage tiers based on access patterns.

**Step-by-Step Walkthrough in the AWS Console**

1. **Creating a Bucket**: Start by navigating to the AWS Management Console and selecting S3. Create a bucket with a unique name and choose the region closest to your application.
2. **Uploading a File**: Once the bucket is created, upload a file using the console. You can add metadata and set permissions as needed.
3. **Exploring Settings**: Dive into bucket settings to configure options like versioning, logging, encryption, and lifecycle management. Block public access to ensure data security.

**Additional Features**

1. **Transfer Acceleration**: Accelerate data transfer to and from S3 using optimized network paths.
2. **Events**: Configure event notifications to trigger actions in response to S3 events like object creation or deletion.
3. **Requester Pays**: Enable Requester Pays to allow bucket owners to pass on data transfer costs to requesters.

# Anomaly Detection In Time Series {#anomaly-detection-in-time-series}

In [Time Series](#time-series)

In [ML_Tools](#ml_tools) see:
- [TS_Anomaly_Detection.py](#ts_anomaly_detectionpy)

To perform anomaly detection specifically for time series data, you can utilize various techniques that account for the <mark>temporal nature</mark> of the data. Here are some common methods:

1. Statistical Methods:
   - Moving Average: Calculate a moving average and identify points that deviate significantly from this average.
   - Seasonal Decomposition: Decompose the time series into trend, seasonal, and residual components. Anomalies can be identified in the residuals.

2. Time Series Models:
   - AutoRegressive Integrated Moving Average: Fit an ARIMA model to the time series data and analyze the residuals for anomalies.
   - State Space Model (ETS): Similar to ARIMA, this model can be used to forecast and identify anomalies in the residuals.

3. Machine Learning Approaches:
   - [LSTM](#lstm) (Long Short-Term Memory): Use LSTM networks to model the time series and detect anomalies based on prediction errors.
   - [Isolated Forest](#isolated-forest): This algorithm can be adapted for time series data by treating time as an additional feature.

4. Change Point Detection:
   - Identify points in time where the statistical properties of the time series change significantly, which may indicate anomalies.

5. Visual Methods:
   - Time Series Plots: Visual inspection of time series plots can help identify anomalies.
   - Control Charts: Use control charts to monitor the time series and flag points that fall outside control limits.


# Anomaly Detection With Clustering {#anomaly-detection-with-clustering}

[Clustering](#clustering)
- Description: Outliers often form small clusters or are isolated from main clusters.

### **8. [DBSCAN](#dbscan) (Density-Based Spatial Clustering of Applications with Noise)**

- **Purpose:**  
    Finds anomalies based on density rather than explicit statistical assumptions.
- **Steps:**
    - Identify points in low-density regions as anomalies.

[Isolated Forest|iForest](#isolated-forestiforest)

### **2. Local Outlier Factor (LOF)**

LOF is a density-based anomaly detection method that identifies anomalies by comparing the local density of a point with that of its neighbors.

**Steps:**

- For each point, calculate the local density based on the distance to its k-nearest neighbors.
- Compute the LOF score, which measures the degree of isolation of a point relative to its neighbors.
- Points with a LOF score significantly greater than 1 are considered anomalies.

# Anomaly Detection With Statistical Methods {#anomaly-detection-with-statistical-methods}


Basic:
- [Z-Normalisation|Z-Score](#z-normalisationz-score)
- [Interquartile Range (IQR) Detection](#interquartile-range-iqr-detection)
- [Percentile Detection](#percentile-detection)

Advanced:
- [Gaussian Model](#gaussian-model)
- [Isolated Forest](#isolated-forest)

### Grubbs' Test

Context:  
Grubbs' test is a hypothesis test designed to detect a single outlier in a normally distributed dataset. It tests the largest deviation from the mean relative to the standard deviation. This test is iterative and removes one outlier at a time.

Purpose:  
To determine whether the most extreme data point (either smallest or largest) is a statistical outlier.

Steps:
- Compute the test statistic:  
    $G = \frac{\max(|X - \mu|)}{\sigma}$  
    where:
    - $X$: Data points
    - $\mu$: Mean of the dataset
    - $\sigma$: Standard deviation of the dataset.
- Compare $G$ to a critical value:
    - The critical value depends on the sample size $n$ and significance level $\alpha$ (e.g., 0.05).
    - If $G$ exceeds the critical value, the data point is considered an outlier.

Limitations:
- Assumes data follows a normal distribution.
- Inefficient for detecting multiple outliers simultaneously.

### Histogram-Based Outlier Detection (HBOS)

Context:  

HBOS is a non-parametric method that detects anomalies by analyzing the distribution of individual features independently. It relies on histograms, which estimate feature density.

Purpose:  
To identify outliers as data points falling in bins with low frequencies or densities.

Steps:
- Create histograms for each feature:
    - Divide each feature's range into bins.
    - Count the frequency of data points in each bin.
- Calculate scores for each data point:
    - Outliers are points in bins with significantly lower densities compared to others.

Advantages:
- Does not assume a specific data distribution.
- Scales well to large datasets.

Limitations:
- Assumes feature independence (not ideal for [multivariate data](#multivariate-data)).
- Sensitive to bin size selection.

### One-Class SVM

One-Class Support Vector Machine is a variation of the SVM algorithm used for anomaly detection. It learns a decision boundary around the normal data points.

Steps:
- Train the model on the normal data points.
- The model attempts to find a hyperplane that separates the normal data from the origin.
- Points that fall outside this boundary are classified as anomalies.

# Anomaly Detection {#anomaly-detection}

Anomaly detection involves identifying [standardised/Outliers|Outliers](#standardisedoutliersoutliers). Detecting these anomalies is crucial for maintaining [data integrity](#data-integrity) and improving model performance.
## Methods for Detecting Anomalies

In [ML_Tools](#ml_tools) see: [Pycaret_Anomaly.ipynb](#pycaret_anomalyipynb)
## Process of Detection

Data Preparation
   - [Data Cleansing](#data-cleansing): Handle missing values and remove any irrelevant data points.
   - [Normalisation](#normalisation)/[Standardisation](#standardisation): Scale the data if necessary, especially if using methods sensitive to the scale.

Anomaly Detection with a model: Use a chosen method to flag anomalies
- [Anomaly Detection with Clustering](#anomaly-detection-with-clustering)
- [PCA-Based Anomaly Detection](#pca-based-anomaly-detection)
- [Anomaly Detection in Time Series](#anomaly-detection-in-time-series)
- [Anomaly Detection with Statistical Methods](#anomaly-detection-with-statistical-methods)

Validation
   - Validate the detected anomalies by comparing them against known anomalies (if available) or using domain knowledge.
   - Adjust thresholds or methods based on validation results.

Visualization

- Visualize the results using plots (e.g., scatter plots, box plots) to understand the distribution of data and the identified anomalies.
- [Boxplot](#boxplot): Displays the distribution and identifies outliers using the interquartile range (IQR).
- Scatter Plot: Helps visually identify outliers.

# Apache Airflow {#apache-airflow}


Schedular think CROM jobs with python.

Apache Nifi may be better.

[Airflow](https://airflow.apache.org/) is a [data orchestrator](term/data%20orchestrator.md) and the first that made task scheduling popular with [Python](term/python.md). 

Airflow programmatically author, schedule, and monitor workflows. It follows the [imperative](term/imperative.md) paradigm of schedule as *how* a DAG [Directed Acyclic Graph (DAG)](#directed-acyclic-graph-dag) is run has to be defined within the Airflow jobs. Airflow calls its *Workflow as code* with the main characteristics
- **Dynamic**: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation.
- **Extensible**: The Airflow framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment.
- **Flexible**: Workflow parameterization is built-in leveraging the [Jinja Templating](term/jinja%20template.md) engine.


# Apache Kafka {#apache-kafka}


Apache Kafka is an open-source distributed event streaming platform used for building real-time data pipelines and data streaming  applications. It is designed to handle high-throughput, fault-tolerant, and scalable messaging. 

### Features

Immutable commit log: Kafka maintains an append-only log of messages, which ensures [Data Integrity](#data-integrity) and replicability.

Kafka allows applications to [Publish and Subscribe](#publish-and-subscribe) to streams of records, similar to a message queue or enterprise messaging system.

Durability and Reliability: Kafka stores streams of records in a fault-tolerant way, ensuring data durability and reliability. It replicates data across multiple nodes to prevent data loss.

[Scalability](#scalability): Kafka is designed to scale horizontally by adding more brokers (servers) to the cluster, which can handle increased load and data volume.

Kafka is optimized for high throughput, making it suitable for processing large volumes of data in real-time.

Data in Kafka is organized into topics, which are further divided into partitions. Each partition is an ordered, immutable sequence of records that is continually appended to.

Producers are applications that publish data to Kafka topics, while consumers are applications that subscribe to topics and process the data.

Kafka is commonly used for log aggregation, real-time analytics, [data integration](#data-integration), stream processing, and building event-driven architectures.

Kafka integrates ([Data Integration](#data-integration)) well with various data processing frameworks like Apache Spark, Apache Flink, and Apache Storm, as well as with databases and other [data storage](#data-storage) systems.


# Apache Spark {#apache-spark}


Apache Spark is an open-source multi-language engine for executing [Data Engineer](Data%20Engineer.md)  and [Machine Learning](Machine%20Learning.md) on single-node machines or clusters. It's optimized for large-scale data processing.

Spark runs well with [Kubernetes](term/kubernetes.md).

Spark is a highly popular framework for large-scale data processing. It allows  [Data Engineer](#data-engineer) to process massive datasets in memory, which makes it faster than traditional disk-based approaches. Spark is versatile, supporting batch processing, real-time data streaming, machine learning, and graph processing.



# Api Driven Microservices {#api-driven-microservices}


API-driven microservices refer to a [software architecture](#software-architecture) approach where [microservices](#microservices) communicate with each other and with external systems primarily through well-defined [API](#api) (Application Programming Interfaces). 

This architecture is designed to enhance modularity, scalability, and flexibility by breaking down an application into smaller, independent services that can be developed, deployed, and scaled independently.

API-driven microservices architecture is particularly beneficial for large, complex applications that require frequent updates and scaling. It allows organizations to innovate faster, improve fault isolation, and better align development efforts with business needs. However, it also introduces complexity in terms of service orchestration, data consistency, and network communication, which must be carefully managed.

Key characteristics of API-driven microservices include:

1. **Decoupled Services**: Each microservice is a separate, self-contained unit that performs a specific business function. Services are loosely coupled, meaning changes to one service do not directly impact others.

2. **API Communication**: Microservices interact with each other and with external clients through APIs. These APIs are typically RESTful, but they can also use other protocols like gRPC, GraphQL, or messaging systems like Kafka.

3. **Independent Deployment**: Each microservice can be developed, tested, deployed, and scaled independently of the others. This allows for more agile development and continuous deployment practices.

4. **Technology Agnostic**: Different microservices can be built using different technologies or programming languages, as long as they adhere to the agreed-upon API contracts.

5. **Scalability and Resilience**: Microservices can be scaled independently based on demand. If one service fails, it does not necessarily bring down the entire system, enhancing resilience.

6. **Focused Functionality**: Each microservice is designed to handle a specific business capability, making it easier to understand, develop, and maintain.

7. **API Gateway**: Often, an API gateway is used to manage and route requests to the appropriate microservices. It can also handle cross-cutting concerns like authentication, logging, and rate limiting.



# Api {#api}


An API (Application Programming Interfaces) allows one system (client) to <mark>request specific actions from another system</mark> (server).

Using a predefined set of rules and <mark>protocols</mark>. 

Good API documentation is necessary for developers to integrate and use APIs effectively.
####  Resources:
- [Link](https://www.youtube.com/watch?v=yBZO5Rb4ibo)
- [REST API](#rest-api)
- [FastAPI](#fastapi)
#### API Principles

1. **Controlled Access**: APIs provide access to certain parts of a system while keeping the core functionalities secure.
2. **System Independence**: APIs function independently of changes in the underlying system.
3. **Simplicity**: APIs are designed to be <mark>user-friendly</mark> and come with comprehensive documentation to guide developers.
#### Implementation

In [ML_Tools](#ml_tools) see: [Wikipedia_API.py](#wikipedia_apipy)

#### Example:

For instance, a weather app querying a weather API to fetch the current weather conditions involves sending a structured request and receiving a response.

Types of API Connections:
1. **Web APIs**: These facilitate communication between web clients (browsers or apps) and servers. For example, online shopping apps use APIs to process transactions on remote servers.
2. **Database APIs**: These allow applications to interact with databases, ensuring data is accessed and manipulated efficiently.
3. **Device APIs**: When apps like Instagram or WhatsApp request access to your phone's camera or microphone, they use device APIs.

# Arima {#arima}


**ARIMA** (AutoRegressive Integrated Moving Average) is a popular method for [Time Series Forecasting](#time-series-forecasting) that models the autocorrelations within the data. It is particularly useful for datasets with trends and patterns that are not seasonal. However, it is not perfect; for example, it struggles with predicting stock trading data.

## ARIMA Components

- **AR (AutoRegressive)**: Utilizes the dependency between an observation and a number of lagged observations.
- **I (Integrated)**: Involves differencing the data to achieve stationarity.
- **MA (Moving Average)**: Models the dependency between an observation and a residual error from a moving average model applied to lagged observations.

### ARIMA Explained

**ARIMA** stands for:

- **A**uto**R**egressive (AR): Uses past values to predict the current one.
- **I**ntegrated (I): Differencing to make the series stationary.
- **MA**ving Average (MA): Uses past forecast errors for prediction.

A typical **ARIMA(p,d,q)** model has:

- $p$: Number of **lagged values** (AR terms)
- $d$: Number of **differences** needed to make the series stationary
- $q$: Number of **lagged forecast errors** (MA terms)

#### 🔧 What ARIMA Does:

1. **Checks for stationarity** – if not, applies differencing ($d$ times).
2. **Models relationships** between current values and:
   - Past values (AR part)
   - Past errors (MA part)
3. **Fits** parameters by minimizing a loss (typically log-likelihood).
4. **Forecasts** future values using this learned structure.
### Why ARIMA Isn’t Enough for Seasonal Data

Your quarterly data might show **repeating patterns every 4 quarters** (seasonality), and ARIMA has **no built-in mechanism to model this periodic structure**. This is where **SARIMA** comes in.
### SARIMA: Seasonal ARIMA

SARIMA extends ARIMA by adding **seasonal components**. It is written as:

$$\text{SARIMA}(p, d, q)(P, D, Q)_s$$

Where:

- $(p, d, q)$ are **non-seasonal** ARIMA parameters (as above).
- $(P, D, Q)_s$ are **seasonal** ARIMA parameters:
  - $P$: Seasonal autoregressive terms.
  - $D$: Seasonal differences.
  - $Q$: Seasonal moving average terms.
  - $s$: Seasonality period (e.g., $s=4$ for quarterly data).

#### What SARIMA Adds:

- **Seasonal differencing** ($D$): e.g., subtract the value from 4 quarters ago to remove annual cycles.
- **Seasonal AR/MA** terms: Model relationships from past seasonal lags and errors.

### ✅ Summary

| Model   | Handles Trend | Handles Seasonality | Use When...                                   |
|---------|---------------|---------------------|------------------------------------------------|
| ARIMA   | ✅            | ❌                  | Data has trend, but no regular cycles         |
| SARIMA  | ✅            | ✅                  | Data has both trend and seasonality           |
## Advanced Variants

### SARIMAX (Seasonal ARIMA with Exogenous Variables)

- Incorporates external variables (e.g., interest rates, volume) into the forecasting model.
- Useful for [Datasets](#datasets) where external factors influence the time series.

### Related terms

In [ML_Tools](#ml_tools) see:
- https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/ARIMA.ipynb
- [Forecasting_AutoArima.py](#forecasting_autoarimapy)
- [pmdarima](#pmdarima)

# Asking Questions {#asking-questions}


### Why Ask Better Questions?

Asking better questions enhances thinking, learning, problem-solving, and communication. Good questions:

- Guide inquiry and exploration.
- Clarify assumptions.
- Elicit insights or novel responses.
- Enable self-reflection and deeper understanding.

### What Makes a Good Question?

A good question:
- **Elicits a novel or thoughtful response** — not just a fact or yes/no.
- **Opens possibilities** rather than closing them.
- **Reveals assumptions** or **forces re-evaluation** of mental models.
- **Matches the context and audience** — good questions for brainstorming differ from those for debugging.
- **Fosters chain-of-thought reasoning**, helping others articulate how they arrive at conclusions.
### Types of Questions

Questions can be classified by their **function**, **depth**, or **structure**.

#### 1. By Function

| Type        | Purpose                                   | Example                             |
|-------------|-------------------------------------------|-------------------------------------|
| Clarifying  | Understand what's being said              | "What do you mean by X?"           |
| Probing     | Dig deeper into reasoning or logic        | "Why do you think that?"           |
| Exploratory | Generate ideas or new perspectives        | "What if we reversed the problem?" |
| Reflective  | Encourage self-awareness                  | "What assumption am I making here?"|
| Critical    | Test or challenge statements              | "What evidence supports that?"     |

#### 2. By Depth
- **Surface-level:** "What is X?"
- **Mid-level:** "How does X relate to Y?"
- **Deep-level:** "Why does X matter?" or "What are the implications of X?"

#### 3. By Structure

- **Open-ended:** Encourage elaboration.  
    → _"How might we design this differently?"_
    
- **Closed:** Seek a specific answer.  
    → _"Is this implementation correct?"_
    
- **Leading:** Suggest an answer.  
    → _"Wouldn't you agree that…?"_
    
- **Falsifiable/Testable:** Can be proven right or wrong.  
    → _"Does increasing X always decrease Y?"_
### Characteristics of Good Questions

| Characteristic | Description                                                       |
|----------------|-------------------------------------------------------------------|
| **Purposeful** | Serves a clear goal in the conversation or inquiry                |
| **Contextual** | Relevant to the topic or the respondent                           |
| **Open/Expansive** | Invites multiple viewpoints or lines of reasoning             |
| **Challenging** | Pushes beyond defaults or surface-level answers                  |
| **Precise**    | Minimizes ambiguity while leaving room for elaboration           |
| **Sequenced**  | Ordered to build thought step-by-step (chain of thought)        |

### Related Concepts
- [Chain of Thought](#chain-of-thought)
- [Design Thinking Questions](#design-thinking-questions)
- [Prompting](#prompting)

### Questions:
- How LLMs generate or refine questions using [Prompting](#prompting) or [Chain of thought](#chain-of-thought) approaches?

# Attention Is All You Need {#attention-is-all-you-need}




# Attention Mechanism {#attention-mechanism}


 Think of attention like human reading behavior: when reading a complex sentence, we don't process all the words equally at every moment. Instead, we might "attend" more to certain words based on the context of what we’ve read so far and what we're trying to understand. This is similar to what the attention mechanism does in neural networks.

The attention mechanism is a key concept in modern [ML](#ml) particularly in natural language processing ([NLP](#nlp)/[LLM](#llm)) and sequence-based models like neural machine translation (NMT). It was introduced to address the limitations of earlier models, like [Recurrent Neural Networks|RNN](#recurrent-neural-networksrnn) and [LSTM](#lstm) network), in handling long sequences and capturing important dependencies within data.

The attention mechanism improves a model's ability to focus on important parts of the input data, helping it manage long-range dependencies, which is especially useful in tasks like machine translation, text generation, and various NLP tasks.

### Core Idea of Attention

The  [Transformer|Transformer](#transformertransformer) architecture, introduced by Vaswani et al. in 2017 ("[Attention Is All You Need](#attention-is-all-you-need)"), is the most popular use of the attention mechanism. The key innovation of Transformers is the <mark>self-attention mechanism</mark>, which <mark>allows each token in a sequence to attend to all other tokens</mark>, making the model more efficient and scalable for parallel processing. <mark>Transformers replaced traditional RNN-based</mark> models and have become the foundation of models like [BERT](#bert), GPT, and T5.

The attention mechanism allows a model to focus on different parts of an input sequence when making predictions, rather than relying on a fixed-size hidden state to encode all information. This selective "focus" can greatly enhance the model's ability to handle long-range dependencies.

For example, in machine translation, while translating a sentence from one language to another, different words in the input sentence might hold varying degrees of relevance to the word currently being translated. Attention helps the model dynamically weigh the importance of different words at each step of the translation.

In its simplest form, attention assigns weights to each input token based on its relevance to the current output token being generated. These weights are typically calculated using a score function and then normalized (usually through a softmax) to produce a distribution over the input sequence. The weighted sum of the input tokens is then passed as context for generating the output token.

### Key Components of Attention and Formula

I see! Here's the text with the math terms wrapped in as requested:

1. Query: Represents the current word or position that requires attention: $Q$
2. Key: Represents each word in the input sequence: $K$
3. Value: Represents the actual content or information in the input sequence:  $V$
4. Attention Scores: The attention mechanism computes the relevance between the query and each key by computing a similarity score (such as dot-product or other scoring methods).
5. Softmax: These scores are then passed through a softmax function to form a probability distribution, which gives us the attention weights.
6. Context Vector: A weighted sum of the values ($V$), using the attention weights, is computed. This context vector is what the model uses to generate the output token.

Given a query matrix, key matrix, and value matrix, attention is calculated as:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Where:
- $Q$,$K$, and $V$are matrices of query, key, and value vectors.
- $d_k$ is the dimension of the keys.
- The softmax is applied row-wise to produce attention weights.

### Applications of Attention

- Machine Translation: Aligns source and target words in a more dynamic and context-dependent way.
- Text Summarization: Helps to identify the most relevant parts of a document.
- Speech Recognition: Enhances the model’s ability to focus on important features over long audio sequences.
- Vision: Self-attention is now used in computer vision tasks (Vision Transformers or ViTs) to model dependencies between different parts of an image.

### Types of Attention Mechanisms
1. Additive Attention: Introduced in the original attention paper by Bahdanau et al. (2014), this method uses a feed-forward neural network to compute the relevance between the query and each key.
   
2. Dot-Product (Multiplicative) Attention: Introduced in the Transformer paper by Vaswani et al. (2017), this method computes the relevance using a simple dot product between the query and key vectors. It is computationally efficient and widely used.

3. Scaled Dot-Product Attention: A variant of dot-product attention, used in the Transformer architecture, where the dot product is divided by the square root of the dimension of the key vectors to avoid excessively large gradients.

4. Self-Attention: In this mechanism, the model applies attention to itself. This means each word in the input sequence attends to all other words in the sequence, including itself. Self-attention is used in models like Transformers to capture dependencies within a sentence.

### Self attention vs multi-head attention

https://www.youtube.com/shorts/Muvjex0nkes

Take every word pays attention to every other word to capture contetxt by:

1. take input word vectors,
2. break words into Q,K,V vectors,
3. compute attention matrix
4. generate final word vectors.
these vectors

Multi-head attention: perform self attention in parallel.

1. take word vectors,
2. break words into Q,K,V vectors,
	1. Break each Q,K,V vector into the number of heads parts
3. compute attention matrix for each head
4. generate final word vectors for each head
5. Combine back together

These have better understanding of the context.

[Multi-head attention](#multi-head-attention)

- This allows the model to weigh the importance of different words in a sequence when making predictions. It captures relationships between words, even if they are far apart in the sequence.
   - The model computes attention scores for each pair of words to determine how much focus one word should place on another in a sequence.

# Auc {#auc}


**AUC (Area Under the Curve)** is a metric for binary classification problems, representing the area under the [ROC (Receiver Operating Characteristic)](#roc-receiver-operating-characteristic)

#### Key Concepts
Represents the area under the ROC curve.

AUC values range from 0 to 1, where 1 indicates perfect classification and 0.5 suggests no discriminative power (equivalent to random guessing).

#### Roc and Auc Score

The `roc_auc_score` is a function from the `sklearn.metrics` module in Python that computes the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores. It is a widely used metric for evaluating the performance of binary classification models.

Key Points about `roc_auc_score`:

- **Purpose**: It quantifies the overall ability of the model to discriminate between the positive and negative classes across all possible classification thresholds.
- **Range**: The score ranges from 0 to 1, where:
    - 1 indicates perfect discrimination (the model perfectly distinguishes between the positive and negative classes).
    - 0.5 suggests no discriminative power (equivalent to random guessing).
    - Values below 0.5 indicate a model that performs worse than random guessing.
- **Input**: The function takes the true binary labels and the predicted probabilities (or decision function scores) as inputs.
- **Output**: It returns a single scalar value representing the AUC.

#### Example Code

```python
from sklearn.metrics import roc_auc_score

# Actual and predicted values
y_act = [1, 0, 1, 1, 0]
y_pred = [1, 1, 0, 1, 0]

# Compute AUC
auc = roc_auc_score(y_act, y_pred)
print(f'AUC: {auc}')
```

# Automated Feature Creation {#automated-feature-creation}


**Question:** Can we autodetect meaningful features.

[Feature Engineering](#feature-engineering) is an ad-hoc manual process that depends on domain knowledge, intuition, data exploration, and creativity. However, this process is dataset-dependent, time-consuming, tedious, subjective, and not a scalable solution.

[Automated Feature Creation](#automated-feature-creation) automatically generates features using a framework; these features can be filtered using [Feature Selection](#feature-selection) to avoid feature explosion. 

Below are some popular open-source libraries for automated feature engineering:

- Pycaret – [PyCaret](https://pycaret.org/)
- Featuretools for advanced usage – [Home](https://www.featuretools.com/) | [What is Featuretools? — Featuretools 1.1.0 documentation](https://featuretools.alteryx.com/en/stable/)
- Optuna – [A hyperparameter optimization framework](https://optuna.org/)
- Feature-engine – [A Python library for Feature Engineering for Machine Learning — 1.1.2](https://feature-engine.readthedocs.io/en/1.1.x/)
- ExploreKit – [GitHub – giladkatz/ExploreKit](https://github.com/giladkatz/ExploreKit)
- https://www.turintech.ai/blog/feature-generation-what-it-is-and-how-to-do-it


# Aws Lambda {#aws-lambda}


AWS Lambda is a serverless computing service provided by Amazon Web Services (AWS) that allows you to run code without provisioning or managing servers. 

AWS Lambda is a powerful tool for building scalable, event-driven applications without the overhead of managing server infrastructure.

With AWS Lambda, you can execute your code in response to various events, such as HTTP requests via Amazon API Gateway, changes to data in an Amazon S3 bucket, updates to a DynamoDB table, or messages arriving in an Amazon SQS queue.

Key features of AWS Lambda include:

1. **[Event Driven Events](#event-driven-events)**: AWS Lambda functions are triggered by events, which can come from a wide range of AWS services or custom applications.

2. **Automatic Scaling**: Lambda automatically scales your application by running code in response to each trigger. Your code runs in parallel and processes each trigger individually, scaling precisely with the size of the workload.

3. **Pay-as-You-Go**: You are charged based on the number of requests for your functions and the time your code executes. This means you only pay for the compute time you consume.

4. **No Server Management**: AWS Lambda abstracts the underlying infrastructure, so you don't need to manage servers, patch operating systems, or worry about scaling.

5. **Supports Multiple Languages**: AWS Lambda supports several programming languages, including Python, Java, Node.js, C#, Ruby, and Go, among others.

6. **Integration with AWS Services**: Lambda integrates seamlessly with other AWS services, allowing you to build complex, scalable applications.

Here's a simple example of how AWS Lambda might be used:

- You have an [S3 bucket](#s3-bucket) where users upload images.
- An AWS Lambda function is triggered whenever a new image is uploaded.
- The Lambda function processes the image, such as generating thumbnails or extracting metadata.
- The processed data is then stored back in S3 or sent to another AWS service for further processing.

# Azure {#azure}


Public cloud computing platform from Microsoft offering various services like infrastructure, data storage, and machine learning.

# B Tree {#b-tree}



# Backpropagation {#backpropagation}


>[!Summary]  
> Backpropagation is an essential algorithm in the training of neural networks and iteratively correcting its mistakes. It involves a process of calculating the gradient of the loss function $L(\theta)$ concerning each weight in the network, allowing the system to update its weights via [Gradient Descent](#gradient-descent). 
> 
> This process helps minimize the difference between predicted outputs and actual target values. Mathematically, the chain rule of calculus is employed to propagate errors backward through the network.
> 
Each layer in the network computes a partial derivative that is used to adjust the weights. This iterative approach continues until a convergence criterion is met, typically when the change in loss falls below a threshold.
>
>The backpropagation algorithm is critical in [Supervised Learning](#supervised-learning), where labeled data is used to train models to recognize patterns.

>[!Breakdown]  
> Key Components:  
> - **Algorithm**: Gradient Descent  
> - **Mathematical Foundation**: Chain Rule for derivatives  
> - **Metrics**: Loss function (e.g., Mean Squared Error, Cross-Entropy)

>[!important]  
> - Gradient descent uses $\nabla L(\theta) = \frac{\partial L}{\partial \theta}$ to iteratively minimize the loss.  
> - Backpropagation optimizes deep learning models by adjusting weights based on error gradients.

>[!attention]  
> - The method is computationally expensive for deep networks due to the need to compute gradients for each layer.  
> - Vanishing/exploding gradients in deep layers can prevent proper weight updates.

>[!Example]  
> A feed-forward neural network trained on image classification data uses backpropagation to minimize cross-entropy loss. The gradient of the loss is calculated layer by layer, adjusting weights through an optimization algorithm like Adam.

>[!Follow up questions]  
> - How does backpropagation compare with other optimization algorithms such as Newton’s method or evolutionary strategies?  
> - What role does [Regularisation](#regularisation) play in addressing overfitting when using backpropagation in deep neural networks?

>[!Related Topics]  
> - Gradient Descent Optimizers ([Adam Optimizer](#adam-optimizer), RMSprop)  
> - [vanishing and exploding gradients problem](#vanishing-and-exploding-gradients-problem)


# Backpropgation

Backpropgation is used to calc the gradient of the loss function with respect to the model parameters, when there are a lot of parameters - i.e in a neural network.
Simple example of backpropgation

Simple example of computation graph
Computation graph - calcing derivatives?
Use sympy to calculate derivatives for the loss function.

> The steps in backprop   
>Now that you have worked through several nodes, we can write down the basic method:\
> working right to left, for each node:
>- calculate the local derivative(s) of the node
>- using the chain rule, combine with the derivative of the cost with respect to the node to the right.   

The 'local derivative(s)' are the derivative(s) of the output of the current node with respect to all inputs or parameters.

Example of using sympy to calculate derivatives for the loss function. Use `diff`, `subs`
```python
from sympy import symbols, diff
```

## [Sympy](#sympy)

# Bag Of Words {#bag-of-words}


In [ML_Tools](#ml_tools) see: [Bag_of_Words.py](#bag_of_wordspy)

In the context of natural language processing (NLP), the Bag of Words (BoW) model is a simple and commonly used <mark>method for text representation</mark>. It converts text data into numerical form by treating each <mark>document as a collection of individual words, disregarding grammar and word order</mark>. Here's how it works:

1. Vocabulary Creation: A vocabulary is created from the entire corpus, which is a list of all unique words appearing in the documents.

2. Vector Representation: Each document is represented as a vector, where each element corresponds to a word in the vocabulary. The value of each element is typically the count of occurrences of the word in the document.

3. Simplicity and Limitations: While BoW is easy to implement and useful for tasks like text classification, it has limitations. It ignores word order and context, and can result in large, sparse vectors for large vocabularies.

Despite its simplicity, BoW can be effective for certain NLP tasks, especially when combined with other techniques like [TF-IDF](#tf-idf) to weigh the importance of words.

Takes key terms of a text in normalised <mark>unordered</mark> form.

`CountVectorizer` from scikit-learn to convert a collection of text documents into a matrix of token counts.

```python
#Need normalize_document
from sklearn.feature_extraction.text import CountVectorizer

# Using CountVectorizer with the custom tokenizer
bow = CountVectorizer(tokenizer=normalize_document)
bow.fit(corpus)  # Fitting text to this model
print(bow.get_feature_names_out())  # Key terms
```

Represent each sentence by a vector of length determined by get_feature_names_out. representing the tokens contained.

# Bag_of_Words.py {#bag_of_wordspy}

### Summary of What the Script Does:

1. It takes a dataset of text (movie reviews in this case) and processes it to remove HTML tags, non-alphabetic characters, and stopwords.
2. It transforms the cleaned text into numerical features using the **Bag of Words** model, where each word in the reviews is counted and represented as a feature.
3. It prints a sample of the top features (words) that were extracted from the reviews.

This is a typical text preprocessing pipeline used to prepare textual data for machine learning models.

# Bagging {#bagging}


# Overview:

Bagging, short for Bootstrap Aggregating, is an [Model Ensemble](#model-ensemble) technique designed to improve the stability and accuracy of machine learning algorithms. 

It works by <mark>training multiple instances of the same learning algorithm on different subsets of the training data</mark> and then <mark>combining their predictions.</mark>

### How Bagging Works:

1. **Bootstrap Sampling**: Bagging involves creating multiple subsets of the training data by sampling with replacement. This means that each subset, or "bootstrap sample," is drawn randomly from the original dataset, and some data points may appear multiple times in a subset while others may not appear at all.

2. **Parallel Training**: Each bootstrap sample is used to train a separate instance of the same base learning algorithm. These models are trained independently and in parallel, which makes bagging computationally efficient.

3. **Combining Predictions**: Once all models are trained, their predictions are combined to produce a final output. For regression tasks, this is typically done by <mark>averaging</mark> the predictions. For classification tasks, <mark>majority voting</mark> is used to determine the final class label.

### Key Concepts of Bagging:

- **Reduction of [Overfitting](#overfitting)**: By averaging the predictions of multiple models, bagging reduces the variance and helps prevent overfitting, especially in high-variance models like decision trees.

- **Diversity**: The use of different subsets of data for each model introduces diversity among the models, which is crucial for the success of ensemble methods.

- **Parallelization**: Since each model is trained independently, bagging can be easily parallelized, making it scalable and efficient for large datasets.
### Example of Bagging:

**Random Forest**: A well-known example of a bagging technique is the [Random Forests](#random-forests) algorithm. 

It uses decision trees as base models and combines their predictions to improve accuracy and robustness. 

Each tree in a random forest is trained on a different bootstrap sample of the data, and the final prediction is made by averaging the outputs (for regression) or majority voting (for classification).
# Further Understanding

### Advantages of Bagging:

- **Increased Accuracy**: By combining multiple models, bagging often achieves higher accuracy than individual models.
- **Robustness**: Bagging is less sensitive to overfitting, especially when using high-variance models like decision trees.
- **Flexibility**: It can be applied to various types of base models and is not limited to a specific algorithm.

### Challenges of Bagging:

- **Complexity**: While bagging reduces overfitting, it can increase the complexity of the model, making it harder to interpret.
- **Computational Cost**: Training multiple models can be computationally intensive, although this can be mitigated by parallel processing.


# Bandit_Example_Fixed.Py {#bandit_example_fixedpy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Bandit_Example_Fixed.py



# Bandit_Example_Nonfixed.Py {#bandit_example_nonfixedpy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/Bandit_Example_Nonfixed.py

# Bash {#bash}

#### Automation Scripts

In [ML_Tools](#ml_tools), see: [Bash_folder](#bash_folder)
#### **Basic Commands**

1. **Show Current Directory**: 
   
```bash
   pwd
   
```
2. **Display Contents of a Text File**: 
   
```bash
   cat filename.txt
   
```
3. **Search for a Word in a File**: 
   
```bash
   grep "word" filename.txt
   
```
4. **Replace Text in a File (Output Only)**: 
   
```bash
   sed 's/old/new/g' filename.txt
   
```

### Writing and Running a Bash Script

1. **Create a Script**: 
   
```bash
   nano hello.sh
   
```
   Add:
   
```bash
   #!/bin/bash
   echo "Hello, $(whoami)! Welcome to Bash scripting!"
   
```
   Save and exit: **Ctrl + O**, **Enter**, **Ctrl + X**.

2. **Make the Script Executable**: 
   
```bash
   chmod +x hello.sh
   
```

3. **Run the Script**: 
   
```bash
   ./hello.sh
   
```

### Useful Bash Automation Tips

- **Clear Screen**: 
   
```bash
   clear
   
```
- **Keyboard Shortcut**: **Ctrl + L**.
- **Clear Screen and Command History**: 
   
```bash
   clear && history -c
   
```
- **Reset Terminal**: 
   
```bash
   reset
   
```

### Managing Command History

1. **Clear Current Session’s History**: 
   
```bash
   history -c
   
```
2. **Save History to a Custom File**: 
   
```bash
   history > my_session_history.txt
   
```
3. **Clear and Remove Saved History**: 
   
```bash
   history -c
   > ~/.bash_history
   
```
4. **Start a Fresh Bash Session**: 
   
```bash
   exec bash
   
```

#### **Example: Conditional Execution**
```bash
if [ -f filename.txt ]; then
  echo "File exists."
else
  echo "File does not exist."
fi
```



# Batch Normalisation {#batch-normalisation}

Links:
- [Batch normalization | What it is and how to implement it](https://www.youtube.com/watch?v=yXOMHOpbon8&list=PLcWfeUsAys2nPgh-gYRlexc6xvscdvHqX&index=2)

Can be used to handle [vanishing and exploding gradients problem](#vanishing-and-exploding-gradients-problem) and [Overfitting](#overfitting) problems within [Neural network](#neural-network).

First note:
[Normalisation vs Standardisation](#normalisation-vs-standardisation)

How does Batch normalisation work?

Batch normalisation works by first standardising the inputs, then scales linearly - coefficients determined through training. This occurs between each layer.

Outcomes of this process:
- epochs take longer, but less epochs are required.

Benefits:
- Batch normalisation occurs at each layer, so do not need separate normalisation step for input data.
- What about bias? We do not need bias in BN.

![Pasted image 20241219071904.png](../content/images/Pasted%20image%2020241219071904.png)


### Example: 

```python
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt

mnist = keras.datasets.mnist
(X_train_full, y_train_full) , (X_test, y_test) = mnist.load_data()

plt.imshow(X_train_full[12], cmap=plt.get_cmap('gray' ))
X_valid, X_train = X_train_full[:5000] / 255, X_train_full[5000:]/255
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]
X test = x test/255

model = keras.models.Sequential([
keras.layers.Flatten(input_shape=[28,28]),
keras.layers.Dense(300, activation = "relu"),
keras.layers.Dense(100, activation = "relu"),
keras.layers.Dense(10, activation = "softmax")])

```

Introducing BN into this model.

Do you put BN before or after a activation function? Author of Paper suggests before.
```python
# Dont need as have BN now
# X valid, X train = X_train_full[ :5000] / 255, X_train_full[5000:]/255
# y_valid, y_train = y_train_full[:5000], y_train_full[5000:]
# X test = X test/255

model = keras.models.Sequential ([
keras.layers.Flatten(input_shape=[28,28]),
keras.layers.BatchNormalization(), # normalisation layer.
keras.layers.Dense(300,use_bias=False),
keras.layers.BatchNormalization(),
keras.layers.Activation('relu'),
keras.layers.Dense(100,use_bias=False), I
keras.layers.BatchNormalization(),
keras.layers.Activation('relu'),
keras.layers.Dense(10, activation = "softmax")

])
```



# Batch Processing {#batch-processing}


**Batch Processing** is a technique used to handle and process large datasets efficiently. It works by breaking the data into smaller chunks and processing them together in a single batch.

[Apache Spark](#apache-spark) is the leading technology for batch processing, offering scalable and distributed data processing. It can handle unmanageable data sizes by using parallelism and [Distributed Computing](#distributed-computing)

A key concept in batch processing is **MapReduce**:
  - **Map**: Splits the data into smaller, manageable pieces for parallel processing.
  - **Reduce**: Aggregates the processed data results from the individual tasks.
  - **Order**: The order of Map and Reduce steps is flexible; the primary focus is on splitting and then aggregating data.

Batch processing is widely supported by cloud infrastructures like **Amazon EMR** and **[Databricks](#databricks)**, which provide scalable environments for running batch jobs.




[Batch Processing](#batch-processing)
   **Tags**: #data_processing, #data_workflow

# Bellman Equations {#bellman-equations}


[What are the Bellman equations that are used in RL?](#what-are-the-bellman-equations-that-are-used-in-rl)

Equations here may not be accurate.

In reinforcement learning, Bellman's equations are fundamental to understanding how agents make decisions to maximize rewards over time. They are used to describe the relationship between the value of a state and the values of its successor states. There are two main types of Bellman's equations:

1. Bellman Equation for State Value Function (V):
   - This equation expresses the value of a state as the expected return starting from that state and following a particular policy. It is defined as:
     $$
     V(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V(s')]
     $$
   - Here, \(V(s)\) is the value of state \(s\), \(\pi(a|s)\) is the policy (probability of taking action \(a\) in state \(s\)), \(P(s'|s, a)\) is the transition probability to state \(s'\) from state \(s\) taking action \(a\), \(R(s, a, s')\) is the reward received, and \(\gamma\) is the discount factor.

2. Bellman Equation for Action Value Function (Q):
   - This equation expresses the value of taking an action in a given state under a particular policy. It is defined as:
     $$
     Q(s, a) = \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma \sum_{a'} \pi(a'|s') Q(s', a')]
     $$
   - Here, \(Q(s, a)\) is the value of taking action \(a\) in state \(s\), and the other terms are similar to those in the state value function.

Bellman's equations are used in dynamic programming methods like Value Iteration and Policy Iteration to find optimal policies and value functions. They provide a recursive decomposition of the value functions.

# Benefits of Data Transformation {#benefits-of-data-transformation}

## Benefits of [Data Transformation](#data-transformation)  

- Efficiency: Faster query performance.  
- [Interoperability](#interoperability): Converting data into the required format for target systems.  
- Enrichment: Adding contextual data for better insights.  
- [Data Quality](#data-quality): Validating, cleansing, and deduplicating data.

# Bernoulli {#bernoulli}



# Bert Pretraining Of Deep Bidirectional Transformers For Language Understanding {#bert-pretraining-of-deep-bidirectional-transformers-for-language-understanding}




# Bert {#bert}


BERT (<mark>Bidirectional Encoder Representations from [Transformer](#transformer)</mark>) is used in [NLP](#nlp)processing, developed by [Google](#google). 

Introduced in the paper "[BERT Pretraining of Deep Bidirectional Transformers for Language Understanding](#bert-pretraining-of-deep-bidirectional-transformers-for-language-understanding)" in 2018. 

It is forward & backward looking in the context.

BERT is a stack of encoders -learning context.

Input [Vector Embedding|embedding](#vector-embeddingembedding):
- [Positional Encoding](#positional-encoding): passes location info to encoder
- Sentence embeddings: differences between sentences
- Token embeddings

Training of BERT:
- Masked Language modelling (hiding words)
- Next Sentence Prediction

Fine tuning ([Transfer Learning](#transfer-learning)) BERT model:
- New output layer dependent
 
Resources:
- [What is BERT and how does it work? | A Quick Review](https://www.youtube.com/watch?v=6ahxPTLZxU8&list=PLcWfeUsAys2my8yUlOa6jEWB1-QbkNSUl&index=12)

### What is BERT?

- BERT is based on the [Transformer](#transformer) architecture and utilizes a bidirectional approach, meaning it considers the <mark>context of a word based on both its left and right surroundings in a sentence.</mark> This allows BERT to capture nuanced meanings and relationships between words more effectively than unidirectional models

- Pre-training and Fine-tuning/[Transfer Learning](#transfer-learning) techniques. It learns to predict masked words in sentences (Masked Language Model) and to determine if one sentence follows another (Next Sentence Prediction).
### What is BERT Used For?

1. Text Classification: Assigning categories or labels to text documents, such as sentiment analysis or topic classification.

2. Named Entity Recognition ([Named Entity Recognition|NER](#named-entity-recognitionner)): Identifying and classifying entities (e.g., names, organizations, locations) within text.

3. Question Answering: Providing answers to questions based on a given context or passage of text.

4. Text [Summarisation](#summarisation): Generating concise summaries of longer documents while retaining key information.

5. Language Translation: Assisting in translating text from one language to another.

6. [Sentence Similarity](#sentence-similarity) :Measuring the similarity between sentences, which can be useful for tasks like paraphrase detection or duplicate question identification.



# Bertscore {#bertscore}



# Bias And Variance {#bias-and-variance}


### Related to [Overfitting](#overfitting)

Ways to Reduce Bias and Variance:
- [Regularisation](#regularisation)
- [Boosting](#boosting)
- [Bagging](#bagging)

What is Bias in Machine Learning?  
Bias occurs when a model produces <mark>consistently unfair or inaccurate results</mark>, usually caused during training due to design choices.

What Does High Bias Mean for a Machine Learning Model?  
High bias refers to a situation where a model has a strong and often <mark>simplistic assumption</mark> about the underlying data, leading to underfitting.

It is biased to the data.

What is the Variance of a Machine Learning Model?  
Variance measures how much a <mark>model's predictions change when trained on different subsets</mark> of the training data. It indicates how much the model overfits the training data.

What is the Difference Between Bias and Variance in Machine Learning?

- Bias: The error that occurs when the model cannot learn the true relationship between input and output variables.
- Variance: The error that arises when the model is <mark>too sensitive</mark> to the training data and does not generalize well to new data.

Explain the Bias-Variance Trade-off in the Context of Model Complexity:

The bias-variance trade-off describes the relationship between model complexity and performance. 
- High bias (underfitting) occurs when a model is too simple, leading to poor performance on both training and test data. 
- High variance (overfitting) happens when a model is overly complex, performing well on training data but poorly on unseen data.

# Big Data {#big-data}


The concept of Big Data revolves around datasets that are too large or complex to be managed using traditional data processing techniques. It’s characterized by four main attributes, commonly referred to as the <mark>Four V’s:</mark>

- Volume: The sheer amount of data being generated, often in terabytes, petabytes, or even exabytes.
- Variety: The diversity in data types, including structured, semi-structured, and unstructured data (e.g., text, images, videos).
- Velocity: The speed at which data is generated and needs to be processed in real-time or near-real-time.
- Veracity: The uncertainty or quality of the data, addressing issues like noise, biases, or incomplete data.

Big Data Technologies
- [Apache Spark](#apache-spark)
- [Hadoop](#hadoop)
- [Scala](#scala)
- [Databricks](#databricks)

Handling big data involves:
- Distributed storage systems/ [Data Storage](#data-storage): Ensuring that data is split and stored across multiple machines for redundancy and speed.
- Processing frameworks: Using tools like [Apache Spark|Spark](#apache-sparkspark) or Hadoop to process data efficiently in parallel.
- Cloud platforms: Leveraging cloud infrastructure (e.g., Azure, AWS, Google Cloud) to scale resources dynamically based on workload.






[Big Data](#big-data)
   **Tags**: #big_data, #data_processing

# Big O Notation {#big-o-notation}


Big-O Notation is an analysis of the algorithm using [Big – O asymptotic notation](https://www.geeksforgeeks.org/analysis-of-algorithms-set-3asymptotic-notations/).  

Mostly related to computing rather than storage

Doing things not exponentially, such as copying the same data many times, will save lots of performance and money.

We can express algorithmic complexity using the big-O notation. For a problem of size N:
-   A constant-time function/method is “order 1” : O(1)
-   A linear-time function/method is “order N” : O(N)
-   A quadratic-time function/method is “order N squared” : O(N^2) 


# Bigquery {#bigquery}

cloud-based [Data Warehouse](#data-warehouse)

BigQuery is a fully managed, serverless data warehouse offered by [Google](#google) Cloud Platform (GCP). It is designed to handle large-scale data analytics and allows users to run fast SQL queries on massive datasets. 

1. **Serverless Architecture:** BigQuery is serverless, meaning users do not need to manage any infrastructure. Google handles the provisioning of resources, scaling, and maintenance, allowing users to focus on analyzing data.

2. **Scalability:** BigQuery can scale to handle petabytes of data, making it suitable for large datasets and complex queries.

3. **SQL Support:** BigQuery supports standard SQL, making it accessible to users familiar with SQL syntax. It also offers extensions for advanced analytics.

4. **Real-Time Analytics:** BigQuery can ingest streaming data and perform real-time analytics, enabling users to gain insights from data as it arrives.

5. **Integration:** BigQuery integrates seamlessly with other Google Cloud services, such as Google Cloud Storage, Google Sheets, and Google Data Studio, as well as third-party tools for data visualization and ETL (Extract, Transform, Load).

6. **Machine Learning:** BigQuery ML allows users to build and deploy machine learning models directly within BigQuery using SQL, without needing to move data to another platform.

7. **Security and Compliance:** BigQuery provides robust security features, including data encryption, identity and access management, and compliance with various industry standards.

8. **Cost-Effective:** BigQuery uses a pay-as-you-go pricing model, where users are charged based on the amount of data processed by queries and the amount of data stored.

# Binary Classification {#binary-classification}

Binary classification is a type of [Classification](#classification) task that involves predicting one of two possible classes or outcomes. It is used in scenarios where the goal is to categorize data into two distinct groups, such as spam vs. not spam in email filtering or disease vs. no disease in medical diagnosis.

# Binder {#binder}


https://mybinder.org/



# Boosting {#boosting}


Boosting is a type of [Model Ensemble](#model-ensemble) in machine learning that focuses on improving the accuracy of predictions by building a <mark>sequence of models</mark>.    Each subsequent model focuses on correcting the errors made by the previous ones.

It combines [Weak Learners](#weak-learners) (models that are slightly better than random guessing) to create a strong learner. 

### Key Concepts of Boosting:

1. Sequential Learning: Boosting involves training models sequentially. Each new model is trained to correct the errors made by the previous models. This means that the models are not independent of each other; instead, <mark>each model is built on the mistakes of the previous ones.</mark>

2. Focus on Misclassified Data: As models are trained in sequence, more emphasis is placed on the data points that were misclassified by earlier models. This helps the ensemble model to gradually improve its performance by focusing on the difficult-to-classify instances.

3. [Weak Learners](#weak-learners): Boosting combines multiple weak learners, which are models that perform slightly better than random guessing. By combining these weak learners, boosting creates a strong learner that has improved accuracy.

4. Examples of Boosting Algorithms: Some well-known boosting algorithms include [Ada boosting](#ada-boosting), [Gradient Boosting](#gradient-boosting), and [XGBoost](#xgboost). Each of these algorithms has its own approach to boosting, but they all share the core principle of sequentially improving model performance.

### Advantages of Boosting:

- Increased Accuracy: By focusing on the errors of previous models, boosting can significantly improve the accuracy of predictions.
- Flexibility: Boosting can be applied to various types of base models and is not limited to a specific algorithm.
- Robustness: Boosting can handle complex datasets and is effective in reducing bias and variance.

### Challenges of Boosting:

- Complexity: Boosting models can be more complex and computationally intensive than single models.
- [Interpretability](#interpretability): The final model may be harder to interpret compared to simpler models like decision trees.

# Bootstrap {#bootstrap}

sampling with replacement from an original dataset.



# Boxplot {#boxplot}


A boxplot, also known as a whisker plot, is a standardized way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. It can also highlight outliers in the dataset.
## Key Components

Uses:
- Identifying [standardised/Outliers](#standardisedoutliers).
- Understanding the spread and skewness of the data [Distributions](#distributions).
- Comparing distributions across different categories.
- Need to remove then in order to do [Data Cleansing](#data-cleansing).

Components:
- **Minimum:** The smallest data point excluding outliers.
- **First Quartile (Q1):** The median of the lower half of the dataset.
- **Median (Q2):** The middle value of the dataset.
- **Third Quartile (Q3):** The median of the upper half of the dataset.
- **Maximum:** The largest data point excluding outliers.
- **Outliers:** Data points that fall outside 1.5 times the interquartile range (IQR) above Q3 or below Q1.

## Implementing Boxplot in Python

You can create a boxplot in Python using libraries like Matplotlib and Seaborn. Here's how you can do it:

## Implementation

```python
import matplotlib.pyplot as plt

# Sample data
data = [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]

# Create a boxplot
plt.boxplot(data)

# Add title and labels
plt.title('Boxplot Example')
plt.ylabel('Values')

# Show plot
plt.show()
```
```python
import seaborn as sns
import matplotlib.pyplot as plt

# Sample data
data = [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]

# Create a boxplot
sns.boxplot(data=data)

# Add title and labels
plt.title('Boxplot Example')
plt.ylabel('Values')

# Show plot
plt.show()
```

# Business Intelligence {#business-intelligence}


Business intelligence (BI) leverages software and services to [transform data](Data%20Transformation.md) into actionable insights that inform an organization’s business decisions. 

The new term is [Data Engineer](Data%20Engineer.md). The language of a BI engineer is [SQL](SQL.md).

## Goals of BI
BI should produce a simple overview of your business, boost efficiency, and automate repetitive tasks across your organization. In more detail:


  * **[rollup](#rollup) capability** - (data) [Visualization](term/analytics.md) over the most important [KPIs][2] (aggregations) - like a cockpit in an airplane which gives you the important information at one glance.

  * **Drill-down possibilities** - from the above high-level overview drill down the very details to figure out why something is not performing as planned. **Slice-and-dice or pivot your data from different angles.

  * **[Single Source of Truth](#single-source-of-truth)** - instead of multiple spreadsheets or other tools with different numbers, the process is automated and done for all unified. Employees can talk about the business problem instead of the various numbers everyone has. Reporting, budgeting, and forecasting are automatically updated and consistent, accurate, and in timely manner.

  * **Empower users**: With the so-called self-service BI, every user can analyze their data instead of only BI or IT persons.




# Business Observability {#business-observability}


Business [Model Observability|observability](#model-observabilityobservability) refers to the ability to gain insights into the internal state and performance of a business through the continuous monitoring and analysis of data. 

It involves collecting, analyzing, and visualizing data from various sources to understand how different parts of the business are functioning and to identify areas for improvement. 

Business observability aims to provide a comprehensive view of operations, customer interactions, and other critical aspects to enable data-driven decision-making.

It helps businesses to detect issues early, optimize operations, enhance customer experiences, and drive growth and innovation.

Key components of business observability include:

1. **[Data Collection](#data-collection)**: Gathering data from various sources such as customer interactions, sales transactions, operational processes, and external market conditions.

2. **Monitoring**: Continuously tracking key performance indicators (KPIs) and metrics to ensure that the business is operating efficiently and effectively.

3. **Analysis**: Using analytical tools and techniques to interpret the data, identify patterns, and uncover insights that can inform strategic decisions.

4. **[Data Visualisation](#data-visualisation)**: Presenting data in an accessible and understandable format, such as dashboards and reports, to facilitate quick comprehension and action by stakeholders.

5. **Feedback Loops**: Implementing mechanisms to use insights gained from observability to make adjustments and improvements in business processes and strategies.



# Career Interest {#career-interest}


This is a portal to notes that I find relevant to my career:

# Casual Inference {#casual-inference}

missing data problem



# Catboost {#catboost}


CatBoost is a [Gradient Boosting](#gradient-boosting) library developed by Yandex, designed to handle [categorical](#categorical) features efficiently and provide robust performance with minimal [Hyperparameter|Hyperparameter tuning](#hyperparameterhyperparameter-tuning)

It is particularly useful in scenarios where datasets contain a significant number of categorical variables.

#### Key Advantages

1. Handling Categorical Features: 
   - CatBoost natively processes categorical features without the need for extensive preprocessing like one-hot encoding, which simplifies the workflow and reduces the risk of introducing errors during data preparation.

2. Robustness to Overfitting:
   - It employs techniques such as ordered boosting and per-feature scaling to reduce overfitting, making it a reliable choice for complex datasets.

3. Performance:
   - CatBoost offers competitive performance with minimal hyperparameter tuning, making it suitable for quick experimentation and deployment.

### Implementing CatBoost in Python

To implement CatBoost in Python, you need to install the CatBoost library and then follow these steps:

#### Step 1: Install CatBoost

You can install CatBoost using pip:
```bash
pip install catboost
```
#### Step 2: Import Necessary Libraries
```python
import catboost as cb
from catboost import CatBoostClassifier, Pool
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
```

#### Step 3: Prepare Your Data

Assume you have a dataset with features `X` and target `y`. Split the data into training and testing sets:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```
#### Step 4: Identify Categorical Features

Identify the indices of categorical features in your dataset:

```python
categorical_features_indices = [0, 1, 2]  # Example indices of categorical features
```
#### Step 5: Create a CatBoost Pool

Create a Pool object for the training data, specifying the categorical features:

```python
train_pool = Pool(data=X_train, label=y_train,cat_features=categorical_features_indices)
test_pool = Pool(data=X_test, label=y_test, cat_features=categorical_features_indices)
```

#### Step 6: Initialize and Train the Model

Initialize the CatBoostClassifier and fit it to the training data:

```python
model = CatBoostClassifier(iterations=1000, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=100)
model.fit(train_pool)
```

#### Step 7: Make Predictions and Evaluate

Make predictions on the test set and evaluate the model's performance:

```python
y_pred = model.predict(test_pool)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
```



# Central Limit Theorem {#central-limit-theorem}


The Central Limit Theorem states that the distribution of the sum (or average) of a large number of independent, identically distributed random variables approaches a normal distribution, regardless of the original distribution.

[Why is the Central Limit Theorem important when working with small sample sizes](#why-is-the-central-limit-theorem-important-when-working-with-small-sample-sizes)

### Key Points

- **Mean of Sampling Distribution:** The mean of the sampling distribution is equal to the mean of the original population.
- **Variance of Sampling Distribution:** The variance of the sampling distribution is the population variance divided by the sample size (\(n\)), making it \(n\) times smaller.
- **Applicability:** The CLT applies when calculating the sum or average of many variables, such as the sum of rolled numbers when rolling dice.

### Importance

- The CLT allows us to assume normality for various variables, which is crucial for:
  - Confidence intervals
  - Hypothesis testing
  - Regression analysis

[Central Limit Theorem](#central-limit-theorem)
**Explain the concept of the Central Limit Theorem.**;; 


<!--SR:!2024-01-26,3,250-->

# Chain Of Thought {#chain-of-thought}

**Chain of Thought (CoT) reasoning**

Asking sequenced questions that guide someone (or yourself) through a reasoning path is a core technique in problem-solving and teaching. Examples:

- "What is the known information?"
- "What is being asked?"
- "What patterns can we observe?"
- "What similar problems have we solved before?"

Used in in AI systems is a cognitive-inspired framework that improves the performance of large [language models](#language-models) (LLMs) by explicitly guiding the AI through intermediate reasoning steps.

Advantages of Chain of Thought:
- **Improved [Interpretability](#interpretability)**: Since the model outputs intermediate steps, it's easier for humans to understand how the final answer was reached.
- **Better Performance on Complex Tasks**: CoT allows the model to handle multi-step reasoning more effectively.
- **Easier Debugging**: If there's an error in reasoning, it can be spotted at a specific step in the chain, which aids in model fine-tuning and debugging.

Related to:
- [Model Ensemble](#model-ensemble)

# Change Management {#change-management}


Change management is a structured approach to transitioning individuals, teams, and organizations from a current state to a desired future state. It involves
- preparing, 
- supporting,
- and helping people to adopt change in order to drive organizational success and outcomes. 

Effective change management helps
- minimize resistance, 
- improves engagement, 
- and increases the likelihood of successful outcomes.

The process typically includes:

1. **Planning**: Identifying the need for change, defining the change, and developing a strategy to implement it.

2. **Communication**: Clearly explaining the reasons for the change, the benefits, and the impact on the organization and its people.

3. **Training and Support**: Providing the necessary training and resources to help employees adapt to the change.

4. **Knowledge Sharing**: Provide training and resources to help teams understand best practices for data quality.

5. **Implementation**: Executing the change plan while managing any resistance or challenges that arise.

6. **Monitoring and Evaluation**: Assessing the effectiveness of the change and making adjustments as needed to ensure successful adoption.

7. **Sustainability**: Ensuring that the change is maintained over time and becomes integrated into the organization's culture and operations.

Why change fails:
- Change is hard, identify the pain points.  
- Resistance is why change fails, due to loss aversion, uncertainty, unexpected change when not bought in.  

How we can accomplish change:
- Story telling will help.  
- Introduce a hook i.e. can we reduce the processing time for tasks by X amount.
- Put ourselves in a better position for tomorrow.

# Checksum {#checksum}

A checksum is a value calculated from a data set that is used to verify the integrity of that data. It acts as a fingerprint for the data, allowing systems to detect errors or alterations that may occur during storage, processing, or transmission.

When data is sent or stored, a checksum is generated based on the contents of the data. This checksum is then sent or stored alongside the data. Upon retrieval or receipt, the checksum is recalculated from the data and compared to the original checksum. If the two checksums match, it indicates that the data has remained unchanged and is likely intact. If they do not match, it suggests that the data may have been corrupted or tampered with.

Checksums are commonly used in various applications, such as:

- **File transfers**: To ensure that files are not corrupted during transfer.
- **[Data storage](#data-storage)**: To verify that data has not changed over time.
- **Networking**: To check the integrity of packets sent over a network.
### Example of a Checksum Calculation

1. **Original Data**: Let's say we have the string "Hello, World!".
   
2. **Checksum Calculation**: A common method for calculating a checksum is to sum the ASCII values of each character in the string. 

   - ASCII values:
     - H = 72
     - e = 101
     - l = 108
     - l = 108
     - o = 111
     - , = 44
     - (space) = 32
     - W = 87
     - o = 111
     - r = 114
     - l = 108
     - d = 100
     - ! = 33

   - Sum of ASCII values:
     $$ 72 + 101 + 108 + 108 + 111 + 44 + 32 + 87 + 111 + 114 + 108 + 100 + 33 =  1,  2,  0 $$

   - Let's say we take the modulo 256 of the sum to get the checksum:
     $$ 1,  2,  0 \mod 256 =  1,  2,  0 $$

3. **Sending Data**: The original data "Hello, World!" is sent along with the checksum value of 1, 2, 0.

4. **Receiving Data**: Upon receiving the data, the receiver calculates the checksum again using the same method.

5. **Verification**: If the calculated checksum matches the received checksum (1, 2, 0), the data is considered intact. If it does not match, it indicates that the data may have been corrupted during transmission.

This is a basic example, and in practice, checksums can be computed using more complex [algorithms](#algorithms) (like CRC32, MD5, or SHA-1) to provide better error detection and  [Security](#security). 



# Chi Squared Test {#chi-squared-test}

## Chi-Squared Test

The Chi-squared test is used to determine if there is a significant association between categorical variables. It assesses whether the observed frequencies in a contingency table differ from the expected frequencies, assuming the data is independent.

# Choosing A Threshold {#choosing-a-threshold}

The optimal threshold depends on the specific problem and the desired trade-off between different types of errors:

1. Manual Selection: Based on domain expertise or prior knowledge, choose a threshold that seems reasonable.
2. Receiver Operating Characteristic ([ROC (Receiver Operating Characteristic)](#roc-receiver-operating-characteristic)) Curve Analysis: Plot the true positive rate (TPR) against the false positive rate (FPR) for different threshold values. The optimal threshold often lies near the "elbow" of the ROC curve, where a small increase in FPR results in a significant increase in TPR.
3. [Precision-Recall Curve](#precision-recall-curve) Analysis: Plot the precision against the recall for different threshold values. The optimal threshold often lies near the "elbow" of the precision-recall curve, where a small decrease in precision results in a significant increase in recall.
4. [Cost-Sensitive Analysis](#cost-sensitive-analysis): Assign different costs to different types of errors (e.g., false positives vs. false negatives) and choose the threshold that minimizes the total cost.

# Choosing The Number Of Clusters {#choosing-the-number-of-clusters}

The optimal number of clusters ([clustering](#clustering)) depends on the data and the desired level of [granularity](#granularity). Here are some common approaches:

1. Elbow Method: [WCSS and elbow method](#wcss-and-elbow-method): Plot the within-cluster sum of squares (WCSS) as a function of the number of clusters. The optimal number of clusters is often the point where the WCSS starts to decrease slowly.
2. [Silhouette Analysis](#silhouette-analysis): Calculate the silhouette coefficient for each data point, which measures how similar a data point is to its own cluster compared to other clusters. The optimal number of clusters 1 is often the one that maximizes the average silhouette coefficient.T

# Ci Cd {#ci-cd}

**CI/CD** stands for **[Continuous Integration](#continuous-integration)** and **[Continuous Delivery/Deployment](#continuous-deliverydeployment)**. It is a set of practices aimed at streamlining and accelerating the [Software Development Life Cycle](#software-development-life-cycle). The main goals of CI/CD are to improve software quality, reduce integration issues, and deliver updates to users more frequently and reliably.

Tools and Technologies
- [Gitlab](#gitlab)
- [Docker](#docker)



# Class Separability {#class-separability}


If you have a perfectly balanced dataset (unlike [Imbalanced Datasets](#imbalanced-datasets)) but still experience poor [classification](#classification) [accuracy](#accuracy), class separability might be an issue due to the following reasons:

1. **Overlapping Classes**: The features of different classes may overlap significantly, making it difficult for the model to distinguish between them. If the decision boundaries are not well-defined, the model may struggle to classify instances correctly.

2. **Complex Decision Boundaries**: The underlying relationship between the features and the classes may be complex, requiring a more sophisticated model to capture the nuances. If the model is too simple, it may not be able to learn the necessary patterns.

3. **Noise in the Data**: If there is a significant amount of noise or irrelevant features in the dataset, it can obscure the true signal, leading to poor classification performance despite balanced class representation.

4. **Insufficient Feature Representation**: The features used for classification may not adequately represent the underlying characteristics that differentiate the classes. This can lead to a lack of separability, even in a balanced dataset.

5. **Model Overfitting or Underfitting**: If the model is overfitting, it may perform well on training data but poorly on unseen data. Conversely, if it is underfitting, it may not capture the complexity of the data, leading to poor accuracy.

Addressing these issues may require exploring different [feature engineering](#feature-engineering) techniques, selecting more appropriate models, or adjusting hyperparameters to improve class separability.

# Classification Report {#classification-report}

The `classification_report` function in `sklearn.metrics` is used to evaluate the performance of a classification model. It provides a summary of key metrics for each class, including precision, recall, F1-score, and support.

## Function Signature

```python
sklearn.metrics.classification_report(
    y_true, 
    y_pred, 
    , 
    labels=None, 
    target_names=None, 
    sample_weight=None, 
    digits=2, 
    output_dict=False, 
    zero_division='warn'
)
```
Parameters:
- `y_true`: Array of true labels.
- `y_pred`: Array of predicted labels.
- `labels`: (Optional) List of label indices to include in the report.
- `target_names`: (Optional) List of string names for the labels.
- `sample_weight`: (Optional) Array of weights for each sample.
- `digits`: Number of decimal places for formatting output.
- `output_dict`: If `True`, return output as a dictionary.
- `zero_division`: Sets the behavior when there is a zero division (e.g., 'warn', 0, 1).

### Metrics Explained

- [Precision](#precision): The ratio of correctly predicted positive observations to the total predicted positives. It indicates the quality of the positive class predictions.
  
- [Recall](#recall) (Sensitivity): The ratio of correctly predicted positive observations to all actual positives. It measures the ability of a model to find all relevant cases.

- [F1 Score](#f1-score): The weighted average of precision and recall. It is a better measure than accuracy for imbalanced classes.

- Support: The number of actual occurrences of the class in the specified dataset.

## Resources

In [ML_Tools](#ml_tools) see: [Evaluation_Metrics.py](#evaluation_metricspy)

[official documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html).


![Pasted image 20240404163858.png|500](../content/images/Pasted%20image%2020240404163858.png|500)




# Classification {#classification}


Classification is a type of [Supervised Learning](#supervised-learning) in machine learning, where the algorithm learns from labeled data to predict which category or class a new, unlabeled data point belongs to. The goal is to assign the correct label to input data based on patterns learned from the training set.

## Examples of Classifiers

Classifier: A model used for classification tasks, predicting discrete labels or categories. For example, determining whether an email is spam or not, or identifying the species of a flower based on its features. This contrasts with a Regressor ([Regression](#regression)), which predicts continuous values.

[Naive Bayes](#naive-bayes)

[Decision Tree](#decision-tree)

[Support Vector Machines](#support-vector-machines)

[K-nearest neighbours](#k-nearest-neighbours)

[Neural network](#neural-network)

[Model Ensemble](#model-ensemble)

## Choosing a Classifier Algorithm

1. Data Characteristics: Some algorithms work better on structured data, while others perform better on unstructured data.
2. Problem Complexity: Simple classifiers for straightforward problems, complex models for intricate tasks.
3. Model Performance: Consider accuracy and speed requirements.
4. Model [Interpretability](#interpretability): Some models, like decision trees, are easier to interpret, while others, like neural networks, can be more challenging.
5. Model Scalability: Large datasets need scalable models like SVM or Naive Bayes.
6. Model Flexibility: Algorithms like KNN are flexible when the data distribution is unknown.
## Use Cases of Classification

1. Object Recognition: Classifying objects in images (e.g., identifying a cat or a dog).
2. Spam Filtering: Classifying emails as either spam or legitimate.
3. Medical Diagnosis: Using patient symptoms and test results to classify diseases.


# Claude {#claude}


Claude is better for code and uses Artifact for tracking code changes.

Claude is crazy see: https://youtu.be/RudrWy9uPZE?t=473

# Cleaning Terminal Path {#cleaning-terminal-path}


https://www.youtube.com/watch?v=18hUejOK0qk

```cmd
prompt $g
```

### powershell
```powershell
$profile

microsfot_Powershell_profile have

function prompt{
$p = -path
"$p> "
}
```

getting the script working 

https://stackoverflow.com/questions/41117421/ps1-cannot-be-loaded-because-running-scripts-is-disabled-on-this-system



# Click_Implementation.Py {#click_implementationpy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Click_Implementation.py

This script implements a command-line interface (CLI) tool using Python's `click` library. The CLI allows users to interact with a JSON file, enabling them to view keys, retrieve values, and update key-value pairs.

## Functionality Overview

1. CLI Initialization (`cli` function)
    - Serves as the main command group.
    - Accepts a JSON file (`document`) as an argument.
    - Reads the JSON file and stores its content in `ctx.obj`, making it accessible to all subcommands.

2. Displaying Keys (`show_keys` command)
    - Lists all top-level keys in the JSON document.

3. Retrieving a Value (`get_value` command)
    - Accepts a key as an argument.
    - Prints the corresponding value if the key exists; otherwise, prints `"Key not found"`.

4. Updating a Value (`update_value` command)
    - Requires `-k/--key` (key to update) and `-v/--value` (new value).
    - Updates the key’s value in memory.
    - Saves the updated JSON data back to the file.

## Example Usage

### 1. Viewing Keys

```sh
python script.py data.json show_keys
```

Example Output (if `data.json` contains `{"name": "Alice", "age": 30}`):

```
Keys: ['name', 'age']
```

### 2. Retrieving a Value

```sh
python script.py data.json get_value name
```

Output:

```
Alice
```

### 3. Updating a Value

```sh
python script.py data.json update_value -k name -v Bob
```

Modifies `data.json` to:

```json
{
    "name": "Bob",
    "age": 30
}
```



# Cloud Providers {#cloud-providers}


Among the biggest cloud providers are [AWS](https://aws.amazon.com/), [Microsoft Azure](https://azure.microsoft.com/), [Google Cloud](https://cloud.google.com/). 

Whereas [Databricks](#databricks) ( [Databrick](https://www.databricks.com/)) and [Snowflake](https://www.snowflake.com/) provide dedicated [Data Warehouse](#data-warehouse)and [Data Lakehouse|Lakehouse](#data-lakehouselakehouse) solutions

## Features

[Scaling Server](#scaling-server)
[Load Balancing](#load-balancing)
[Memory Caching](#memory-caching)


# Clustering {#clustering}


Clustering involves grouping a set of data points into subsets or clusters based on inherent patterns or similarities. It is an [Unsupervised Learning](#unsupervised-learning)technique used for tasks like customer segmentation and [standardised/Outliers|anomalies](#standardisedoutliersanomalies) detection. The primary goal of clustering is to organize data by grouping similar items.

## Applications

- Customer Segmentation: Group customers with similar purchasing behavior or demographics for targeted marketing.
- Image Segmentation: Group pixels in an image based on color or texture to identify objects or regions.
- [Anomaly Detection](#anomaly-detection): Identify clusters of normal behavior to detect anomalies that deviate significantly from these clusters.
## Methods

- [K-means](#k-means)
- [DBScan](#dbscan)
- [Hierarchical Clustering](#hierarchical-clustering)
- [Gaussian Mixture Models](#gaussian-mixture-models)

## [Interpretability](#interpretability)

 [Feature Scaling](#feature-scaling): Essential for bringing features to the same scale, as clusters may appear distorted without it.
  
```python
  from sklearn.preprocessing import scale
  from sklearn.preprocessing import MinMaxScaler
  
```

Use clustering to find [Correlation](#correlation) between features. Utilize a [Dendrograms](#dendrograms) to visualize the relationship between features.

# Clustering_Dashboard.Py {#clustering_dashboardpy}


https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Clustering_Dashboard.py

# Clustermap {#clustermap}

## Clustermap

Related to:
[Preprocessing|Preprocess](#preprocessingpreprocess)

- Purpose: Identify which features are most similar using [Dendrograms](#dendrograms).
- Visualization: Regions of color show clustering, similar to a heatmap.
- Functionality: Performs clustering on both rows and columns.

Requirements: Input should be numerical; data needs to be scaled.
  
```python
  import seaborn as sns
  sns.clustermap(x_scaled, cmap='mako', standard_scale=0)  # 0 for rows, 1 for columns
  
```
## Resources
- [Video Explanation](https://youtu.be/crQkHHhY7aY?t=149)
- [Seaborn Clustermap Documentation](https://seaborn.pydata.org/generated/seaborn.clustermap.html)

# Code Diagrams {#code-diagrams}

[Documentation & Meetings](#documentation--meetings)

There are class diagrams showing the hierarchy of classes [Classes](#classes) (Object orientated). 

Done in [Mermaid](#mermaid).

Overall [Architecture Diagram](#architecture-diagram): showing how software components interact.

[Sequence diagram](#sequence-diagram) of componets interact. 

Sequence diagraph: how the componets interact 
Architecture diagram : main componts fitting together



# Columnar Storage {#columnar-storage}

A database storage technique that stores <mark>data by columns</mark> rather than rows, 

Useful for read-heavy operations and <mark>large-scale data analytics</mark>, as it enables the retrieval of specific columns without the need to access the entire row. 

Columnar Storage Example (Analytical Workloads)**:

| `order_id`  | `customer_id` | `order_date` | `order_amount` |
|-------------|---------------|--------------|----------------|
| 1           | 101           | 2024-10-01   | $100           |
| 2           | 102           | 2024-10-02   | $150           |
| 3           | 103           | 2024-10-03   | $200           |
In **columnar storage**, the data would be stored by columns, like:
- `customer_id`: [101, 102, 103]

If you're querying for the total sales (`order_amount`) in a specific period, only the `order_amount` and `order_date` columns are accessed. 


Use case: **Data Analytics/OLAP (Online Analytical Processing)**
- Running a query to get the **total sales for October** only needs to scan the `order_amount` and `order_date` columns, rather than scanning entire rows, faster [Querying](#querying)

# Command Line {#command-line}


The command line is a text-based interface used to interact with a computer's operating system or software. It allows users to execute commands, run scripts, and perform various tasks.

[PowerShell](#powershell)

[Powershell vs Bash](#powershell-vs-bash)

[Bash](#bash)

[Command Prompt](#command-prompt)

# Command Prompt {#command-prompt}


Command Prompt (cmd) is a command-line interpreter on Windows systems that allows users to execute commands to perform various basic tasks. Below are some common tasks that can be performed in cmd, along with examples:

Related to:
- [Bash](#bash)

## 1. Navigating the File System

- **Changing Directories:**
  
```cmd
  cd C:\path\to\directory
  
```
  Changes the current directory to `C:\path\to\directory`.

- **Listing Files and Directories:**
  
```cmd
  dir
  
```
  Lists the files and directories in the current directory.

## 2. Managing Files and Directories

- **Creating a Directory:**
  
```cmd
  mkdir newfolder
  
```
  Creates a new directory named `newfolder`.

- **Deleting a Directory:**
  
```cmd
  rmdir /s /q newfolder
  
```
  Deletes the directory `newfolder` and its contents. The `/s` flag removes all directories and files in the specified directory, and the `/q` flag runs the command quietly without asking for confirmation.

- **Copying Files:**
  
```cmd
  copy C:\source\file.txt D:\destination\
  
```
  Copies `file.txt` from the `C:\source` directory to the `D:\destination` directory.

- **Renaming Files:**
  
```cmd
  ren oldfile.txt newfile.txt
  
```
  Renames `oldfile.txt` to `newfile.txt`.

- **Deleting Files:**
  
```cmd
  del file.txt
  
```
  Deletes `file.txt`.

## 3. Viewing and Managing System Information

- **Viewing IP Configuration:**
  
```cmd
  ipconfig
  
```
  Displays the current network configuration.

- **Viewing System Information:**
  
```cmd
  systeminfo
  
```
  Provides detailed system information including OS version, hardware details, and network configurations.

## 4. Managing Processes

- **Viewing Running Processes:**
  
```cmd
  tasklist
  
```
  Lists all currently running processes.

- **Killing a Process:**
  
```cmd
  taskkill /F /PID 1234
  
```
  Terminates the process with the Process ID (PID) `1234`. The `/F` flag forces the process to terminate.

## 5. Networking Commands

- **Pinging a Server:**
  
```cmd
  ping www.example.com
  
```
  Sends ICMP Echo Request packets to the specified host and displays the response.

- **Tracing Route to a Server:**
  
```cmd
  tracert www.example.com
  
```
  Traces the route packets take to the specified host.

## 6. Batch File Scripting

- **Creating and Running a Simple Batch File:**
  - Create a file named `example.bat` with the following content:
    
```cmd
    @echo off
    echo Hello, World!
    pause
    
```
  - Run the batch file:
    
```cmd
    example.bat
    
```
  This batch file prints "Hello, World!" to the console and waits for the user to press a key before closing.

## 7. Environment Variables

- **Viewing Environment Variables:**
  
```cmd
  set
  
```
  Displays all current environment variables and their values.

- **Setting an Environment Variable:**
  
```cmd
  set MYVAR=Hello
  
```
  Sets an environment variable `MYVAR` with the value `Hello`.

## 8. Disk Operations

- **Checking Disk Usage:**
  
```cmd
  chkdsk C:
  
```
  Checks the file system and file system metadata of the C: drive for logical and physical errors.

- **Formatting a Disk:**
  
```cmd
  format D: /FS:NTFS
  
```
  Formats the D: drive with the NTFS file system.

## 9. Echoing Messages

- **Displaying a Message:**
  
```cmd
  echo Hello, World!
  
```
  Prints `Hello, World!` to the console.

## 10. Redirecting Output

- **Redirecting Command Output to a File:**
  
```cmd
  dir > output.txt
  
```
  Redirects the output of the `dir` command to `output.txt`.

These examples illustrate some of the basic functionalities of Command Prompt. While cmd is less powerful compared to [PowerShell](#powershell), it remains useful for simple file system navigation, file management, and running legacy scripts.



# Common Security Vulnerabilities In Software Development {#common-security-vulnerabilities-in-software-development}


[Security](#security) vulnerabilities can be encountered and mitigated in [Software Development Portal](#software-development-portal).

In this not describe potential security risks in their applications.

Useful Tools
- [tool.bandit](#toolbandit)
## Examples

### Command Injection

General Description: Command injection is a security vulnerability that occurs when an attacker is able to execute arbitrary commands on the host operating system via a vulnerable application. This typically happens when user input is improperly handled and passed to a system shell.

Example: 
The `dangerous_subprocess` function uses `subprocess.call` with `shell=True`, which can lead to command injection if user input is not properly sanitized.
  
```python
  import subprocess
  def dangerous_subprocess(user_input):
      subprocess.call(user_input, shell=True)
  
```
Mitigation:
  - Avoid using `subprocess.call` with `shell=True`. Use `subprocess.run` or `subprocess.call` with a list of arguments.
  - Validate and sanitize user inputs ([Input is Not Properly Sanitized](#input-is-not-properly-sanitized)). 

### Hardcoded Password

General Description: Hardcoded passwords refer to credentials that are embedded directly in the source code. This practice is insecure as it exposes sensitive information and makes it difficult to change passwords without modifying the code.

Example:
The `hardcoded_password` function contains a hardcoded password, which is a common security issue.
  
```python
  def hardcoded_password():
      password = "123456"
      return password
  
```
  
Mitigation:
  - Use environment variables or configuration files to store sensitive information.
  - Consider using a secrets management tool.

### Use of `eval`

General Description: The `eval` function in Python evaluates a string as a Python expression. If not properly controlled, it can execute arbitrary code, leading to security vulnerabilities.

Example:
  - The `unsafe_eval` function uses `eval`, which can execute arbitrary code if the input is not controlled.
  
```python
  def unsafe_eval(user_input):
      return eval(user_input)
  
```
Mitigation:
  - Avoid using `eval`. Use safer alternatives like `ast.literal_eval`.
  - Ensure input is strictly controlled and sanitized.

### Insecure Deserialization

General Description: Insecure deserialization occurs when untrusted data is used to reconstruct objects. This can lead to arbitrary code execution, data tampering, or other malicious activities.

Example:
  - The `insecure_deserialization` function uses `pickle.loads`, which can be exploited if untrusted data is deserialized.
  
```python
  import pickle
  def insecure_deserialization(data):
      return pickle.loads(data)
  
```
Mitigation:
  - Avoid using `pickle` for untrusted data. Use safer formats like JSON ([Why JSON is Better than Pickle for Untrusted Data](#why-json-is-better-than-pickle-for-untrusted-data)).
  - Ensure data is from a trusted source.

### [SQL Injection](#sql-injection)

### Cross-Site Scripting (XSS)

**General Description**: Cross-Site Scripting (XSS) is a security vulnerability that allows an attacker to inject malicious scripts into content from otherwise trusted websites. It occurs when an application includes untrusted data in a web page without proper validation or escaping.

**Example**:
- The `display_user_input` function directly inserts user input into HTML, which can lead to XSS if the input is not properly sanitized.
```html
<div>
	<%= user_input %>
</div>
```
**Mitigation**:
- Escape user input before rendering it in HTML.
- Use security libraries or frameworks that automatically handle escaping

# Common Table Expression {#common-table-expression}


A Common Table Expression (CTE) is a temporary named result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement. 

The CTE can also be used in a [Views](#views). Serve as temporary views for a single [Querying|Queries](#queryingqueries).

```sql
WITH cte_query AS
(SELECT … subquery ...)
SELECT main query ... FROM/JOIN with cte_query ...
```

In [DE_Tools](#de_tools) see:
- https://github.com/rhyslwells/DE_Tools/blob/main/ExplorationsSQLite/Utilities/Common_Table_Expression.ipynb
### Non-Recursive CTE

The non-recursive are simple where CTE is used to <mark>avoid SQL duplication</mark> by referencing a name instead of the actual SQL statement. See [Views](#views) simplification usage.

```sql
WITH avg_per_store AS
  (SELECT store, AVG(amount) AS average_order
   FROM orders
   GROUP BY store)
SELECT o.id, o.store, o.amount, avg.average_order AS avg_for_store
FROM orders o
JOIN avg_per_store avg
ON o.store = avg.store;
```

### Recursive CTE

CTEs can be used in [Recursive Algorithm](#recursive-algorithm). The recursive query calls itself until the query satisfied the condition. In a recursive CTE, we should provide a where condition to terminate the recursion.

A recursive CTE is useful in querying hierarchical data such as organization charts where one employee reports to a manager or multi-level bill of materials when a product consists of many components, and each component itself also consists of many other components.

```sql
WITH levels AS (
  SELECT
    id,
    first_name,
    last_name,
    superior_id,
    1 AS level
  FROM employees
  WHERE superior_id IS NULL
  UNION ALL
  SELECT
    employees.id,
    employees.first_name,
    employees.last_name,
    employees.superior_id,
    levels.level + 1
  FROM employees, levels
  WHERE employees.superior_id = levels.id
)
 
SELECT *
FROM levels;
```


# Communication Principles {#communication-principles}


![Pasted image 20240916075433.png](../content/images/Pasted%20image%2020240916075433.png)

![Pasted image 20240916075439.png](../content/images/Pasted%20image%2020240916075439.png)



# Communication Techniques {#communication-techniques}


## Overview

Using these structured communication bridges can  enhance clarity and engagement, especially in spontaneous or high-stakes discussions.

Tips for Using Communication Bridges
1. Start Small: Begin by integrating 2-3 bridges that feel natural to you.
2. Observe Reactions: Notice how listeners respond when you clarify changes, summarize key points, or highlight actions.
3. Practice Consistency: Make these bridges a regular part of your speaking style.

[Speak More Clearly: 8 Precise Steps to Improve Communication](https://www.youtube.com/watch?v=Tc5dCLE_GP0)

### 1. Context Bridge

- Purpose: Aligns everyone by setting the context before diving into details.
- How to Use: Start with phrases like:
  - "At a high level..."
  - "This is our goal..."
  - <mark>"The main problem is..."</mark>
- Effect: Helps to focus thoughts and prevents initial rambling.

### 2. Change Bridge

- Purpose: Emphasizes shifts, trends, or significant moments in the discussion.
- How to Use: Use phrases that highlight changes, such as:
  - "Here's the before, and here's the after..."
  - "We’re shifting from X to Y..."`
  - "We are at a tipping point..."
- Effect: Grabs attention by making the change clear.

### 3. Insight Bridge

- Purpose: Shares deeper insights or unique perspectives, creating "aha" moments.
- How to Use: Key phrases include:
  - "Counterintuitively, ..."
  - "Here's what most people miss..."
  - "The deeper insight is..."
  - "The key point here is..."
- Effect: Signals that you’ve thought deeply, which moves the conversation forward.

### 4. Analysis Bridge

- Purpose: Anchors discussion in evidence, keeping it grounded.
- How to Use: Reference specific data points or comparisons with:
  - "The evidence shows..."
  - "The data indicates..."
  - "When we compared X and Y..."
- Effect: Focuses on facts, minimizing loss of direction.

### 5. Logical Transition Bridge

- Purpose: Provides a clear flow in the conversation, avoiding confusion.
- How to Use: Classic transitions include:
  - "First, second, third..."
  - "This leads to..."
  - "On the other hand..."
- Effect: Helps listeners follow along without losing the thread.

### 6. Summary Bridge

- Purpose: Ensures that key messages stay clear, especially in long discussions.
- How to Use: Frequently summarize main points with phrases like:
  - "The bottom line is..."
  - <mark>"If you remember one thing, it’s this..."</mark>
  - "To bring it back to the goal..."
- Effect: Reinforces the main message throughout the discussion.

### 7. Refinement Bridge

- Purpose: Allows for clarification or expansion of ideas as needed.
- How to Use: Rephrase or elaborate with:
  - "Let me break this down further..."
  - "Another way of looking at it is..."
  - "A useful analogy might be..."
- Effect: Clarifies complex points, helping everyone understand the core message.

### 8. Action Bridge

- Purpose: Concludes with actionable steps, defining the next moves.
- How to Use: Conclude with statements like:
  - <mark>"Our immediate priority is..."</mark>
  - "Here’s what we’ll do next..."
  - "The deliverables are..."
- Effect: Ends discussions with clear direction and accountability.


# Comparing Llm {#comparing-llm}


Use lmarena.ai as a bench marking tool. 

[LLM](#llm)

web dev arena

text to image leader board



# Components Of The Database {#components-of-the-database}

[Fact Table](#fact-table) in main table that [Dimension Table](#dimension-table) connect to them.

![Obsidian_CSP0FnAVD1.png](../content/images/Obsidian_CSP0FnAVD1.png)

# Computer Science {#computer-science}


[Algorithms](#algorithms)



# Concatenate {#concatenate}



# Conceptual Data Model {#conceptual-data-model}



# Conceptual Model {#conceptual-model}

Conceptual Model
   - Entities: Customer, Order, Book
   - Relationships: Customers place Orders, Orders include Books



Conceptual Model
   - Focuses on high-level business requirements.
   - Defines important data entities and their relationships.
   - Tools: [ER Diagrams](#er-diagrams), ER Studio, DbSchema.

# Concurrency {#concurrency}

In [DE_Tools](#de_tools) see:
- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Transactions/Concurrency.ipynb

# Confidence Interval {#confidence-interval}


A confidence interval is a range of values, derived from sample data, that is likely to contain the true population parameter. It is associated with a confidence level, such as 95%, indicating the probability that the interval captures the true parameter.

Key Points
- **Confidence Level:** The likelihood that the interval includes the true parameter (e.g., 95%).
- **Purpose:** Quantifies the uncertainty of an estimate, providing a range rather than a single value.
### Example
- A 95% confidence interval for a mean of (50, 60) suggests that, in repeated sampling, 95% of such intervals would contain the true mean.


# Confusion Matrix {#confusion-matrix}


A Confusion Matrix is a table used to evaluate the performance of a [Classification](#classification) model. It provides a detailed breakdown of the model's predictions across different classes, showing the number of true positives, true negatives, false positives, and false negatives.
## Purpose

- The confusion matrix helps identify where the classifier is making errors, indicating where it is "confused" in its predictions.
## Structure


![Pasted image 20240120215414.png](../content/images/Pasted%20image%2020240120215414.png)

## Structure

- True Positives (TP): Correctly predicted positive instances.
- False Positives (FP): Incorrectly predicted positive instances (Type 1 error).
- True Negatives (TN): Correctly predicted negative instances.
- False Negatives (FN): Incorrectly predicted negative instances (Type 2 error).

## Metrics

- [Accuracy](#accuracy): The overall percentage of correct predictions. In this case, the accuracy is 78.3%.
- [Precision](#precision): The ratio of true positives to all positive predictions (including both TPs and FPs). In this case, the precision for class 0 is 85.7% and the precision for class 1 is 66.4%.
- [Recall](#recall): The ratio of true positives to all actual positive cases (including both TPs and FNs). In this case, the recall for class 0 is 80.6% and the recall for class 1 is 74.1%.
- [F1 Score](#f1-score): A harmonic average of precision and recall. In this case, the F1-score for class 0 is 83.0% and the F1-score for class 1 is 70.0%.
- [Specificity](#specificity)
- [Recall](#recall)

## Further Examples
![Pasted image 20240116205937.png|500](../content/images/Pasted%20image%2020240116205937.png|500)

![Pasted image 20240116210541.png|500](../content/images/Pasted%20image%2020240116210541.png|500)

## Example Code

```python
from sklearn.metrics import confusion_matrix

# Assuming y_train and y_train_pred are your true and predicted labels
conf_matrix = confusion_matrix(y_train, y_train_pred)
print(conf_matrix)
```

Example Output:

```
array([[377, 63],
       [ 91, 180]], dtype=int64)
```

# Continuous Delivery   Deployment {#continuous-delivery---deployment}

Continuous Delivery
   - Ensures that code changes are automatically prepared for a release to production.
   - Builds, tests, and releases are automated, but the deployment is manual.

Continuous Deployment:
   - Extends continuous delivery by automating the deployment process.
   - Every change that passes the automated tests is deployed to production automatically.

[Model Deployment](#model-deployment)

A continuous integration and continuous deployment (CI/CD) pipeline is **a series of steps that must be performed in order to deliver a new version of software**

# Continuous Integration {#continuous-integration}

   - Developers frequently integrate code into a shared repository.
   - Automated builds and tests are run to detect issues early.
   - Encourages smaller, more manageable code changes.

# Converting Categorical Variables To A Dummy Indicators {#converting-categorical-variables-to-a-dummy-indicators}



# Convolutional Neural Networks {#convolutional-neural-networks}


Convolutional networks, or CNNs, are specialized [Deep Learning](#deep-learning) architectures designed for processing data with grid-like structures, such as images. 

They use convolutional layers with learnable filters to extract spatial features from the input data. The convolutional operation involves sliding these filters across the input, performing element-wise multiplications and summations to create feature maps. 

CNNs are particularly effective for image classification, object detection, and image segmentation tasks.

Primarily used in image recognition and processing tasks. CNNs use convolutional layers to automatically detect spatial patterns in images, like edges and textures.

Pooling:

The idea of pooling in convolutional neural networks is to do two things:
- Reduce the number of parameters in your network (pooling is also called “down-sampling” for this reason)
- To make feature detection ([Feature Extraction](#feature-extraction)) more robust by making it more impervious to scale and orientation changes
- shrink multiple data to single points.

![Pasted image 20241006124829.png|500](../content/images/Pasted%20image%2020241006124829.png|500)

![Pasted image 20241006124735.png|500](../content/images/Pasted%20image%2020241006124735.png|500)


# Correlation Vs Causation {#correlation-vs-causation}

What is the meaning of [Correlation](#correlation) does not imply causation?

Correlation measures the statistical association between two variables, while causation implies a cause-and-effect relationship. 


- **Correlation**: Indicates an association between variables but does not imply that changes in one variable cause changes in the other.
- **Causation**: Suggests a direct cause-and-effect relationship between variables, requiring experimentation to establish.


# Correlation {#correlation}


Use in understanding relationships between variables in data analysis. 

While it helps identify associations, it's important to remember that <mark>correlation does not imply causation.</mark> 

Visualization tools like heatmaps and clustering can aid in identifying and interpreting these relationships effectively.

- What is Correlation?: A measure of the strength and direction of the relationship between two variables.
### Description

- Correlation measures the relationship between two variables, indicating how they change together. It ranges from -1 to 1:

  - -1: Perfect negative correlation
  - 0: No correlation
  - 1: Perfect positive correlation

### Key Points

- [Correlation vs Causation](#correlation-vs-causation): Correlation does not imply causation. While correlation highlights associations, causation establishes a direct influence.
- Significance: Correlation values < -0.5 or > +0.5 are considered significant.
- Impact of Outliers: [standardised/Outliers](#standardisedoutliers) can distort correlation results.
- Standardization: Correlation is a standardized version of [Covariance](#covariance).

### Model Preparation

[Feature Selection](#feature-selection):
  - Identify features correlated with the target. If all are correlated, keep all.
  - For features correlated with each other, consider dropping one to avoid redundancy.
  - If two features are highly correlated with the target, both can be retained.

If two variables are strongly positively correlated, it often makes sense to drop one of them to simplify the model. This is because <mark>highly correlated variables can introduce redundancy</mark>, leading to [multicollinearity](#multicollinearity) in regression models.

By removing one of the correlated variables, you can:

1. Reduce Complexity: Simplifying the model by reducing the number of predictors can make it easier to interpret and manage.
2. Improve Stability: Reducing multicollinearity can lead to more stable and reliable coefficient estimates.
3. Enhance Performance: In some cases, removing redundant features can improve the model's predictive performance by reducing overfitting.

However, it's important to ensure that the variable you choose to keep is the one that is more relevant or has a stronger theoretical justification for inclusion in the model. 

### Viewing Correlations

- Use [Heatmap](#heatmap) or [Clustering](#clustering) to visualize correlations between features.

### Example Code

To find the correlation between two features:

```python
df['var1', 'target'](#var1-target).groupby(['var1'], as_index=False).mean().sort_values(by='target', ascending=False)
```


# Cosine Similarity {#cosine-similarity}

Cosine similarity is a [Metric](#metric) used to measure how similar two vectors are by calculating the cosine of the angle between them. It ranges from -1 to 1.where 1 indicates identical orientation, 0 indicates orthogonality, and -1 indicates opposite orientation. 

Cosine similarity is commonly used in
- text analysis, 
- information retrieval, 
- recommendation systems to compare document similarity, user preferences, or item features.

In [Binary Classification](#binary-classification), cosine similarity can be used as a feature to help distinguish between two classes. For instance, in text classification tasks, you might represent documents as vectors using techniques like [TF-IDF](#tf-idf). 


# Cost Function {#cost-function}

The concept of a Cost Function is central to [Model Optimisation](#model-optimisation), particularly in training models.

A cost function, also known as a loss function or error function, is a mathematical function used in optimization and machine learning to measure the difference between predicted values and actual values. It quantifies the error or "cost" of a model's predictions. The goal of many machine learning algorithms is to minimize this cost function, thereby improving the accuracy of the model. Common examples of cost functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.

1. Relation to [Loss Function](#loss-function): The cost function is related to the loss function. While the loss function measures the error for a single data point, the cost function typically aggregates these errors over the entire dataset, often by taking an average. See [Loss versus Cost function](#loss-versus-cost-function).

3. Parameter Space ([Model Parameters](#model-parameters)) and Surface Plotting: By plotting the cost function over the parameter space, you can visualize how different parameter values affect the cost. This surface can have various peaks and valleys representing different levels of error.

4. [Gradient Descent](#gradient-descent): This is an optimization algorithm used to find the minimum of the cost function. By iteratively adjusting the parameters in the direction that reduces the cost, gradient descent helps in finding the optimal parameters for the model.

5. Caveats: The cost function is dependent on the dataset and may not always have an explicit formula. This means that the shape of the cost function surface can vary greatly depending on the data, and finding the global minimum can be challenging.



![Pasted image 20241216202825.png|500](../content/images/Pasted%20image%2020241216202825.png|500)

![Pasted image 20241216202917.png|500](../content/images/Pasted%20image%2020241216202917.png|500)


**[Reward Function](#reward-function)**: Mentioned as the opposite of a cost function, typically used in [Reinforcement learning](#reinforcement-learning) to indicate the desirability of an outcome.

# Covariance {#covariance}



In statistics, covariance is a measure of the degree to which two random variables change together. It indicates the direction of the linear relationship between the variables. Specifically, covariance can be defined as follows:

- **Positive Covariance**: If the covariance is positive, it means that as one variable increases, the other variable tends to also increase. Conversely, if one variable decreases, the other variable tends to decrease as well.
  
- **Negative Covariance**: If the covariance is negative, it indicates that as one variable increases, the other variable tends to decrease, and vice versa.

- **Zero Covariance**: A covariance close to zero suggests that there is no linear relationship between the two variables.

The formula for calculating the covariance between two random variables $X$ and $Y$ is given by:

$$
\text{Cov}(X, Y) = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})
$$

where:
- $X_i$ and $Y_i$ are the individual sample points,
- $\bar{X}$ and $\bar{Y}$ are the means of $X$ and $Y$ respectively,
- $n$ is the number of data points.

Covariance is used in:
- in the calculation of [correlation](#correlation) coefficients 
- and in multivariate statistics, such as in [Gaussian Mixture Models](#gaussian-mixture-models) where it helps describe the shape and orientation of the data distribution.

# Covering Index {#covering-index}

Like an [Database Index|Index](#database-indexindex) but for partial indexes?

# Cron Jobs {#cron-jobs}



# Cross Entropy {#cross-entropy}


Cross entropy is a [Loss function](#loss-function) used in [Classification](#classification) tasks, particularly for [categorical data](#categorical-data). The cross entropy loss function is particularly effective for multi-class classification problems, where the goal is to assign an input to one of several categories. 

<mark>Cross entropy measures confidence.</mark>

Cross entropy works by measuring the (difference/loss) <mark>dissimilarity between two probability distributions</mark>: the true distribution (actual class labels) and the predicted distribution (model's output probabilities). 

Fit of Predictions:
- A low cross entropy loss means the predicted probabilities are close to the true labels (e.g., assigning high probability to the correct class).
- A high loss indicates significant divergence, meaning the model's predictions are inaccurate or uncertain.

By minimizing cross entropy, the model learns to produce probability distributions that closely match the true class distributions, thereby improving its classification <mark>accuracy</mark>.

1. Probability Distributions: In a classification task, the model outputs a probability distribution over the possible classes for each input. For example, in a three-class problem, the model might output probabilities like [0.7, 0.2, 0.1] for classes A, B, and C, respectively.

2. True Labels: The true class label is represented as a one-hot encoded vector. If the true class is A, the vector would be [1, 0, 0].

3. Cross Entropy Calculation calculates the loss by comparing the predicted probabilities with the true labels. The formula for cross entropy loss $L$ for a single instance is:

   $$ L = -\sum_{i=1}^{N} y_i \log(p_i)$$

   where:
   - $N$ is the number of classes.
   - $y_i$ is the true label (1 if the class is the true class, 0 otherwise).
   - $p_i$ is the predicted probability for class $i$.

2. Interpretation: The cross entropy loss increases as the predicted probability diverges from the actual label. If the model assigns a high probability to the correct class, the loss is low. Conversely, if the model assigns a low probability to the correct class, the loss is high.

3. Optimization: During training, the model's parameters are adjusted to minimize the cross entropy loss across all training examples. This process helps the model improve its predictions over time.

## Where is it used

Cross entropy is widely used in classification for several reasons:

Probabilistic Modeling:
    - It directly aligns with the goals of probabilistic classifiers, as it measures how well the predicted probability distribution matches the true distribution.
    
Focus on Confidence:
    - Encourages the model to assign higher probabilities to the correct classes, improving not just accuracy but also confidence in predictions.

Optimization Efficiency:
    - Cross entropy is smooth and convex for logistic regression-like models, enabling efficient gradient-based optimization.

Multi-Class Support:
    - Works seamlessly in multi-class scenarios where the true labels are one-hot encoded and predictions are probability distributions.

### Implementation 

In [ML_Tools](#ml_tools) see: 
- [Cross_Entropy_Single.py](#cross_entropy_singlepy)
- [Cross_Entropy.py](#cross_entropypy)
- [Cross_Entropy_Net.py](#cross_entropy_netpy)





# Cross Validation {#cross-validation}


Cross-validation is a statistical technique used in machine learning to <mark>assess how well a model will generalize</mark> to an independent dataset. It is a crucial step in the model-building process because it helps ensure that the model is not [overfitting](#overfitting) or underfitting the training data.

- Cross-validation is a technique used in machine learning and statistics to evaluate the performance ([Model Optimisation](#model-optimisation)) of a predictive model.
- It provides a robust evaluation by splitting the training data into smaller chunks and training the model multiple times.
- K-Fold Cross-Validation: Involves dividing the dataset into \( k \) equal-sized subsets (called "folds") and using each fold as a validation set once, while the remaining \( k-1 \) folds are used for training.
- The model's performance is averaged across all \( k \) folds to provide a more robust estimate of its generalization performance.
### Common Variations

- K-Fold Cross-Validation: The most common method, where the data is split into \( k \) folds and the model is trained \( k \) times, each time using a different fold as the validation set.
- Stratified K-Fold: Ensures each fold has a similar proportion of class labels, important for imbalanced datasets.
- Repeated K-Fold: Repeats the process multiple times with different random splits for more robust results.
- Leave-One-Out Cross-Validation (LOOCV): Each data point is used once as a test set while the rest serve as the training set.

### How Cross-Validation Fits into Building a Machine Learning Model

1. [Model Evaluation](#model-evaluation): Used to evaluate the performance of different models or algorithms to choose the best one.
2. [Hyperparameter](#hyperparameter) Tuning: Provides a reliable performance metric for each set of hyperparameters.
3. [Model Validation](#model-validation): Ensures consistent performance across different subsets of data.
4. [Bias and variance](#bias-and-variance) tradeoff: Helps in understanding the tradeoff between bias and variance, guiding the choice of model complexity.

Advantages:
- Reduced Bias: Offers a more reliable performance estimate compared to using a single validation set.
- Efficient Data Use: All data is used for both training and validation.
- Prevents Overfitting: By evaluating on multiple folds, it can detect if the model is overfitting to the training data.
### Choosing \( k \)

- Common values: 5 or 10
- Higher \( k \) leads to more accurate estimates but increases computation time.
- Consider dataset size and complexity when choosing \( k \).

### Code Implementation

In [ML_Tools](#ml_tools) see:
- [KFold_Cross_Validation.py](#kfold_cross_validationpy)

### Cross-Validation Strategy in [Time Series](#time-series)

All notebooks use cross-validation based on `TimeSeriesSplit` to ensure proper evaluation of performance with no [Data Leakage](#data-leakage). This method ensures that training and test data are split while maintaining the chronological order of the data.

# Cross_Entropy.Py {#cross_entropypy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Model_Evaluation/Classification/Cross_Entropy.py
### Generalized Script Description:

1. **Dataset**: Uses the Iris dataset from `sklearn` to classify flower species.
2. **Preprocessing**: One-hot encodes the target labels and splits the data into training and testing sets.
3. **Model**: Trains a multinomial logistic regression model to predict probabilities for each class.
4. **Cross Entropy Calculation**: Computes cross entropy loss for all predictions in the test set.
5. **Visualization**: Plots a histogram to show the distribution of loss values across the test samples.
6. **Summary Statistics**: Outputs mean, median, maximum, and minimum loss values for analysis.

This approach provides insight into the model's performance by analyzing the spread and typical values of cross entropy loss over multiple predictions.

### Strengths:

1. **Real-World Dataset**: The Iris dataset is well-known and intuitive, making it easier to follow and validate the results.
2. **Generalization**: The script calculates the cross entropy loss for multiple predictions, demonstrating the loss function in a real-world, multi-class classification scenario.
3. **Insights Through Visualization**: The histogram of losses provides a clear picture of how well the model performs across different test samples.
4. **Statistical Summary**: The inclusion of mean, median, max, and min loss values gives a quick overview of the model's performance.
5. **Numerical Stability**: The small epsilon value in the log computation ensures stability when dealing with probabilities close to zero.
6. **Reproducibility**: Using `sklearn`'s preprocessing and modeling tools ensures that the example is easy to replicate.

### Possible Enhancements:

1. **Alternative Models**: Incorporating another model (e.g., a neural network) could showcase the versatility of cross entropy in various settings.
2. **Analysis of Misclassifications**: Add a breakdown of where the model performed poorly and why (e.g., confusion matrix analysis).
3. **Feature Exploration**: Include visualizations or explanations of feature importance to show how the model makes decisions.
4. **Comparative Losses**: Compare cross entropy loss with other loss functions (e.g., mean squared error) to highlight its advantages in classification.

**Distribution Insights**:

- The histogram of loss values shows how well the model performs across the test dataset.
    - A **narrow distribution** around a low value suggests consistent, accurate predictions.
    - A **wide or skewed distribution** indicates variability in the model's performance, with some instances being predicted poorly.

### [Mean Squared Error](#mean-squared-error) versus [Cross Entropy](#cross-entropy)

- **When Comparison Makes Sense**:
    - MSE can highlight how "far off" the predicted probabilities are in terms of magnitude but doesn’t account for the probabilistic nature of classification tasks.
    - Comparing cross entropy with MSE can show:
        - How the model performs when considering confidence (cross entropy).
        - How the model performs when focusing on numerical proximity (MSE).
        
- **Insights Gained**:
    - If cross entropy is low but MSE is high, it might indicate that the model predicts probabilities close to the correct class but has poor numerical calibration for other classes.
    - If both are high, the model is likely underperforming across the board.


# Cross_Entropy_Single.Py {#cross_entropy_singlepy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Model_Evaluation/Classification/Cross_Entropy_Single.py
## Example

Let's consider a three-class classification problem with classes A, B, and C. Suppose we have a single data point with the true class label being A. The true label in one-hot encoded form would be [1, 0, 0].

Assume the model predicts the following probabilities for this data point:

- Probability of class A: 0.7
- Probability of class B: 0.2
- Probability of class C: 0.1

The predicted probability vector is [0.7, 0.2, 0.1].

To calculate the cross entropy loss for this example, we use the formula:

$L = -\sum_{i=1}^{N} y_i \log(p_i)$

Substituting the values:

- For class A: $y_1 = 1$ and $p_1 = 0.7$
- For class B: $y_2 = 0$ and $p_2 = 0.2$
- For class C: $y_3 = 0$ and $p_3 = 0.1$

The cross entropy loss $L$ is calculated as:

$L = -(1 \cdot \log(0.7) + 0 \cdot \log(0.2) + 0 \cdot \log(0.1))$

$L = -(\log(0.7))$

$L \approx -(-0.3567) = 0.3567$

So, the cross entropy loss for this example is approximately 0.3567. This value represents the penalty for the model's predicted probabilities not perfectly matching the true class distribution. The lower the loss, the better the model's predictions align with the true labels.

### Script Description:

1. **Cross Entropy Function**: Computes the cross entropy loss given true labels and predicted probabilities.
2. **True and Predicted Probabilities Visualization**: Bar plots display the true one-hot encoded labels and the predicted probability distribution.
3. **Cross Entropy Loss Calculation**: Prints the loss value for a sample data point.
4. **Loss Curve**: A line graph shows how the loss changes as the predicted probability for the true class increases.

# Crosstab {#crosstab}

Used to compute a simple cross-tabulation of two (or more) factors. It is particularly useful for computing frequency tables. Here's an example:

```python
# Sample DataFrame
df = pd.DataFrame({
    'Category': ['A', 'B', 'A', 'B', 'A'],
    'Subcategory': ['X', 'X', 'Y', 'Y', 'X']
})

# Cross-tabulation of 'Category' and 'Subcategory'
crosstab = pd.crosstab(df['Category'], df['Subcategory'])
print(crosstab)
```

Input
```
  Category Subcategory
0        A           X
1        B           X
2        A           Y
3        B           Y
4        A           X
```

Output:
```
Subcategory  X  Y
Category         
A            2  1
B            1  1
```

In [DE_Tools](#de_tools) see:
- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/reshaping.ipynb

# Crud {#crud}

Create,Read,Update,Delete.

# Cryptography {#cryptography}


Cryptography is the foundation of digital [Security](#security), enabling privacy and secure communication over the internet.

Examples are implemented in [Node.JS](#nodejs) (using `crypto` module) and are written in [JavaScript](#javascript).

Resources:
- [7 Cryptography Concepts EVERY Developer Should Know](https://www.youtube.com/watch?v=NuyzuNBFWxQ)
- https://fireship.io/lessons/node-crypto-examples/
## [Hash](#hash) (Chop and mix)

A hashing function takes an input of any length and outputs a fixed-length value, ensuring:

- The same input always produces the same output.
- It is computationally expensive to reverse the hash.
- It has a low probability of collisions.

### Create a Hash in Node.js

```javascript
const { createHash } = require('crypto');

function hash(str) {
    return createHash('sha256').update(str).digest('hex');
}

let password = 'hi-mom!';
const hash1 = hash(password);
console.log(hash1);

password = 'hi-mom';
const hash2 = hash(password);
console.log(hash1 === hash2 ? '✔️ Good password' : '❌ Password does not match');
```

## Salting

Salting strengthens hashes by appending a random string before hashing, preventing attacks using precomputed hash tables.

### Password Salt with Scrypt in Node.js

```javascript
const { scryptSync, randomBytes, timingSafeEqual } = require('crypto');

function signup(email, password) {
    const salt = randomBytes(16).toString('hex');
    const hashedPassword = scryptSync(password, salt, 64).toString('hex');
    users.push({ email, password: `${salt}:${hashedPassword}` });
}

function login(email, password) {
    const user = users.find(v => v.email === email);
    if (!user) return 'login fail!';
    
    const [salt, key] = user.password.split(':');
    const hashedBuffer = scryptSync(password, salt, 64);
    const match = timingSafeEqual(hashedBuffer, Buffer.from(key, 'hex'));
    return match ? 'login success!' : 'login fail!';
}

const users = [];
signup('foo@bar.com', 'pa$$word');
console.log(login('foo@bar.com', 'password'));
```

## HMAC (Hash-based Message Authentication Code)

HMAC combines a hash with a secret key, ensuring authenticity and integrity.

### HMAC in Node.js

```javascript
const { createHmac } = require('crypto');

const password = 'super-secret!';
const message = '🎃 hello jack';

const hmac = createHmac('sha256', password).update(message).digest('hex');
console.log(hmac);
```

## Symmetric Encryption

Symmetric encryption uses the same key to encrypt and decrypt data.

### Symmetric Encryption in Node.js

```javascript
const { createCipheriv, randomBytes, createDecipheriv } = require('crypto');

const message = 'i like turtles';
const key = randomBytes(32);
const iv = randomBytes(16);
const cipher = createCipheriv('aes256', key, iv);
const encryptedMessage = cipher.update(message, 'utf8', 'hex') + cipher.final('hex');

const decipher = createDecipheriv('aes256', key, iv);
const decryptedMessage = decipher.update(encryptedMessage, 'hex', 'utf-8') + decipher.final('utf8');
console.log(`Decrypted: ${decryptedMessage}`);
```

## Keypairs

Keypairs consist of a public key (shared) and a private key (kept secret) for secure communication.

### Generate an RSA Keypair in Node.js

```javascript
const { generateKeyPairSync } = require('crypto');

const { privateKey, publicKey } = generateKeyPairSync('rsa', {
  modulusLength: 2048,
  publicKeyEncoding: { type: 'spki', format: 'pem' },
  privateKeyEncoding: { type: 'pkcs8', format: 'pem' },
});

console.log(publicKey);
console.log(privateKey);
```

## Asymmetric Encryption

Asymmetric encryption encrypts with a public key and decrypts with a private key, securing communication over networks.

### RSA Encryption in Node.js

```javascript
const { publicEncrypt, privateDecrypt } = require('crypto');
const { publicKey, privateKey } = require('./keypair');

const secretMessage = 'Confidential message';
const encryptedData = publicEncrypt(publicKey, Buffer.from(secretMessage));
console.log(encryptedData.toString('hex'));

const decryptedData = privateDecrypt(privateKey, encryptedData);
console.log(decryptedData.toString('utf-8'));
```

## Signing

Signing verifies the authenticity of a message by hashing it and encrypting the hash with a private key.

### RSA Signing in Node.js

```javascript
const { createSign, createVerify } = require('crypto');
const { publicKey, privateKey } = require('./keypair');

const data = 'this data must be signed';
const signer = createSign('rsa-sha256');
signer.update(data);
const signature = signer.sign(privateKey, 'hex');
console.log(signature);

const verifier = createVerify('rsa-sha256');
verifier.update(data);
const isVerified = verifier.verify(publicKey, signature, 'hex');
console.log(isVerified);
```

# Current Challenges Within The Energy Sector {#current-challenges-within-the-energy-sector}


[Current challenges within the energy sector](#current-challenges-within-the-energy-sector) related to reinforcement learning and that can be progressed with recent technological advances



# Dagster {#dagster}


[Dagster](https://dagster.io/) is a [data orchestrator] focusing on data-aware scheduling that supports the whole development [Data Lifecycle Management](#data-lifecycle-management)  lifecycle, with integrated lineage and observability, a [declarative](#declarative) programming model, and best-in-class testability.

Key features are: 
- Manage your data assets with code
- A single pane of glass for your data platform 

# Dash {#dash}


**Dash** is an open-source framework for building interactive web applications using Python. 

It is particularly well-suited for data visualization and dashboard creation. 

Dash integrates  with popular libraries such as Plotly, Pandas, and NumPy, making it ideal for creating dynamic and interactive visualizations.

In [ML_Tools](#ml_tools) see [Clustering_Dashboard.py](#clustering_dashboardpy)

Key Components of Dash
1. **Dash App**: The main application instance, created using `dash.Dash(__name__)`.
2. **Dash HTML Components (`dash_html_components`)**: Provides wrappers for standard HTML elements (e.g., `html.Div`, `html.H1`).
3. **Dash Core Components (`dash_core_components`)**: Includes interactive UI components like graphs, dropdowns, sliders, and more (e.g., `dcc.Graph`, `dcc.Dropdown`).
4. **Callback Functions**: Used to make components interactive by linking inputs (user actions) to outputs (changes in the UI).
5. **[Plotly](#plotly) Integration**: Dash apps leverage Plotly for creating interactive visualizations.

# Dashboarding {#dashboarding}




[Dash](#dash)
[Streamlit.io](#streamlitio)



# Data Ai Education At Work {#data-ai-education-at-work}


### Introduction

Organizations are increasingly recognizing the importance of integrating data and AI learning into their people strategies. This involves practical steps to ensure employees are equipped with the necessary skills to leverage these technologies effectively.

Integrating data and AI education into organizational strategies is essential for maintaining competitiveness and fostering a culture of continuous learning. By addressing these areas, organizations can better prepare their workforce for the evolving technological landscape.

### Practical Steps for Integration

1. **Access to Training**:
   - Provide clear guidance on how to access training courses.
   - Offer details on accessing training funds and budgets.
   - Partner with training providers to offer relevant courses.

2. **Learning Resources**:
   - Collect and distribute clear and concise training materials.
   - Capture, document, and discuss use cases from staff experiences.
   - Encourage peer-to-peer learning and collaboration.

3. **Organizational Support**:
   - Align training with professional competencies.
   - Foster communication and collaboration with external partners.
   - Encourage staff to experiment with AI tools to enhance efficiency.

4. **Governance and Strategy**:
   - Establish governance and skill strategies before deploying AI.
   - Develop acceptable use policies for AI tools.
   - Recognize that adopting AI is a gradual process requiring leadership support.

### Fostering a Culture of Continuous Learning

1. **Leadership and Culture**:
   - Connect learning initiatives with employee incentives and pay.
   - Allow time and space for learning by alleviating workloads.
   - Protect training time and build networks to showcase use cases.

2. **Mindset and Adaptability**:
   - Promote digital literacy and adaptability among employees.
   - Encourage openness to new ideas and recognize skills beyond the technical team.

### Business Risks of Not Upskilling

1. **Competitive Disadvantage**:
   - Risk of losing competitive and productivity edges.
   - Potential loss of market differentiation as competitors advance.

2. **Staff Retention**:
   - Risk of losing skilled staff uncomfortable with new technologies.
   - Employees may move to companies at the cutting edge of AI.

3. **Operational Challenges**:
   - Inconsistencies in ways of working between partner organizations.
   - Inappropriate use of AI tools by untrained staff.

4. **Productivity and Trust**:
   - AI's potential to enhance productivity is yet to be fully realized.
   - Trust and verifiability of AI systems (black boxes) are crucial for business benefits.


# Data Analysis Portal {#data-analysis-portal}

[Data Analyst](#data-analyst)

[Data Visualisation](#data-visualisation)

# Data Analysis {#data-analysis}

What is it? Usually done with a [Data Analyst](#data-analyst).After processing, data is analyzed to extract meaningful insights and derive value from the data.
### Types of analysis:

Exploration and understanding:
- [EDA](#eda): Involves exploring data sets to find patterns, anomalies, or relationships without having a specific hypothesis in mind. It is often used in the initial stages of data analysis to generate insights.
- Descriptive: <mark>Focuses on summarizing historical data to understand what has happened</mark> in the past. It often involves the use of [Statistics](#statistics) measures and [Data Visualisation](#data-visualisation) tools to present data trends and patterns.
- Diagnostic: <mark>Seeks to understand why something happened</mark>. It involves examining data to identify causes and correlations, often using techniques like data mining and statistical analysis.

Forward looking:
- Predictive: Uses historical data and statistical algorithms to <mark>forecast future outcome</mark>s. It helps in identifying trends and making predictions about future events based on past data.
- Prescriptive: Goes a step further by <mark>recommending actions based on the predictions made</mark>. It uses optimization and simulation algorithms to suggest the best course of action for a given situation.
- Inferential: Makes inferences and predictions about a population based on a sample of data. It often involves [Hypothesis testing](#hypothesis-testing) and [Confidence Interval](#confidence-interval).

# Data Analyst {#data-analyst}

Summary:
- Gathers and processes data to generate reports.
- Communicates insights and findings to management
- Conducts [Data Analysis](#data-analysis).

### Key responsibilities of a data analyst:

- Define Objectives: Clearly outline the goals of the analysis to guide the process.
  
- [Data Collection](#data-collection): Gather relevant data from various sources, ensuring accuracy, completeness, and timeliness.
  
- [Data Cleansing](#data-cleansing): Clean the data to remove errors, duplicates, and inconsistencies for reliable findings.
  
- Data Exploration: Perform exploratory data analysis ([EDA](#eda)) to understand data structure, [Distributions|distribution](#distributionsdistribution), and relationships.
  
- Choose the Right Tools: Utilize appropriate tools and software for analysis, such as Excel, R, Python, SQL, or specialized platforms.
  
- [Statistics](#statistics): Apply various statistical methods and techniques, such as regression analysis, clustering, and [hypothesis testing](#hypothesis-testing).
  
- [Data Visualisation](#data-visualisation): Use visualization techniques to effectively present findings and communicate insights.
  
- Interpret Results: Analyze results in the context of objectives and consider implications for decision-making.
  
- Documentation: Maintain thorough [Documentation & Meetings](#documentation--meetings) of the analysis process, including data sources, methodologies, and findings.
  
- Continuous Learning: Stay updated with the latest tools, techniques, and best practices in the evolving field of data analysis.

# Data Architect {#data-architect}

Data Architect
  - Designs and manages the data infrastructure.
  - Ensures data is stored, organized, and accessible for analysis.

# Data Archive Graph Analysis {#data-archive-graph-analysis}


Use the following to 

[Dataview](#dataview)
[Graph View](#graph-view)

Check out [Graph Analysis Plugin](#graph-analysis-plugin)

Convert Dataview to CSV


# Data Asset {#data-asset}



# Data Cleansing {#data-cleansing}


Data cleansing is the process of correcting or removing inaccurate, incomplete, or inconsistent data to improve its [Data Quality](#data-quality) for analysis. Involves:

- [standardised/Outliers|Handling Outliers](#standardisedoutliershandling-outliers)
- [Handling Missing Data](#handling-missing-data)
- [Handling Different Distributions](#handling-different-distributions)

In [DE_Tools](#de_tools) see:
- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Cleaning/Dataframe_Cleaing.ipynb
- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Cleaning

Related terms:
- [Data Selection](#data-selection)

Follow-up questions:
- [Deleting rows or filling them with the mean is not always best ](#deleting-rows-or-filling-them-with-the-mean-is-not-always-best)



# Data Collection {#data-collection}

Determine the [Data Quality](#data-quality) and quantity of data required and get it.

[Imbalanced Datasets](#imbalanced-datasets)

# Data Contract {#data-contract}

### [Data Contract](#data-contract)

pattern to handle schema changes

Pattern to apply to organisation using tools they have.

Tooling:
- [dbt](#dbt)

Data contracts help prevent preventable data issues while increasing collaboration and reducing costs.

A data contract is an agreed interface between
the generators of data and its consumers. It sets
the expectations around that data, defines how
it should be governed, and facilitates the
explicit generation of quality data that meets
the business requirements.

Interfaces:
- [API](#api) for data.

A document to codify what has been agreed.

Q: How does the Data Contract allow for contextual rules? Example the same schema can support multiple products in our org but the DQ rules can be different for different Products
A: Data contracts are particular business. Could use <mark>inheritance</mark> of rules in data contracts - basic template. To get standardisation across products.

[Data Contract](#data-contract)
By establishing a [data contract](#data-contract) and building interfaces based on it, organizations can improve data quality. Implementing structured agreements and automated change management processes can help business users, who may not be data experts, produce higher-quality data ([Data Quality](#data-quality)).

### Images

![Pasted image 20250312163351.png](../content/images/Pasted%20image%2020250312163351.png)

# Data Distribution {#data-distribution}

Data distribution refers to the process of making processed and analyzed data available for downstream applications and systems. 

This can involve supplying data to
- business applications, 
- reporting systems, 
- or other data-driven processes, 
- ensuring that stakeholders

have access to the information they need for decision-making and operations.

# Data Drift {#data-drift}

Data drift refers to changes in the statistical properties of input data that a machine learning (ML) model encounters during production. Such shifts can lead to decreased model performance, as the model may struggle to make accurate predictions on data that differ from its training set. 

Regular monitoring and prompt response to data drift are essential to maintain the effectiveness of ML models in dynamic production environments.

Concepts:

- Data drift involves changes in input data distributions
- Concept drift pertains to alterations in the relationship between inputs and outputs.
- [Performance Drift](#performance-drift) drift relates to changes in model outputs. 

**Training-Serving Skew:** This refers to discrepancies between training data and production data, which can arise from data drift or other factors, leading to performance issues. 

<mark>**Detecting Data Drift:**</mark>

Identifying data drift is crucial for maintaining model accuracy. Techniques include:

- **Statistical Hypothesis Testing:** Assessing whether differences between training and production data distributions are statistically significant.

- **Distance Metrics:** Quantifying the divergence between data distributions using measures like Kullback-Leibler divergence or Kolmogorov-Smirnov tests.

- **Monitoring Summary Statistics:** Regularly reviewing key statistical indicators (e.g., mean, variance) of input features to detect anomalies.

**Addressing Data Drift:**

Once detected, strategies to manage data drift include:

1. **Data Quality Checks:** Ensure that the drift isn't due to data quality issues, such as errors in data collection or processing. 

2. **Investigate the Drift:** Analyze the source and nature of the drift to understand its implications.

3. **Model Retraining:** Update the model using recent data to help it adapt to new patterns.

4. **Model Rebuilding:** In cases of significant drift, it may be necessary to redesign the model architecture or feature engineering processes.

5. **Fallback Strategies:** Implement alternative decision-making processes, such as rule-based systems or human judgment, when the model's reliability is compromised.





 

# Data Engineer {#data-engineer}


The primary responsibility of a data engineer is to take data from its source and make it available for analysis. They focus on
- automating the data collection, 
- processing, 
- and analysis workflows,
- solving how systems manage and handle the flow of data. 

<mark>Develops data pipelines and ensures data flow between systems.</mark>

Resources:
- [Link](https://www.youtube.com/watch?v=qWru-b6m030)

### Key Responsibilities:

1. Infrastructure Design and Maintenance:  
   Data engineers design, build, and maintain the necessary infrastructure to collect, process, and store large amounts of data. This infrastructure is crucial for ensuring data is accessible and usable for analysis and reporting.

2. [Data Pipeline](#data-pipeline): 

3. Support Role:  
   Data engineers act as a bridge between <mark>data producers and consumers</mark>, ensuring smooth and reliable data flow. They support business operations through scalable and efficient [Data Management](#data-management) solutions, contributing indirectly to product delivery and decision-making.

### Core Activities:

What engineers do & interact with: see [Data Engineering Portal](#data-engineering-portal)

Stakeholders they interact with see [Data Roles](#data-roles)

Tools they use: [Data Engineering Tools](#data-engineering-tools)

Tasks They Are Usually Given
  - Project Management: Tracking tasks, bugs, and progress through Azure Boards.
  - Collaboration: Facilitating teamwork with shared repositories and [continuous integration](#continuous-integration) workflows.
  - Continuous Learning: Keeping up-to-date with the latest technologies and updating pipelines due to obsolescence of tech
  - [Documentation & Meetings](#documentation--meetings) and Security: Creating documentation, implementing security measures, and exploring system upgrades for enhanced efficiency.



# Data Engineering Portal {#data-engineering-portal}


Databases manage large data volumes with scalability, speed, and flexibility. Key systems include:

- [MySql](#mysql)
- [PostgreSQL](#postgresql)


They facilitate efficient [CRUD.md](obsidian://open?vault=content&file=standardised%2FCRUD.md) operations and transactional processing ([OLTP.md](obsidian://open?vault=content&file=standardised%2FOLTP.md)), structured by a [Database Schema.md](obsidian://open?vault=content&file=standardised%2FDatabase%20Schema.md) that organizes data into tables and relationships.

## Key Features

- **[Structured Data](#structured-data)**: Organized for efficient CRUD operations, allowing reliable access.
- **Relational Databases**: Use SQL to manage data in tables with relationships expressed through foreign keys and joins, minimizing redundancy.

Structure
- Data is organized into tables (like spreadsheets) with columns (fields) and rows (records), enabling efficient storage and retrieval.

Flexibility
- Databases have a flexible schema that adapts to evolving requirements, unlike static solutions like spreadsheets.

Related Ideas:
- [Spreadsheets vs Databases](#spreadsheets-vs-databases)
- [Database Management System (DBMS)](#database-management-system-dbms)
- [Components of the database](#components-of-the-database)
- [Relating Tables Together](#relating-tables-together)
- [Turning a flat file into a database](#turning-a-flat-file-into-a-database)
- [Database Techniques](#database-techniques)

# Data Engineering Tools {#data-engineering-tools}


  - **Snowflake:** [Cloud](#cloud)-based data warehousing for scalable storage and processing.
  - **Microsoft SQL Server:** [SQL](#sql)-based relational database management.
  - **[Azure](#azure) SQL Database:** Managed relational database service on Azure.
  - **Azure Data Lake Storage:** Scalable storage for big data analytics.
  - **SQL and T-SQL:** Query languages for managing and querying relational databases.
  - **AWS [Amazon S3|S3](#amazon-s3s3):** Storage for data lakes.

[Data Ingestion](#data-ingestion) Tools and Technologies:
- [Apache Kafka](#apache-kafka)
- AWS Kinesis: A cloud service for real-time data processing, enabling the collection and analysis of streaming data.
- Google Pub/Sub: A messaging service that allows for asynchronous communication between applications, supporting real-time data ingestion.

[Data Storage](#data-storage)
Tools: Amazon S3, Google BigQuery, Snowflake.

[dbt](#dbt)

### Tags
- **Tags**: #data_tools, #data_management

# Data Engineering {#data-engineering}


The definition from the [Fundamentals of Data Engineering](https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/), as it’s one of the most recent and complete: 

> Data engineering is the development, implementation, and maintenance of systems and processes that take in raw data and produce high-quality, consistent information that supports downstream use cases, such as analysis and machine learning. Data engineering intersects security, [Data Management](#data-management), DataOps, data architecture, orchestration, and software engineering.

A [Data Engineer](#data-engineer) today oversees the whole data engineering process, from collecting data from various sources to making it available for downstream processes. The role requires familiarity with the multiple stages of the [Data Engineering Lifecycle](Data%20Lifecycle%20Management.md) and an aptitude for evaluating data tools for optimal performance across several dimensions, including price, speed, flexibility, scalability, simplicity, reusability, and interoperability.

Data Engineering helps also overcome the bottlenecks of [Business Intelligence](term/business%20intelligence.md):
- More transparency as tools are open-source mostly
- More frequent data loads
- Supporting [Machine Learning](Machine%20Learning.md) capabilities 

Compared to existing roles it would be a **software engineering plus business intelligence engineer including big data abilities** as the [Hadoop](term/apache%20hadoop.md) ecosystem, streaming, and computation at scale. Business creates more reporting artifacts themselves but with more data that needs to be collected, cleaned, and updated near real-time and complexity is expanding every day.

With that said more programmatic skills are needed similar to software engineering. **The emerging language at the moment is [Python](term/python.md)** which is used in engineering with tools alike [Apache Airflow](#apache-airflow), [dagster](dagster.md), [Prefect](#prefect) as well as data science with powerful libraries.

As a data engineer, you use mainly [SQL](SQL.md) for almost everything except when using external data from an API. Here you'd use [ELT](term/elt.md) tools or write some [Data Pipeline](#data-pipeline) with the tools mentioned above.

# Data Governance {#data-governance}


[**Data governance**](https://www.talend.com/resources/what-is-data-governance/) **is a collection of processes, roles, policies, standards, and metrics that ensure the effective and efficient use of information in enabling an organization to achieve its goals.**

It establishes the processes and responsibilities that ensure the [Data Quality](Data%20Quality.md) and security of the data used across a business or organization. Data governance defines who can take what action, upon what data, in what situations, and using what methods.

**Data Governance**: Focuses on ensuring that data is managed consistently and adheres to policies, often working in tandem with [Data Observability](#data-observability) to enforce quality standards.

# Data Hierarchy Of Needs {#data-hierarchy-of-needs}


![Pasted image 20241005170237.png|500](../content/images/Pasted%20image%2020241005170237.png|500)

The **Data Hierarchy of Needs** is a framework that outlines the stages required to effectively use data in organizations. It resembles Maslow’s hierarchy, progressing from basic data needs to advanced capabilities:

1. **Data Collection**:  (bottom)
   Start by collecting raw data from various sources, ensuring it's stored securely and reliably.

2. **Data Storage and Access**:  
   Organize and store data so it's easily accessible for those who need it, using databases or data warehouses.

3. **Data Cleaning and Preparation**:  
   Clean, preprocess, and transform data to ensure it’s accurate, consistent, and ready for analysis.

4. **Data Analytics**:  
   Analyze the prepared data to generate insights, identify patterns, and create reports.

5. **Data-Driven Decision Making**:  
   Use the insights from data analytics to inform and improve decision-making across the organization.

6. **Advanced Data Capabilities (AI/ML)**:  (top)
   Once the foundation is in place, apply advanced techniques like machine learning and artificial intelligence for predictive and prescriptive insights.


# Data Ingestion {#data-ingestion}


Data ingestion is the process of collecting and importing raw data from various sources ([Database](#database), [API](#api), [Data Streaming](#data-streaming) services) into a system for processing and analysis, and can be performed in batch and realtime ingestion. The goal is to gather raw data that can be processed and analyzed.

Used for building [Data Pipeline](#data-pipeline)

Challenges
- [Data Quality](#data-quality): Ensuring that the ingested data is accurate, complete, and consistent.
- [Scalability](#scalability): Handling large volumes of data efficiently as the data sources grow.
- [Latency](#latency): Minimizing the delay between data generation and processing, especially in real-time scenarios.

Use Cases:
- Data ingestion is used in various applications, including: [business intelligence](#business-intelligence), [Machine Learning](#machine-learning)

Related to:
- [Data Engineering Tools](#data-engineering-tools)



[Data Ingestion](#data-ingestion)
   **Tags**: #data_collection, #data_management

# Data Integration {#data-integration}


Data integration is the process of combining data from disparate source systems into a single unified view, moving data to a [Single Source of Truth](#single-source-of-truth).

## Manual Integration
Manual integration involves analysts manually logging into source systems, analyzing and/or exporting data, and creating reports. 

### Disadvantages of Manual Integration:
- **Time-consuming**: The process requires significant time investment.
- **Security Risks**: Analysts need access to multiple operational systems.
- **Performance Issues**: Running analytics on non-optimized systems can interfere with their functioning.
- **Outdated Reports**: Data changes frequently, leading to quickly outdated reports.

## [Data Virtualization](#data-virtualization)

Data virtualization is a method that allows access to data without needing to replicate it, providing a unified view of data from multiple sources.

## Application Integration
Application integration links multiple applications to move data directly between them. 

### Methods of Application Integration:
- **Point-to-Point Communications**: Direct connections between applications.
- **Middleware Layer**: Using tools like an Enterprise Service Bus (ESB).
- **Application Integration Tools**: Specialized tools for integrating applications.

### Disadvantages of Application Integration:
- **Data Redundancy**: May result in multiple copies of the same data across systems.
- **Increased Costs**: Managing multiple copies can lead to higher costs.
- **Point-to-Point Traffic**: Can create excessive traffic between systems.
- **Performance Impact**: Executing analytics on operational systems may interfere with their functioning.

# Data Integrity {#data-integrity}

Data integrity refers to the 
- accuracy, 
- consistency, and 
- reliability of data

throughout its lifecycle. It ensures that data remains <mark>unaltered</mark> and <mark>trustworthy</mark>, whether it is being 
- stored, 
- processed, 
- or transmitted. 

Maintaining data integrity involves implementing measures to prevent unauthorized access, corruption, or loss of data.

In the context of [Database](#database) and information systems, data integrity can be enforced through:

1. **Validation Rules**: Ensuring that data entered into a system meets certain criteria.
2. **Access Controls**: Limiting who can view or modify data.
3. **Backups**: Regularly saving copies of data to prevent loss.
4. **Error Checking**: Using [Checksum](#checksum) or [Hash](#hash) to verify data integrity during transmission.



[Data Integrity](#data-integrity)
   **Tags**: #data_quality, #data_management

# Data Lake {#data-lake}


A Data Lake is a storage system with vast amounts of [unstructured data](#unstructured-data) and [structured data](#structured-data), stored as-is, without a specific purpose in mind, that can be built on multiple technologies such as Hadoop, NoSQL, Amazon Simple Storage Service, a relational database, or various combinations and different formats (e.g. Excel, CSV, Text, Logs, etc.).

**Definition**: A repository that <mark>stores diverse data types</mark>, including structured, semi-structured, and unstructured data. If cant fit into a database.

Features:
- **Versatility**: Can accommodate various data formats, including videos, images, documents, and more.
- **Raw Data Storage**: Preserves data in its raw form, suitable for advanced analytics, particularly in machine learning and AI.
- **Data Usability**: Raw data <mark>may require cleaning and transformation for analytical use</mark>, often transferred to databases or data warehouses.
- **Use Case**: Valuable for storing large volumes of raw data, especially in contexts requiring advanced analytics and experimentation.

[unstructured data](#unstructured-data) for predictive modeling and analysis. This leads to the creation of a **data lake**, which stores raw data without predefined schemas. 

The data lake supports the following capabilities:
-   To capture and store raw data at scale for a low cost
-   To store many types of data in the same repository
-   To perform [Data Transformation](Data%20Transformation.md) on the data where the purpose may not be defined
-   To perform new types of data processing
-   To perform single-subject analytics based on particular use cases

Components of a data lake
		1. [Storage Layer](term/storage%20layer%20object%20store.md)
		2. [Data Lake File Format](term/data%20lake%20file%20format.md)
		3. [Data Lake Table Format](term/data%20lake%20table%20format.md) with [Apache Parquet](term/apache%20parquet.md), [Apache Iceberg](term/apache%20iceberg.md), and [Apache Hudi](term/apache%20hudi.md)

# Data Lakehouse {#data-lakehouse}


A Data Lakehouse open [Data Management](#data-management) architecture that combines the flexibility, cost-efficiency, and scale of [Data Lake](Data%20Lake.md) with the data management and ACID transactions of [Data Warehouse](Data%20Warehouse.md) with Data Lake Table Formats ([Delta Lake](term/delta%20lake.md), [Apache Iceberg](term/apache%20iceberg.md) & [Apache Hudi](term/apache%20hudi.md)) that enable Business Intelligence (BI) and Machine Learning (ML) on all data.

A **data lakehouse** is an emerging architectural approach that combines the best features of data lakes and data warehouses to provide a unified platform for storing, processing, and analyzing large volumes of structured and unstructured data. Here’s a breakdown of its key characteristics and benefits:

The data lakehouse architecture represents a significant evolution in [Data Management](#data-management), addressing the limitations of traditional data lakes and [Data Warehouse|Warehouse](#data-warehousewarehouse) by providing a unified platform for all data types.
### Key Characteristics

1. **Unified Storage**:
   - Data lakehouses store data in a single repository, accommodating both structured data (like tables in a database) and unstructured data (like images, videos, and text). This eliminates the need for separate systems, simplifying data management.

2. **Support for Multiple Data Types**:
   - They can handle various data formats, such as **CSV**, **JSON**, **Parquet**, and **Avro**, enabling flexibility in how data is ingested and stored.

3. [ACID Transaction](#acid-transaction):
   - Unlike traditional data lakes, data lakehouses provide [ACID Transaction](#acid-transaction) which ensure reliable data operations and integrity, even in concurrent processing environments.

4. **Schema Enforcement**:
   - Data lakehouses can enforce [Database Schema|schema](#database-schemaschema) at the time of data write, allowing users to define data structures while still benefiting from the flexibility of a data lake.

5. **Performance Optimization**:
   - They incorporate various optimization techniques, such as indexing and caching, to improve query performance and provide faster access to data.

6. **Integration with BI Tools**:
   - Data lakehouses are designed to work seamlessly with business intelligence (BI) tools and data analytics platforms, enabling users to derive insights without needing extensive data preparation.

### Benefits

1. **Cost-Effectiveness**:
   - By merging the functionalities of data lakes and data warehouses, organizations can reduce the costs associated with maintaining separate systems for structured and unstructured data.

2. **Scalability**:
   - Data lakehouses leverage cloud storage solutions, allowing for scalable data storage that can grow with the organization’s needs.

3. **Data Accessibility**:
   - With a unified architecture, data from different sources can be accessed and analyzed together, breaking down silos and fostering a more holistic view of the organization’s data landscape.

4. **Simplified Data Pipelines**:
   - Data lakehouses streamline the data ingestion process, enabling organizations to build more efficient data pipelines that accommodate a variety of data sources.

5. **Support for Advanced Analytics**:
   - They provide a robust foundation for advanced analytics, including machine learning and real-time data processing, allowing organizations to extract actionable insights more effectively.

Platforms that implement the data lakehouse architecture include:
- **Databricks Lakehouse Platform**: Combines data engineering, data science, and BI capabilities with a focus on collaboration.
- **Apache Iceberg**: A high-performance table format for large analytic datasets that supports ACID transactions and schema evolution.




# Data Leakage {#data-leakage}

**Data Leakage** refers to the unintentional inclusion of information in the training data that would not be available in a real-world scenario, leading to overly optimistic model performance. It occurs when the model has access to data it shouldn't during training, such as future information or test data, which can result in misleading evaluation metrics and poor generalization to new data.

# Data Lifecycle Management {#data-lifecycle-management}


This is the comprehensive process of managing data from its initial ingestion to its final use in downstream processes. 

Used for maintaining [data integrity](#data-integrity), optimizing performance, and ensuring that data-driven decisions are based on accurate and timely information. 

Not the same as the [Software Development Life Cycle](#software-development-life-cycle)

Key Stages of Full Lifecycle Management

1. [Data Ingestion](#data-ingestion)
2. [Data Storage](#data-storage)
3. [Preprocessing](#preprocessing)
4. [Data Analysis](#data-analysis)
5. [Data Visualisation](#data-visualisation)
6. [Data Distribution](#data-distribution)

Data engineers must evaluate and select tools and technologies based on several [Performance Dimensions](#performance-dimensions)

# Data Lineage {#data-lineage}


Data lineage uncovers the [Data Lifecycle Management](#data-lifecycle-management) life cycle of data. It aims to show the complete data flow from start to finish. 

Data lineage is the process of understanding, recording, and visualizing data as it flows from data sources to consumption. 

This includes all [Data Transformation](Data%20Transformation.md) (what changed and why).

# Data Literacy {#data-literacy}


Data literacy is the ability to read, work with, analyze, and argue with data in order to extract meaningful information and make informed decisions. This skill set is crucial for employees across various levels of an organization, especially as data-driven decision-making becomes increasingly important.

Organizations should invest in data literacy training programs to empower their employees with the necessary skills to effectively engage with data. A data-literate employee can read charts, draw correct conclusions, recognize when data is being used inappropriately or misleadingly, and gain a deeper understanding of the business domain. This enables them to communicate more effectively using a common language of data, spot unexpected operational issues, identify root causes, and prevent poor decision-making due to data misinterpretation.

Examples of data literacy in action include:

* Implementing the Adoptive Framework to create a Data Literacy Program.
* Employees working with spreadsheets to understand the rationale behind data-driven decisions and advocating for alternative courses of action.
* Work teams identifying areas where data needs clarification for a project.

By nurturing a data-literate workforce, businesses can improve their ability to make informed decisions, drive innovation, and achieve better outcomes.


# Data Management {#data-management}


Data management involves overseeing processes to maintain data integrity and quality. It includes:

- **Responsibility**: Identifying accountable individuals or teams.
- **Issue Resolution**: Mechanisms for detecting and addressing data-related problems.

Data management ensures that a [Data Pipeline](#data-pipeline) operates efficiently, focusing on monitoring errors, performance issues, and [data quality](#data-quality).

**Tools**:
- [Apache Airflow](#apache-airflow)
- Prefect
- [Dagster](#dagster)

Related Concepts:
- [Database Management System (DBMS)](#database-management-system-dbms)
- [Master Data Management](#master-data-management)
- [Data Distribution](#data-distribution)



[Data Management](#data-management)
   **Tags**: #data_management, #data_quality

# Data Modelling {#data-modelling}


Data modelling is the process of creating a visual representation of a system's data and the relationships between different data elements. 

This helps in organizing and structuring the data so it can be efficiently managed and utilized.

Data modelling ensures that data is logically structured and organized, making it easier to store, retrieve, and manipulate in a database.

Workflow of Data Modeling:
1) [Conceptual Model](#conceptual-model)
2) [Logical Model](#logical-model)
3) [Physical Model](#physical-model)


Types of Modeling:
- Relational: Organizes data into tables.
- Object-Oriented: Focuses on objects and their state changes, e.g., robots in a car factory.
- Entity: Uses [ER Diagrams](#er-diagrams) to represent data entities and relationships.
- Network: An extension of hierarchical models.
- Hierarchical: Organizes data in a tree-like structure.





[Data Modelling](#data-modelling)
   **Tags**: #data_modeling, #database_design

# Data Observability {#data-observability}


Data observability refers to the continuous monitoring and collection of metrics about your data to ensure its [Data Quality](#data-quality), reliability, and availability. 

It covers various aspects, such as data quality, pipeline health, metadata management, and infrastructure performance. By tracking key metrics and [standardised/Outliers|anomalies](#standardisedoutliersanomalies), it helps detect issues like data freshness problems, schema changes, or pipeline failures before they impact downstream processes or users.
### Categories of Observability

Auto-profiling Data:

Automatically tracks data attributes, such as row count, column types, data distributions, and schema changes.
 - Bigeye: Provides ML-driven threshold tests and automatic alerts when data drifts beyond expected ranges.
 - Datafold: Integrates with GitHub to run data diffs between environments, offering insights into differences between datasets during development.
 - Monte Carlo: Enterprise-focused with data lake integrations for comprehensive observability.
 - Metaplane: Offers a high level of configuration and both out-of-the-box and custom tests.

Pipeline Testing:

Ensures that data transformation pipelines are functioning correctly by verifying the quality and accuracy of data as it moves through different stages.
 - Great Expectations: An open-source tool that allows you to define tests and automatically generate documentation for those tests, promoting transparency in data quality checks.
 - Soda: Offers pipeline testing with the flexibility of a self-hosted option for more control over data quality monitoring.
 - [dbt](#dbt)tests: Integrated with [dbt](#dbt) Core and dbt Cloud, allowing testing during the transformation process in a dbt project.

 Infrastructure Monitoring:
 
Monitors the health and performance of the underlying data infrastructure, such as databases, pipelines, and servers, to prevent failures and bottlenecks.
 - DataDog: Provides deep monitoring capabilities, including for Airflow, containers, and custom metrics, allowing visibility at various layers of the data stack.

### Managing Metadata

Managing metadata is critical for observability, as it provides context and lineage for your data. Metadata can include:

- Technical Metadata: Information about the dataset’s structure, such as table schema, data types, and column descriptions.
- Operational Metadata: Information about the dataset’s freshness, when it was last updated, and the number of records processed.
- Business Metadata: Describes the meaning of data, such as field definitions and business rules, helping stakeholders understand the context and usage of the dataset.

How to Manage Metadata:

- Manual Documentation: Teams may manually document metadata, but this can be prone to human error and inconsistency.
- Automated Metadata Management: Many modern data tools, such as data catalogs (e.g., Atlan, Alation), automatically track and manage metadata, offering insights into data lineage, schema changes, and data usage.
- Integration with Data Pipelines: Tools like dbt also generate metadata about transformations, which can be included in downstream monitoring systems to ensure consistency and traceability.

[Data Observability](#data-observability)
- Tracking the issues.
- Alerting and ensuring data owners fix it.

# Data Pipeline To Data Products {#data-pipeline-to-data-products}


The journey from [Data Pipeline](#data-pipeline) to [Data Product](#data-product) involves transforming raw data into valuable insights or applications that can be used to drive business decisions. This process typically includes several stages, each with its own set of tasks and objectives.

Read more on [Data Orchestration Trends: The Shift From Data Pipelines to Data Products](https://airbyte.com/blog/data-orchestration-trends).
### Workflow

1. **Define Objectives**:
   - Understand the business goals and what insights or products are needed.

2. **Design the Pipeline**:
   - Plan the architecture and select appropriate tools for each stage of the pipeline.

3. **Implement and Test**:
   - Build the pipeline, ensuring data flows smoothly from ingestion to product delivery.
   - Test for accuracy, performance, and reliability.

4. **Deploy and Monitor**:
   - Deploy the pipeline in a production environment.
   - Continuously monitor for performance and make adjustments as needed.

5. **Iterate and Improve**:
   - Gather feedback and refine the pipeline and products to better meet business needs.
### Example

Imagine a retail company wants to create a recommendation system for its online store:

1. **Data Ingestion**: Collect customer browsing and purchase data from the website.
2. **Data Processing**: Clean and transform the data to identify patterns in customer behavior.
3. **Data Storage**: Store the processed data in a data warehouse for easy access.
4. **Data Analysis**: Use machine learning algorithms to analyze the data and generate recommendations.
5. **Data Visualization**: Create dashboards to visualize customer trends and recommendation performance.
6. **Data Products**: Deploy the recommendation system on the website to enhance customer experience.



# Data Pipeline {#data-pipeline}


A data pipeline is a series of processes that automate the movement and transformation of data from various sources to a destination where it can be stored, analyzed, and used to generate insights. 

It ensures that data flows smoothly and efficiently through different stages, maintaining data quality and [Data Integrity](#data-integrity).

By implementing a data pipeline, organizations can automate data workflows, reduce manual effort, and ensure timely and accurate data delivery for decision-making.
### Workflow

1. [Data Ingestion](#data-ingestion)
2. [Data Transformation](#data-transformation)
3. [Data Storage](#data-storage)
4. [Preprocessing|Data Preprocessing](#preprocessingdata-preprocessing)
5. [Data Management](#data-management)
#### Other steps:

Design:
   - Define the objectives and requirements of the data pipeline.
   - Choose appropriate tools and technologies.

Development:
   - Build the pipeline components and integrate them into a cohesive system.

Testing:
   - Validate the pipeline to ensure data accuracy and performance.

Deployment:
   - Deploy the pipeline in a production environment.

Monitoring and Maintenance:
   - Continuously monitor the pipeline and make necessary adjustments to improve performance and reliability.

### Related Notes

- [Data Pipeline to Data Products](#data-pipeline-to-data-products)




[Data Pipeline](#data-pipeline)
   **Tags**: #data_workflow, #data_management

# Data Principles {#data-principles}


Data principles are essential for ensuring that data is managed, used, and maintained effectively and ethically.

1. [Data Quality](#data-quality) Ensure data is accurate, complete, reliable, and up-to-date. High-quality data is crucial for making informed decisions.

2. [Data Governance](#data-governance): Establish clear policies and procedures for data management, including roles and responsibilities, to ensure [data integrity](#data-integrity) and compliance with regulations.

3. Data Privacy: Protect personal and sensitive information by adhering to privacy laws and regulations, such as GDPR or CCPA, and implementing appropriate security measures.

4. Data Security: Safeguard data against unauthorized access, breaches, and other security threats through encryption, access controls, and regular [security](#security) audits.

5. Data Accessibility: Ensure that data is easily accessible to those who need it while maintaining appropriate security and privacy controls. This includes providing the necessary tools and training for data access.

6. Data Transparency: Maintain transparency about data collection, usage, and sharing practices. This helps build trust with stakeholders and ensures accountability.

7. Data Consistency: Standardize data formats and definitions across the organization to ensure consistency and interoperability.

8. Data Stewardship: Assign data stewards to oversee [data management](#data-management) practices, ensuring data quality, compliance, and proper usage.

9. [Data Lifecycle Management](#data-lifecycle-management) Manage data throughout its lifecycle, from creation and storage to archiving and deletion, ensuring that data is retained only as long as necessary.

10. Ethical Data Use: Use data ethically and responsibly, considering the potential impact on individuals and society. Avoid biases and ensure fairness in data-driven decisions.

11. Data [Documentation & Meetings](#documentation--meetings): Maintain thorough documentation of data sources, definitions, and processes to facilitate understanding and reproducibility.

12. Data Sharing and Collaboration: Encourage data sharing and collaboration within and across organizations to maximize the value of data, while respecting privacy and security constraints.
    
13. DRY

Related:
- [Performance Dimensions](#performance-dimensions)

# Data Product {#data-product}


A data product is

"a product that facilitates an end goal through data".

Delivering the final output, which could be dashboards, reports, or machine learning models. For example Recommendation systems or predictive analytics dashboards.

It applies more product thinking, whereas the "Data Product" essentially is a dashboard, report, and table in a [Data Warehouse](Data%20Warehouse.md) or a Machine Learning model.

Sometimes Data Products are also called [data asset](#data-asset).

# Data Quality {#data-quality}


Data quality is the process of ensuring that data meets established expectations. High-quality data is crucial for effective decision-making and analysis.

**Definition**: Data quality refers to the <mark>accuracy, consistency, and reliability of data.</mark> It is essential for maintaining trust in data-driven processes and outcomes. 

**Importance**: The principle of "garbage in, garbage out" highlights that poor-quality data leads to poor model performance.

Related terms:
- [Data Observability](#data-observability)
- [Change Management](#change-management)
- [Prevention Is Better Than The Cure](#prevention-is-better-than-the-cure)


Related terms:
- [Data Observability](#data-observability)
- [Data Contract](#data-contract)
- [Change Management](#change-management)

# Data Reduction {#data-reduction}

Reducing the volume of data through techniques:

[Dimensionality Reduction](#dimensionality-reduction)

[Sampling](#sampling): Use subsets of data for training to speed up the process and address issues like imbalanced data representation.

Remove features with zero or low [variance](#variance) and redundant features to improve model performance.

# Data Roles {#data-roles}

A data team is a specialized group within an organization responsible for managing, analyzing, and leveraging data to drive business decisions and strategies. 

The team collaborates across various functions to ensure data integrity, accessibility, and usability.

## Key Roles and Responsibilities

| Role                         | Focus Area                    | Key Responsibilities                                                                           |
| ---------------------------- | ----------------------------- | ---------------------------------------------------------------------------------------------- |
| **[Data Steward](#data-steward)**         | [Data quality](#data-quality) & governance | Enforces data policies, resolves [data quality](#data-quality) issues, manages metadata.                    |
| **[Data Governance](#data-governance) Team** | Policy & compliance           | Defines [data management](#data-management) rules, ensures regulatory adherence.                               |
| **[Data Engineer](#data-engineer)**        | Data infrastructure           | Builds data pipelines, integrates data sources, and ensures data flow.                         |
| **[Data Scientist](#data-scientist)**       | [Data analysis](#data-analysis) & modeling  | Utilizes BI tools, analyzes data, develops and deploys ML models.                              |
| **[ML Engineer](#ml-engineer)**          | Machine learning              | Configures and optimizes ML models, monitors performance in production.                        |
| **[Data Architect](#data-architect)**       | Data architecture             | Designs and manages data infrastructure, ensures data accessibility.                           |
| **[Data Analyst](#data-analyst)**         | Reporting & visualization     | Gathers and processes data, generates reports, communicates insights using tools like Tableau. |

#### Other Stakeholders
- **Business Analysts:** Ensure data is structured and accessible for analysis and reporting.
- **Senior Stakeholders and Business Ambassadors:** Communicate requirements, progress, and solutions to align with business goals.
- **Software Engineers and Data Teams:** Coordinate on data production and integration processes.

# Data Science {#data-science}


A field that uses the [Scientific Method](#scientific-method), algorithms, and systems to <mark>extract knowledge</mark> and insights from structured and [unstructured data](#unstructured-data). It combines techniques from [statistics](#statistics), computer science, and domain expertise to analyze and interpret complex data sets, enabling informed decision-making and predictive modeling.

Resources:
- https://scikit-learn.org/stable/auto_examples/index.html


# Data Scientist {#data-scientist}

Data Scientist
  - Utilizes [Business Intelligence](#business-intelligence) (BI) tools to analyze data.
  - Works with data lakes to extract insights.
  - Develops and deploys production Machine Learning (ML) models for predictions.

# Data Selection In Ml {#data-selection-in-ml}

When selecting data for machine learning models, several important considerations can significantly impact the model's performance/[Model Optimisation](#model-optimisation) and the insights you can derive from it. Here are key factors to consider:

1. Relevance:
   - Ensure that the features (input variables) you select are relevant to the problem you are trying to solve. Irrelevant features can introduce noise and reduce model accuracy.

2. Quality: [Data Quality](#data-quality)
   - Assess the quality of the data, including checking for missing values, outliers, and errors. Poor quality data can lead to inaccurate models.

3. Quantity:
   - Consider the size of your dataset. More data can lead to better models, but it also requires more computational resources. Ensure you have enough data to train your model effectively.

4. Balance: [Imbalanced Datasets](#imbalanced-datasets)
   - Check for [Imbalanced Datasets|class imbalance](#imbalanced-datasetsclass-imbalance) in classification problems. An imbalanced dataset can bias the model towards the majority class. Techniques like resampling, synthetic data generation, or using different evaluation metrics can help address this.

5. Feature Distribution: [Distributions](#distributions)
   - Analyze the distribution of your features. Features with skewed [distributions](#distributions) may need transformation ([Data Transformation](#data-transformation)) (e.g., log transformation) to improve model performance.

6. [Correlation](#correlation):
   - Examine the correlation between features. Highly correlated features can lead to [multicollinearity](#multicollinearity), which can affect model stability and interpretability. Consider removing or combining correlated features.

7. Dimensionality: [Dimensionality Reduction](#dimensionality-reduction)
   - High-dimensional data can lead to overfitting. Techniques like [feature selection](#feature-selection), dimensionality reduction (e.g., PCA), or regularization can help manage this.

8. Temporal Considerations:
- For time series data, ensure that the temporal order is maintained. Avoid data leakage by ensuring that future information is not used in training.

9. Domain Knowledge:
   - Leverage domain expertise to select features that are known to be important for the problem. This can guide feature engineering and selection.

10. Data Leakage:
  - Be cautious of [Data Leakage](#data-leakage), where information from the test set is inadvertently used in training. This can lead to overly optimistic performance estimates.

11. Scalability:
- Consider the scalability of your data selection process. As datasets grow, ensure that your methods can handle larger volumes efficiently.

# Data Selection {#data-selection}


Data selection is a crucial part of data manipulation and analysis. Pandas provides several methods to select data from a DataFrame.

In [DE_Tools](#de_tools)  we explore how to do Data Selection with Pandas
- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/selection.ipynb

Related:
- [Data Selection in ML](#data-selection-in-ml)
## Examples
### Selecting Columns

You can select a single column from a DataFrame using either bracket notation or dot notation:

```python
df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})
column_a = df['A']  # or df.A
```
### Selecting Rows by Index

To select rows by their index position, you can use slicing:
```python
rows_0_to_2 = df[0:3]  # Selects the first three rows
```
### Selecting Rows by Date Range

If your DataFrame has a DateTime index, you can select rows within a specific date range:

```python
date_rng = pd.date_range(start='2013-01-01', end='2013-01-06', freq='D')
df = pd.DataFrame(date_rng, columns=['date'])
df.set_index('date', inplace=True)
selected_dates = df['2013-01-02':'2013-01-04']
```
### Label-based Selection

Use `.loc` or `.at` to select rows by label:

```python
df = pd.DataFrame({'Weather': ['Sunny', 'Rain', 'Cloudy'], 'Temp': [30, 22, 25]})
df.set_index('Weather', inplace=True)
rain_row = df.loc['Rain']  # or df.at['Rain']
```
### Position-based Selection

Use `.iloc` or `.iat` to select rows by position:

```python
third_row = df.iloc[2]  # Selects the third row
specific_value = df.iat[1, 1]  # Selects the value at row 1, column 1
```
### Conditional Selection

Create a new DataFrame based on a condition:
```python
df_new = df[df['var1'] >= 999]  # Selects rows where 'var1' is greater than or equal to 999
```
The condition `df["var1"] >= 999` creates a boolean Series that filters the rows of `df`.




The Z-test is a statistical method used to determine if there is a <mark>significant difference between the means of two groups or to compare a sample mean to a known population mean when the population [standard deviation](#standard-deviation) is known</mark>. 

It is typically applied when the sample size is large (usually n > 30).

## Types of Z-tests

1. **One-Sample Z-test**: This test compares the mean of a single sample to a known population mean. It assesses whether the sample mean significantly differs from the population mean.

2. **Two-Sample Z-test**: This test compares the means of two independent samples. It is used when both sample sizes are large and the population variances are known or can be assumed to be equal.

## Characteristics of the Z-distribution

The Z-distribution is a normal distribution with a mean of 0 and a standard deviation of 1. It is symmetric and bell-shaped, which allows for the application of the [Central Limit Theorem](#central-limit-theorem). As sample sizes increase, the distribution of sample means approaches a normal distribution, making the Z-test applicable.

## Assumptions

For the Z-test to be valid, certain assumptions must be met:
- The data should be normally distributed, especially for smaller sample sizes. However, with large samples, the Central Limit Theorem allows for the Z-test to be used even if the data is not perfectly normal.
- The samples should be independent of each other.
- The population standard deviation should be known.

## Test Statistic

The test statistic for the Z-test is calculated using the formula:

$$ Z = \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} $$

where:
- $\bar{X}$ = sample mean
- $\mu$ = population mean (or mean of the second sample in the two-sample test)
- $\sigma$ = population standard deviation
- $n$ = sample size

This formula allows for the comparison of the sample mean to the population mean, standardizing the difference in terms of standard deviations.