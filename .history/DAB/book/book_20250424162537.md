# Preface {#preface}

Remember the Data Archive is non‑linear — use internal links for navigation.
If you need a specific page see the **Table of Contents** to jump to a topic.



# Columnar Storage {#columnar-storage}

A database storage technique that stores <mark>data by columns</mark> rather than rows, 

Useful for read-heavy operations and <mark>large-scale data analytics</mark>, as it enables the retrieval of specific columns without the need to access the entire row. 

Columnar Storage Example (Analytical Workloads)**:

| `order_id`  | `customer_id` | `order_date` | `order_amount` |
|-------------|---------------|--------------|----------------|
| 1           | 101           | 2024-10-01   | $100           |
| 2           | 102           | 2024-10-02   | $150           |
| 3           | 103           | 2024-10-03   | $200           |
In **columnar storage**, the data would be stored by columns, like:
- `customer_id`: [101, 102, 103]

If you're querying for the total sales (`order_amount`) in a specific period, only the `order_amount` and `order_date` columns are accessed. 


Use case: **Data Analytics/OLAP (Online Analytical Processing)**
- Running a query to get the **total sales for October** only needs to scan the `order_amount` and `order_date` columns, rather than scanning entire rows, faster [Querying](#querying)

# Command Line {#command-line}


The command line is a text-based interface used to interact with a computer's operating system or software. It allows users to execute commands, run scripts, and perform various tasks.

[PowerShell](#powershell)

[Powershell vs Bash](#powershell-vs-bash)

[Bash](#bash)

[Command Prompt](#command-prompt)

# Command Prompt {#command-prompt}


Command Prompt (cmd) is a command-line interpreter on Windows systems that allows users to execute commands to perform various basic tasks. Below are some common tasks that can be performed in cmd, along with examples:

Related to:
- [Bash](#bash)

## 1. Navigating the File System

- **Changing Directories:**
  
```cmd
  cd C:\path\to\directory
  
```
  Changes the current directory to `C:\path\to\directory`.

- **Listing Files and Directories:**
  
```cmd
  dir
  
```
  Lists the files and directories in the current directory.

## 2. Managing Files and Directories

- **Creating a Directory:**
  
```cmd
  mkdir newfolder
  
```
  Creates a new directory named `newfolder`.

- **Deleting a Directory:**
  
```cmd
  rmdir /s /q newfolder
  
```
  Deletes the directory `newfolder` and its contents. The `/s` flag removes all directories and files in the specified directory, and the `/q` flag runs the command quietly without asking for confirmation.

- **Copying Files:**
  
```cmd
  copy C:\source\file.txt D:\destination\
  
```
  Copies `file.txt` from the `C:\source` directory to the `D:\destination` directory.

- **Renaming Files:**
  
```cmd
  ren oldfile.txt newfile.txt
  
```
  Renames `oldfile.txt` to `newfile.txt`.

- **Deleting Files:**
  
```cmd
  del file.txt
  
```
  Deletes `file.txt`.

## 3. Viewing and Managing System Information

- **Viewing IP Configuration:**
  
```cmd
  ipconfig
  
```
  Displays the current network configuration.

- **Viewing System Information:**
  
```cmd
  systeminfo
  
```
  Provides detailed system information including OS version, hardware details, and network configurations.

## 4. Managing Processes

- **Viewing Running Processes:**
  
```cmd
  tasklist
  
```
  Lists all currently running processes.

- **Killing a Process:**
  
```cmd
  taskkill /F /PID 1234
  
```
  Terminates the process with the Process ID (PID) `1234`. The `/F` flag forces the process to terminate.

## 5. Networking Commands

- **Pinging a Server:**
  
```cmd
  ping www.example.com
  
```
  Sends ICMP Echo Request packets to the specified host and displays the response.

- **Tracing Route to a Server:**
  
```cmd
  tracert www.example.com
  
```
  Traces the route packets take to the specified host.

## 6. Batch File Scripting

- **Creating and Running a Simple Batch File:**
  - Create a file named `example.bat` with the following content:
    
```cmd
    @echo off
    echo Hello, World!
    pause
    
```
  - Run the batch file:
    
```cmd
    example.bat
    
```
  This batch file prints "Hello, World!" to the console and waits for the user to press a key before closing.

## 7. Environment Variables

- **Viewing Environment Variables:**
  
```cmd
  set
  
```
  Displays all current environment variables and their values.

- **Setting an Environment Variable:**
  
```cmd
  set MYVAR=Hello
  
```
  Sets an environment variable `MYVAR` with the value `Hello`.

## 8. Disk Operations

- **Checking Disk Usage:**
  
```cmd
  chkdsk C:
  
```
  Checks the file system and file system metadata of the C: drive for logical and physical errors.

- **Formatting a Disk:**
  
```cmd
  format D: /FS:NTFS
  
```
  Formats the D: drive with the NTFS file system.

## 9. Echoing Messages

- **Displaying a Message:**
  
```cmd
  echo Hello, World!
  
```
  Prints `Hello, World!` to the console.

## 10. Redirecting Output

- **Redirecting Command Output to a File:**
  
```cmd
  dir > output.txt
  
```
  Redirects the output of the `dir` command to `output.txt`.

These examples illustrate some of the basic functionalities of Command Prompt. While cmd is less powerful compared to [PowerShell](#powershell), it remains useful for simple file system navigation, file management, and running legacy scripts.



# Common Security Vulnerabilities In Software Development {#common-security-vulnerabilities-in-software-development}


[Security](#security) vulnerabilities can be encountered and mitigated in [Software Development Portal](#software-development-portal).

In this not describe potential security risks in their applications.

Useful Tools
- [tool.bandit](#toolbandit)
## Examples

### Command Injection

General Description: Command injection is a security vulnerability that occurs when an attacker is able to execute arbitrary commands on the host operating system via a vulnerable application. This typically happens when user input is improperly handled and passed to a system shell.

Example: 
The `dangerous_subprocess` function uses `subprocess.call` with `shell=True`, which can lead to command injection if user input is not properly sanitized.
  
```python
  import subprocess
  def dangerous_subprocess(user_input):
      subprocess.call(user_input, shell=True)
  
```
Mitigation:
  - Avoid using `subprocess.call` with `shell=True`. Use `subprocess.run` or `subprocess.call` with a list of arguments.
  - Validate and sanitize user inputs ([Input is Not Properly Sanitized](#input-is-not-properly-sanitized)). 

### Hardcoded Password

General Description: Hardcoded passwords refer to credentials that are embedded directly in the source code. This practice is insecure as it exposes sensitive information and makes it difficult to change passwords without modifying the code.

Example:
The `hardcoded_password` function contains a hardcoded password, which is a common security issue.
  
```python
  def hardcoded_password():
      password = "123456"
      return password
  
```
  
Mitigation:
  - Use environment variables or configuration files to store sensitive information.
  - Consider using a secrets management tool.

### Use of `eval`

General Description: The `eval` function in Python evaluates a string as a Python expression. If not properly controlled, it can execute arbitrary code, leading to security vulnerabilities.

Example:
  - The `unsafe_eval` function uses `eval`, which can execute arbitrary code if the input is not controlled.
  
```python
  def unsafe_eval(user_input):
      return eval(user_input)
  
```
Mitigation:
  - Avoid using `eval`. Use safer alternatives like `ast.literal_eval`.
  - Ensure input is strictly controlled and sanitized.

### Insecure Deserialization

General Description: Insecure deserialization occurs when untrusted data is used to reconstruct objects. This can lead to arbitrary code execution, data tampering, or other malicious activities.

Example:
  - The `insecure_deserialization` function uses `pickle.loads`, which can be exploited if untrusted data is deserialized.
  
```python
  import pickle
  def insecure_deserialization(data):
      return pickle.loads(data)
  
```
Mitigation:
  - Avoid using `pickle` for untrusted data. Use safer formats like JSON ([Why JSON is Better than Pickle for Untrusted Data](#why-json-is-better-than-pickle-for-untrusted-data)).
  - Ensure data is from a trusted source.

### [SQL Injection](#sql-injection)

### Cross-Site Scripting (XSS)

**General Description**: Cross-Site Scripting (XSS) is a security vulnerability that allows an attacker to inject malicious scripts into content from otherwise trusted websites. It occurs when an application includes untrusted data in a web page without proper validation or escaping.

**Example**:
- The `display_user_input` function directly inserts user input into HTML, which can lead to XSS if the input is not properly sanitized.
```html
<div>
	<%= user_input %>
</div>
```
**Mitigation**:
- Escape user input before rendering it in HTML.
- Use security libraries or frameworks that automatically handle escaping

# Common Table Expression {#common-table-expression}


A Common Table Expression (CTE) is a temporary named result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement. 

The CTE can also be used in a [Views](#views). Serve as temporary views for a single [Querying|Queries](#queryingqueries).

```sql
WITH cte_query AS
(SELECT … subquery ...)
SELECT main query ... FROM/JOIN with cte_query ...
```

In [DE_Tools](#de_tools) see:
- https://github.com/rhyslwells/DE_Tools/blob/main/ExplorationsSQLite/Utilities/Common_Table_Expression.ipynb
### Non-Recursive CTE

The non-recursive are simple where CTE is used to <mark>avoid SQL duplication</mark> by referencing a name instead of the actual SQL statement. See [Views](#views) simplification usage.

```sql
WITH avg_per_store AS
  (SELECT store, AVG(amount) AS average_order
   FROM orders
   GROUP BY store)
SELECT o.id, o.store, o.amount, avg.average_order AS avg_for_store
FROM orders o
JOIN avg_per_store avg
ON o.store = avg.store;
```

### Recursive CTE

CTEs can be used in [Recursive Algorithm](#recursive-algorithm). The recursive query calls itself until the query satisfied the condition. In a recursive CTE, we should provide a where condition to terminate the recursion.

A recursive CTE is useful in querying hierarchical data such as organization charts where one employee reports to a manager or multi-level bill of materials when a product consists of many components, and each component itself also consists of many other components.

```sql
WITH levels AS (
  SELECT
    id,
    first_name,
    last_name,
    superior_id,
    1 AS level
  FROM employees
  WHERE superior_id IS NULL
  UNION ALL
  SELECT
    employees.id,
    employees.first_name,
    employees.last_name,
    employees.superior_id,
    levels.level + 1
  FROM employees, levels
  WHERE employees.superior_id = levels.id
)
 
SELECT *
FROM levels;
```


# Communication Principles {#communication-principles}


![Pasted image 20240916075433.png](../content/images/Pasted%20image%2020240916075433.png)

![Pasted image 20240916075439.png](../content/images/Pasted%20image%2020240916075439.png)



# Communication Techniques {#communication-techniques}


## Overview

Using these structured communication bridges can  enhance clarity and engagement, especially in spontaneous or high-stakes discussions.

Tips for Using Communication Bridges
1. Start Small: Begin by integrating 2-3 bridges that feel natural to you.
2. Observe Reactions: Notice how listeners respond when you clarify changes, summarize key points, or highlight actions.
3. Practice Consistency: Make these bridges a regular part of your speaking style.

[Speak More Clearly: 8 Precise Steps to Improve Communication](https://www.youtube.com/watch?v=Tc5dCLE_GP0)

### 1. Context Bridge

- Purpose: Aligns everyone by setting the context before diving into details.
- How to Use: Start with phrases like:
  - "At a high level..."
  - "This is our goal..."
  - <mark>"The main problem is..."</mark>
- Effect: Helps to focus thoughts and prevents initial rambling.

### 2. Change Bridge

- Purpose: Emphasizes shifts, trends, or significant moments in the discussion.
- How to Use: Use phrases that highlight changes, such as:
  - "Here's the before, and here's the after..."
  - "We’re shifting from X to Y..."`
  - "We are at a tipping point..."
- Effect: Grabs attention by making the change clear.

### 3. Insight Bridge

- Purpose: Shares deeper insights or unique perspectives, creating "aha" moments.
- How to Use: Key phrases include:
  - "Counterintuitively, ..."
  - "Here's what most people miss..."
  - "The deeper insight is..."
  - "The key point here is..."
- Effect: Signals that you’ve thought deeply, which moves the conversation forward.

### 4. Analysis Bridge

- Purpose: Anchors discussion in evidence, keeping it grounded.
- How to Use: Reference specific data points or comparisons with:
  - "The evidence shows..."
  - "The data indicates..."
  - "When we compared X and Y..."
- Effect: Focuses on facts, minimizing loss of direction.

### 5. Logical Transition Bridge

- Purpose: Provides a clear flow in the conversation, avoiding confusion.
- How to Use: Classic transitions include:
  - "First, second, third..."
  - "This leads to..."
  - "On the other hand..."
- Effect: Helps listeners follow along without losing the thread.

### 6. Summary Bridge

- Purpose: Ensures that key messages stay clear, especially in long discussions.
- How to Use: Frequently summarize main points with phrases like:
  - "The bottom line is..."
  - <mark>"If you remember one thing, it’s this..."</mark>
  - "To bring it back to the goal..."
- Effect: Reinforces the main message throughout the discussion.

### 7. Refinement Bridge

- Purpose: Allows for clarification or expansion of ideas as needed.
- How to Use: Rephrase or elaborate with:
  - "Let me break this down further..."
  - "Another way of looking at it is..."
  - "A useful analogy might be..."
- Effect: Clarifies complex points, helping everyone understand the core message.

### 8. Action Bridge

- Purpose: Concludes with actionable steps, defining the next moves.
- How to Use: Conclude with statements like:
  - <mark>"Our immediate priority is..."</mark>
  - "Here’s what we’ll do next..."
  - "The deliverables are..."
- Effect: Ends discussions with clear direction and accountability.


# Comparing Llm {#comparing-llm}


Use lmarena.ai as a bench marking tool. 

[LLM](#llm)

web dev arena

text to image leader board



# Components Of The Database {#components-of-the-database}

[Fact Table](#fact-table) in main table that [Dimension Table](#dimension-table) connect to them.

![Obsidian_CSP0FnAVD1.png](../content/images/Obsidian_CSP0FnAVD1.png)

# Computer Science {#computer-science}


[Algorithms](#algorithms)



# Concatenate {#concatenate}



# Conceptual Data Model {#conceptual-data-model}



# Conceptual Model {#conceptual-model}

Conceptual Model
   - Entities: Customer, Order, Book
   - Relationships: Customers place Orders, Orders include Books



Conceptual Model
   - Focuses on high-level business requirements.
   - Defines important data entities and their relationships.
   - Tools: [ER Diagrams](#er-diagrams), ER Studio, DbSchema.

# Concurrency {#concurrency}

In [DE_Tools](#de_tools) see:
- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Transactions/Concurrency.ipynb

# Confidence Interval {#confidence-interval}


A confidence interval is a range of values, derived from sample data, that is likely to contain the true population parameter. It is associated with a confidence level, such as 95%, indicating the probability that the interval captures the true parameter.

Key Points
- **Confidence Level:** The likelihood that the interval includes the true parameter (e.g., 95%).
- **Purpose:** Quantifies the uncertainty of an estimate, providing a range rather than a single value.
### Example
- A 95% confidence interval for a mean of (50, 60) suggests that, in repeated sampling, 95% of such intervals would contain the true mean.


# Confusion Matrix {#confusion-matrix}


A Confusion Matrix is a table used to evaluate the performance of a [Classification](#classification) model. It provides a detailed breakdown of the model's predictions across different classes, showing the number of true positives, true negatives, false positives, and false negatives.
## Purpose

- The confusion matrix helps identify where the classifier is making errors, indicating where it is "confused" in its predictions.
## Structure


![Pasted image 20240120215414.png](../content/images/Pasted%20image%2020240120215414.png)

## Structure

- True Positives (TP): Correctly predicted positive instances.
- False Positives (FP): Incorrectly predicted positive instances (Type 1 error).
- True Negatives (TN): Correctly predicted negative instances.
- False Negatives (FN): Incorrectly predicted negative instances (Type 2 error).

## Metrics

- [Accuracy](#accuracy): The overall percentage of correct predictions. In this case, the accuracy is 78.3%.
- [Precision](#precision): The ratio of true positives to all positive predictions (including both TPs and FPs). In this case, the precision for class 0 is 85.7% and the precision for class 1 is 66.4%.
- [Recall](#recall): The ratio of true positives to all actual positive cases (including both TPs and FNs). In this case, the recall for class 0 is 80.6% and the recall for class 1 is 74.1%.
- [F1 Score](#f1-score): A harmonic average of precision and recall. In this case, the F1-score for class 0 is 83.0% and the F1-score for class 1 is 70.0%.
- [Specificity](#specificity)
- [Recall](#recall)

## Further Examples
![Pasted image 20240116205937.png|500](../content/images/Pasted%20image%2020240116205937.png|500)

![Pasted image 20240116210541.png|500](../content/images/Pasted%20image%2020240116210541.png|500)

## Example Code

```python
from sklearn.metrics import confusion_matrix

# Assuming y_train and y_train_pred are your true and predicted labels
conf_matrix = confusion_matrix(y_train, y_train_pred)
print(conf_matrix)
```

Example Output:

```
array([[377, 63],
       [ 91, 180]], dtype=int64)
```

# Continuous Delivery   Deployment {#continuous-delivery---deployment}

Continuous Delivery
   - Ensures that code changes are automatically prepared for a release to production.
   - Builds, tests, and releases are automated, but the deployment is manual.

Continuous Deployment:
   - Extends continuous delivery by automating the deployment process.
   - Every change that passes the automated tests is deployed to production automatically.

[Model Deployment](#model-deployment)

A continuous integration and continuous deployment (CI/CD) pipeline is **a series of steps that must be performed in order to deliver a new version of software**

# Continuous Integration {#continuous-integration}

   - Developers frequently integrate code into a shared repository.
   - Automated builds and tests are run to detect issues early.
   - Encourages smaller, more manageable code changes.

# Converting Categorical Variables To A Dummy Indicators {#converting-categorical-variables-to-a-dummy-indicators}



# Convolutional Neural Networks {#convolutional-neural-networks}


Convolutional networks, or CNNs, are specialized [Deep Learning](#deep-learning) architectures designed for processing data with grid-like structures, such as images. 

They use convolutional layers with learnable filters to extract spatial features from the input data. The convolutional operation involves sliding these filters across the input, performing element-wise multiplications and summations to create feature maps. 

CNNs are particularly effective for image classification, object detection, and image segmentation tasks.

Primarily used in image recognition and processing tasks. CNNs use convolutional layers to automatically detect spatial patterns in images, like edges and textures.

Pooling:

The idea of pooling in convolutional neural networks is to do two things:
- Reduce the number of parameters in your network (pooling is also called “down-sampling” for this reason)
- To make feature detection ([Feature Extraction](#feature-extraction)) more robust by making it more impervious to scale and orientation changes
- shrink multiple data to single points.

![Pasted image 20241006124829.png|500](../content/images/Pasted%20image%2020241006124829.png|500)

![Pasted image 20241006124735.png|500](../content/images/Pasted%20image%2020241006124735.png|500)


# Correlation Vs Causation {#correlation-vs-causation}

What is the meaning of [Correlation](#correlation) does not imply causation?

Correlation measures the statistical association between two variables, while causation implies a cause-and-effect relationship. 


- **Correlation**: Indicates an association between variables but does not imply that changes in one variable cause changes in the other.
- **Causation**: Suggests a direct cause-and-effect relationship between variables, requiring experimentation to establish.


# Correlation {#correlation}


Use in understanding relationships between variables in data analysis. 

While it helps identify associations, it's important to remember that <mark>correlation does not imply causation.</mark> 

Visualization tools like heatmaps and clustering can aid in identifying and interpreting these relationships effectively.

- What is Correlation?: A measure of the strength and direction of the relationship between two variables.
### Description

- Correlation measures the relationship between two variables, indicating how they change together. It ranges from -1 to 1:

  - -1: Perfect negative correlation
  - 0: No correlation
  - 1: Perfect positive correlation

### Key Points

- [Correlation vs Causation](#correlation-vs-causation): Correlation does not imply causation. While correlation highlights associations, causation establishes a direct influence.
- Significance: Correlation values < -0.5 or > +0.5 are considered significant.
- Impact of Outliers: [standardised/Outliers](#standardisedoutliers) can distort correlation results.
- Standardization: Correlation is a standardized version of [Covariance](#covariance).

### Model Preparation

[Feature Selection](#feature-selection):
  - Identify features correlated with the target. If all are correlated, keep all.
  - For features correlated with each other, consider dropping one to avoid redundancy.
  - If two features are highly correlated with the target, both can be retained.

If two variables are strongly positively correlated, it often makes sense to drop one of them to simplify the model. This is because <mark>highly correlated variables can introduce redundancy</mark>, leading to [multicollinearity](#multicollinearity) in regression models.

By removing one of the correlated variables, you can:

1. Reduce Complexity: Simplifying the model by reducing the number of predictors can make it easier to interpret and manage.
2. Improve Stability: Reducing multicollinearity can lead to more stable and reliable coefficient estimates.
3. Enhance Performance: In some cases, removing redundant features can improve the model's predictive performance by reducing overfitting.

However, it's important to ensure that the variable you choose to keep is the one that is more relevant or has a stronger theoretical justification for inclusion in the model. 

### Viewing Correlations

- Use [Heatmap](#heatmap) or [Clustering](#clustering) to visualize correlations between features.

### Example Code

To find the correlation between two features:

```python
df['var1', 'target'](#var1-target).groupby(['var1'], as_index=False).mean().sort_values(by='target', ascending=False)
```


# Cosine Similarity {#cosine-similarity}

Cosine similarity is a [Metric](#metric) used to measure how similar two vectors are by calculating the cosine of the angle between them. It ranges from -1 to 1.where 1 indicates identical orientation, 0 indicates orthogonality, and -1 indicates opposite orientation. 

Cosine similarity is commonly used in
- text analysis, 
- information retrieval, 
- recommendation systems to compare document similarity, user preferences, or item features.

In [Binary Classification](#binary-classification), cosine similarity can be used as a feature to help distinguish between two classes. For instance, in text classification tasks, you might represent documents as vectors using techniques like [TF-IDF](#tf-idf). 


# Cost Function {#cost-function}

The concept of a Cost Function is central to [Model Optimisation](#model-optimisation), particularly in training models.

A cost function, also known as a loss function or error function, is a mathematical function used in optimization and machine learning to measure the difference between predicted values and actual values. It quantifies the error or "cost" of a model's predictions. The goal of many machine learning algorithms is to minimize this cost function, thereby improving the accuracy of the model. Common examples of cost functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.

1. Relation to [Loss Function](#loss-function): The cost function is related to the loss function. While the loss function measures the error for a single data point, the cost function typically aggregates these errors over the entire dataset, often by taking an average. See [Loss versus Cost function](#loss-versus-cost-function).

3. Parameter Space ([Model Parameters](#model-parameters)) and Surface Plotting: By plotting the cost function over the parameter space, you can visualize how different parameter values affect the cost. This surface can have various peaks and valleys representing different levels of error.

4. [Gradient Descent](#gradient-descent): This is an optimization algorithm used to find the minimum of the cost function. By iteratively adjusting the parameters in the direction that reduces the cost, gradient descent helps in finding the optimal parameters for the model.

5. Caveats: The cost function is dependent on the dataset and may not always have an explicit formula. This means that the shape of the cost function surface can vary greatly depending on the data, and finding the global minimum can be challenging.



![Pasted image 20241216202825.png|500](../content/images/Pasted%20image%2020241216202825.png|500)

![Pasted image 20241216202917.png|500](../content/images/Pasted%20image%2020241216202917.png|500)


**[Reward Function](#reward-function)**: Mentioned as the opposite of a cost function, typically used in [Reinforcement learning](#reinforcement-learning) to indicate the desirability of an outcome.

# Covariance {#covariance}



In statistics, covariance is a measure of the degree to which two random variables change together. It indicates the direction of the linear relationship between the variables. Specifically, covariance can be defined as follows:

- **Positive Covariance**: If the covariance is positive, it means that as one variable increases, the other variable tends to also increase. Conversely, if one variable decreases, the other variable tends to decrease as well.
  
- **Negative Covariance**: If the covariance is negative, it indicates that as one variable increases, the other variable tends to decrease, and vice versa.

- **Zero Covariance**: A covariance close to zero suggests that there is no linear relationship between the two variables.

The formula for calculating the covariance between two random variables $X$ and $Y$ is given by:

{% raw %}
$$
\text{Cov}(X, Y) = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})
$$
{% endraw %}

where:
- $X_i$ and $Y_i$ are the individual sample points,
- $\bar{X}$ and $\bar{Y}$ are the means of $X$ and $Y$ respectively,
- $n$ is the number of data points.

Covariance is used in:
- in the calculation of [correlation](#correlation) coefficients 
- and in multivariate statistics, such as in [Gaussian Mixture Models](#gaussian-mixture-models) where it helps describe the shape and orientation of the data distribution.

# Covering Index {#covering-index}

Like an [Database Index|Index](#database-indexindex) but for partial indexes?

# Cron Jobs {#cron-jobs}



# Cross Entropy {#cross-entropy}


Cross entropy is a [Loss function](#loss-function) used in [Classification](#classification) tasks, particularly for [categorical data](#categorical-data). The cross entropy loss function is particularly effective for multi-class classification problems, where the goal is to assign an input to one of several categories. 

<mark>Cross entropy measures confidence.</mark>

Cross entropy works by measuring the (difference/loss) <mark>dissimilarity between two probability distributions</mark>: the true distribution (actual class labels) and the predicted distribution (model's output probabilities). 

Fit of Predictions:
- A low cross entropy loss means the predicted probabilities are close to the true labels (e.g., assigning high probability to the correct class).
- A high loss indicates significant divergence, meaning the model's predictions are inaccurate or uncertain.

By minimizing cross entropy, the model learns to produce probability distributions that closely match the true class distributions, thereby improving its classification <mark>accuracy</mark>.

1. Probability Distributions: In a classification task, the model outputs a probability distribution over the possible classes for each input. For example, in a three-class problem, the model might output probabilities like [0.7, 0.2, 0.1] for classes A, B, and C, respectively.

2. True Labels: The true class label is represented as a one-hot encoded vector. If the true class is A, the vector would be [1, 0, 0].

3. Cross Entropy Calculation calculates the loss by comparing the predicted probabilities with the true labels. The formula for cross entropy loss $L$ for a single instance is:

   $$ L = -\sum_{i=1}^{N} y_i \log(p_i)$$

   where:
   - $N$ is the number of classes.
   - $y_i$ is the true label (1 if the class is the true class, 0 otherwise).
   - $p_i$ is the predicted probability for class $i$.

2. Interpretation: The cross entropy loss increases as the predicted probability diverges from the actual label. If the model assigns a high probability to the correct class, the loss is low. Conversely, if the model assigns a low probability to the correct class, the loss is high.

3. Optimization: During training, the model's parameters are adjusted to minimize the cross entropy loss across all training examples. This process helps the model improve its predictions over time.

## Where is it used

Cross entropy is widely used in classification for several reasons:

Probabilistic Modeling:
    - It directly aligns with the goals of probabilistic classifiers, as it measures how well the predicted probability distribution matches the true distribution.
    
Focus on Confidence:
    - Encourages the model to assign higher probabilities to the correct classes, improving not just accuracy but also confidence in predictions.

Optimization Efficiency:
    - Cross entropy is smooth and convex for logistic regression-like models, enabling efficient gradient-based optimization.

Multi-Class Support:
    - Works seamlessly in multi-class scenarios where the true labels are one-hot encoded and predictions are probability distributions.

### Implementation 

In [ML_Tools](#ml_tools) see: 
- [Cross_Entropy_Single.py](#cross_entropy_singlepy)
- [Cross_Entropy.py](#cross_entropypy)
- [Cross_Entropy_Net.py](#cross_entropy_netpy)





# Cross Validation {#cross-validation}


Cross-validation is a statistical technique used in machine learning to <mark>assess how well a model will generalize</mark> to an independent dataset. It is a crucial step in the model-building process because it helps ensure that the model is not [overfitting](#overfitting) or underfitting the training data.

- Cross-validation is a technique used in machine learning and statistics to evaluate the performance ([Model Optimisation](#model-optimisation)) of a predictive model.
- It provides a robust evaluation by splitting the training data into smaller chunks and training the model multiple times.
- K-Fold Cross-Validation: Involves dividing the dataset into \( k \) equal-sized subsets (called "folds") and using each fold as a validation set once, while the remaining \( k-1 \) folds are used for training.
- The model's performance is averaged across all \( k \) folds to provide a more robust estimate of its generalization performance.
### Common Variations

- K-Fold Cross-Validation: The most common method, where the data is split into \( k \) folds and the model is trained \( k \) times, each time using a different fold as the validation set.
- Stratified K-Fold: Ensures each fold has a similar proportion of class labels, important for imbalanced datasets.
- Repeated K-Fold: Repeats the process multiple times with different random splits for more robust results.
- Leave-One-Out Cross-Validation (LOOCV): Each data point is used once as a test set while the rest serve as the training set.

### How Cross-Validation Fits into Building a Machine Learning Model

1. [Model Evaluation](#model-evaluation): Used to evaluate the performance of different models or algorithms to choose the best one.
2. [Hyperparameter](#hyperparameter) Tuning: Provides a reliable performance metric for each set of hyperparameters.
3. [Model Validation](#model-validation): Ensures consistent performance across different subsets of data.
4. [Bias and variance](#bias-and-variance) tradeoff: Helps in understanding the tradeoff between bias and variance, guiding the choice of model complexity.

Advantages:
- Reduced Bias: Offers a more reliable performance estimate compared to using a single validation set.
- Efficient Data Use: All data is used for both training and validation.
- Prevents Overfitting: By evaluating on multiple folds, it can detect if the model is overfitting to the training data.
### Choosing \( k \)

- Common values: 5 or 10
- Higher \( k \) leads to more accurate estimates but increases computation time.
- Consider dataset size and complexity when choosing \( k \).

### Code Implementation

In [ML_Tools](#ml_tools) see:
- [KFold_Cross_Validation.py](#kfold_cross_validationpy)

### Cross-Validation Strategy in [Time Series](#time-series)

All notebooks use cross-validation based on `TimeSeriesSplit` to ensure proper evaluation of performance with no [Data Leakage](#data-leakage). This method ensures that training and test data are split while maintaining the chronological order of the data.

# Cross_Entropy.Py {#cross_entropypy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Model_Evaluation/Classification/Cross_Entropy.py
### Generalized Script Description:

1. **Dataset**: Uses the Iris dataset from `sklearn` to classify flower species.
2. **Preprocessing**: One-hot encodes the target labels and splits the data into training and testing sets.
3. **Model**: Trains a multinomial logistic regression model to predict probabilities for each class.
4. **Cross Entropy Calculation**: Computes cross entropy loss for all predictions in the test set.
5. **Visualization**: Plots a histogram to show the distribution of loss values across the test samples.
6. **Summary Statistics**: Outputs mean, median, maximum, and minimum loss values for analysis.

This approach provides insight into the model's performance by analyzing the spread and typical values of cross entropy loss over multiple predictions.

### Strengths:

1. **Real-World Dataset**: The Iris dataset is well-known and intuitive, making it easier to follow and validate the results.
2. **Generalization**: The script calculates the cross entropy loss for multiple predictions, demonstrating the loss function in a real-world, multi-class classification scenario.
3. **Insights Through Visualization**: The histogram of losses provides a clear picture of how well the model performs across different test samples.
4. **Statistical Summary**: The inclusion of mean, median, max, and min loss values gives a quick overview of the model's performance.
5. **Numerical Stability**: The small epsilon value in the log computation ensures stability when dealing with probabilities close to zero.
6. **Reproducibility**: Using `sklearn`'s preprocessing and modeling tools ensures that the example is easy to replicate.

### Possible Enhancements:

1. **Alternative Models**: Incorporating another model (e.g., a neural network) could showcase the versatility of cross entropy in various settings.
2. **Analysis of Misclassifications**: Add a breakdown of where the model performed poorly and why (e.g., confusion matrix analysis).
3. **Feature Exploration**: Include visualizations or explanations of feature importance to show how the model makes decisions.
4. **Comparative Losses**: Compare cross entropy loss with other loss functions (e.g., mean squared error) to highlight its advantages in classification.

**Distribution Insights**:

- The histogram of loss values shows how well the model performs across the test dataset.
    - A **narrow distribution** around a low value suggests consistent, accurate predictions.
    - A **wide or skewed distribution** indicates variability in the model's performance, with some instances being predicted poorly.

### [Mean Squared Error](#mean-squared-error) versus [Cross Entropy](#cross-entropy)

- **When Comparison Makes Sense**:
    - MSE can highlight how "far off" the predicted probabilities are in terms of magnitude but doesn’t account for the probabilistic nature of classification tasks.
    - Comparing cross entropy with MSE can show:
        - How the model performs when considering confidence (cross entropy).
        - How the model performs when focusing on numerical proximity (MSE).
        
- **Insights Gained**:
    - If cross entropy is low but MSE is high, it might indicate that the model predicts probabilities close to the correct class but has poor numerical calibration for other classes.
    - If both are high, the model is likely underperforming across the board.


# Cross_Entropy_Single.Py {#cross_entropy_singlepy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Selection/Model_Evaluation/Classification/Cross_Entropy_Single.py
## Example

Let's consider a three-class classification problem with classes A, B, and C. Suppose we have a single data point with the true class label being A. The true label in one-hot encoded form would be [1, 0, 0].

Assume the model predicts the following probabilities for this data point:

- Probability of class A: 0.7
- Probability of class B: 0.2
- Probability of class C: 0.1

The predicted probability vector is [0.7, 0.2, 0.1].

To calculate the cross entropy loss for this example, we use the formula:

$L = -\sum_{i=1}^{N} y_i \log(p_i)$

Substituting the values:

- For class A: $y_1 = 1$ and $p_1 = 0.7$
- For class B: $y_2 = 0$ and $p_2 = 0.2$
- For class C: $y_3 = 0$ and $p_3 = 0.1$

The cross entropy loss $L$ is calculated as:

$L = -(1 \cdot \log(0.7) + 0 \cdot \log(0.2) + 0 \cdot \log(0.1))$

$L = -(\log(0.7))$

$L \approx -(-0.3567) = 0.3567$

So, the cross entropy loss for this example is approximately 0.3567. This value represents the penalty for the model's predicted probabilities not perfectly matching the true class distribution. The lower the loss, the better the model's predictions align with the true labels.

### Script Description:

1. **Cross Entropy Function**: Computes the cross entropy loss given true labels and predicted probabilities.
2. **True and Predicted Probabilities Visualization**: Bar plots display the true one-hot encoded labels and the predicted probability distribution.
3. **Cross Entropy Loss Calculation**: Prints the loss value for a sample data point.
4. **Loss Curve**: A line graph shows how the loss changes as the predicted probability for the true class increases.

# Crosstab {#crosstab}

Used to compute a simple cross-tabulation of two (or more) factors. It is particularly useful for computing frequency tables. Here's an example:

```python
# Sample DataFrame
df = pd.DataFrame({
    'Category': ['A', 'B', 'A', 'B', 'A'],
    'Subcategory': ['X', 'X', 'Y', 'Y', 'X']
})

# Cross-tabulation of 'Category' and 'Subcategory'
crosstab = pd.crosstab(df['Category'], df['Subcategory'])
print(crosstab)
```

Input
```
  Category Subcategory
0        A           X
1        B           X
2        A           Y
3        B           Y
4        A           X
```

Output:
```
Subcategory  X  Y
Category         
A            2  1
B            1  1
```

In [DE_Tools](#de_tools) see:
- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/reshaping.ipynb

# Crud {#crud}

Create,Read,Update,Delete.

# Cryptography {#cryptography}


Cryptography is the foundation of digital [Security](#security), enabling privacy and secure communication over the internet.

Examples are implemented in [Node.JS](#nodejs) (using `crypto` module) and are written in [JavaScript](#javascript).

Resources:
- [7 Cryptography Concepts EVERY Developer Should Know](https://www.youtube.com/watch?v=NuyzuNBFWxQ)
- https://fireship.io/lessons/node-crypto-examples/
## [Hash](#hash) (Chop and mix)

A hashing function takes an input of any length and outputs a fixed-length value, ensuring:

- The same input always produces the same output.
- It is computationally expensive to reverse the hash.
- It has a low probability of collisions.

### Create a Hash in Node.js

```javascript
const { createHash } = require('crypto');

function hash(str) {
    return createHash('sha256').update(str).digest('hex');
}

let password = 'hi-mom!';
const hash1 = hash(password);
console.log(hash1);

password = 'hi-mom';
const hash2 = hash(password);
console.log(hash1 === hash2 ? '✔️ Good password' : '❌ Password does not match');
```

## Salting

Salting strengthens hashes by appending a random string before hashing, preventing attacks using precomputed hash tables.

### Password Salt with Scrypt in Node.js

```javascript
const { scryptSync, randomBytes, timingSafeEqual } = require('crypto');

function signup(email, password) {
    const salt = randomBytes(16).toString('hex');
    const hashedPassword = scryptSync(password, salt, 64).toString('hex');
    users.push({ email, password: `${salt}:${hashedPassword}` });
}

function login(email, password) {
    const user = users.find(v => v.email === email);
    if (!user) return 'login fail!';
    
    const [salt, key] = user.password.split(':');
    const hashedBuffer = scryptSync(password, salt, 64);
    const match = timingSafeEqual(hashedBuffer, Buffer.from(key, 'hex'));
    return match ? 'login success!' : 'login fail!';
}

const users = [];
signup('foo@bar.com', 'pa$$word');
console.log(login('foo@bar.com', 'password'));
```

## HMAC (Hash-based Message Authentication Code)

HMAC combines a hash with a secret key, ensuring authenticity and integrity.

### HMAC in Node.js

```javascript
const { createHmac } = require('crypto');

const password = 'super-secret!';
const message = '🎃 hello jack';

const hmac = createHmac('sha256', password).update(message).digest('hex');
console.log(hmac);
```

## Symmetric Encryption

Symmetric encryption uses the same key to encrypt and decrypt data.

### Symmetric Encryption in Node.js

```javascript
const { createCipheriv, randomBytes, createDecipheriv } = require('crypto');

const message = 'i like turtles';
const key = randomBytes(32);
const iv = randomBytes(16);
const cipher = createCipheriv('aes256', key, iv);
const encryptedMessage = cipher.update(message, 'utf8', 'hex') + cipher.final('hex');

const decipher = createDecipheriv('aes256', key, iv);
const decryptedMessage = decipher.update(encryptedMessage, 'hex', 'utf-8') + decipher.final('utf8');
console.log(`Decrypted: ${decryptedMessage}`);
```

## Keypairs

Keypairs consist of a public key (shared) and a private key (kept secret) for secure communication.

### Generate an RSA Keypair in Node.js

```javascript
const { generateKeyPairSync } = require('crypto');

const { privateKey, publicKey } = generateKeyPairSync('rsa', {
  modulusLength: 2048,
  publicKeyEncoding: { type: 'spki', format: 'pem' },
  privateKeyEncoding: { type: 'pkcs8', format: 'pem' },
});

console.log(publicKey);
console.log(privateKey);
```

## Asymmetric Encryption

Asymmetric encryption encrypts with a public key and decrypts with a private key, securing communication over networks.

### RSA Encryption in Node.js

```javascript
const { publicEncrypt, privateDecrypt } = require('crypto');
const { publicKey, privateKey } = require('./keypair');

const secretMessage = 'Confidential message';
const encryptedData = publicEncrypt(publicKey, Buffer.from(secretMessage));
console.log(encryptedData.toString('hex'));

const decryptedData = privateDecrypt(privateKey, encryptedData);
console.log(decryptedData.toString('utf-8'));
```

## Signing

Signing verifies the authenticity of a message by hashing it and encrypting the hash with a private key.

### RSA Signing in Node.js

```javascript
const { createSign, createVerify } = require('crypto');
const { publicKey, privateKey } = require('./keypair');

const data = 'this data must be signed';
const signer = createSign('rsa-sha256');
signer.update(data);
const signature = signer.sign(privateKey, 'hex');
console.log(signature);

const verifier = createVerify('rsa-sha256');
verifier.update(data);
const isVerified = verifier.verify(publicKey, signature, 'hex');
console.log(isVerified);
```

# Current Challenges Within The Energy Sector {#current-challenges-within-the-energy-sector}


[Current challenges within the energy sector](#current-challenges-within-the-energy-sector) related to reinforcement learning and that can be progressed with recent technological advances



# Dagster {#dagster}


[Dagster](https://dagster.io/) is a [data orchestrator] focusing on data-aware scheduling that supports the whole development [Data Lifecycle Management](#data-lifecycle-management)  lifecycle, with integrated lineage and observability, a [declarative](#declarative) programming model, and best-in-class testability.

Key features are: 
- Manage your data assets with code
- A single pane of glass for your data platform 

# Dash {#dash}


**Dash** is an open-source framework for building interactive web applications using Python. 

It is particularly well-suited for data visualization and dashboard creation. 

Dash integrates  with popular libraries such as Plotly, Pandas, and NumPy, making it ideal for creating dynamic and interactive visualizations.

In [ML_Tools](#ml_tools) see [Clustering_Dashboard.py](#clustering_dashboardpy)

Key Components of Dash
1. **Dash App**: The main application instance, created using `dash.Dash(__name__)`.
2. **Dash HTML Components (`dash_html_components`)**: Provides wrappers for standard HTML elements (e.g., `html.Div`, `html.H1`).
3. **Dash Core Components (`dash_core_components`)**: Includes interactive UI components like graphs, dropdowns, sliders, and more (e.g., `dcc.Graph`, `dcc.Dropdown`).
4. **Callback Functions**: Used to make components interactive by linking inputs (user actions) to outputs (changes in the UI).
5. **[Plotly](#plotly) Integration**: Dash apps leverage Plotly for creating interactive visualizations.

# Dashboarding {#dashboarding}




[Dash](#dash)
[Streamlit.io](#streamlitio)



# Data Ai Education At Work {#data-ai-education-at-work}


### Introduction

Organizations are increasingly recognizing the importance of integrating data and AI learning into their people strategies. This involves practical steps to ensure employees are equipped with the necessary skills to leverage these technologies effectively.

Integrating data and AI education into organizational strategies is essential for maintaining competitiveness and fostering a culture of continuous learning. By addressing these areas, organizations can better prepare their workforce for the evolving technological landscape.

### Practical Steps for Integration

1. **Access to Training**:
   - Provide clear guidance on how to access training courses.
   - Offer details on accessing training funds and budgets.
   - Partner with training providers to offer relevant courses.

2. **Learning Resources**:
   - Collect and distribute clear and concise training materials.
   - Capture, document, and discuss use cases from staff experiences.
   - Encourage peer-to-peer learning and collaboration.

3. **Organizational Support**:
   - Align training with professional competencies.
   - Foster communication and collaboration with external partners.
   - Encourage staff to experiment with AI tools to enhance efficiency.

4. **Governance and Strategy**:
   - Establish governance and skill strategies before deploying AI.
   - Develop acceptable use policies for AI tools.
   - Recognize that adopting AI is a gradual process requiring leadership support.

### Fostering a Culture of Continuous Learning

1. **Leadership and Culture**:
   - Connect learning initiatives with employee incentives and pay.
   - Allow time and space for learning by alleviating workloads.
   - Protect training time and build networks to showcase use cases.

2. **Mindset and Adaptability**:
   - Promote digital literacy and adaptability among employees.
   - Encourage openness to new ideas and recognize skills beyond the technical team.

### Business Risks of Not Upskilling

1. **Competitive Disadvantage**:
   - Risk of losing competitive and productivity edges.
   - Potential loss of market differentiation as competitors advance.

2. **Staff Retention**:
   - Risk of losing skilled staff uncomfortable with new technologies.
   - Employees may move to companies at the cutting edge of AI.

3. **Operational Challenges**:
   - Inconsistencies in ways of working between partner organizations.
   - Inappropriate use of AI tools by untrained staff.

4. **Productivity and Trust**:
   - AI's potential to enhance productivity is yet to be fully realized.
   - Trust and verifiability of AI systems (black boxes) are crucial for business benefits.


# Data Analysis Portal {#data-analysis-portal}

[Data Analyst](#data-analyst)

[Data Visualisation](#data-visualisation)

# Data Analysis {#data-analysis}

What is it? Usually done with a [Data Analyst](#data-analyst).After processing, data is analyzed to extract meaningful insights and derive value from the data.
### Types of analysis:

Exploration and understanding:
- [EDA](#eda): Involves exploring data sets to find patterns, anomalies, or relationships without having a specific hypothesis in mind. It is often used in the initial stages of data analysis to generate insights.
- Descriptive: <mark>Focuses on summarizing historical data to understand what has happened</mark> in the past. It often involves the use of [Statistics](#statistics) measures and [Data Visualisation](#data-visualisation) tools to present data trends and patterns.
- Diagnostic: <mark>Seeks to understand why something happened</mark>. It involves examining data to identify causes and correlations, often using techniques like data mining and statistical analysis.

Forward looking:
- Predictive: Uses historical data and statistical algorithms to <mark>forecast future outcome</mark>s. It helps in identifying trends and making predictions about future events based on past data.
- Prescriptive: Goes a step further by <mark>recommending actions based on the predictions made</mark>. It uses optimization and simulation algorithms to suggest the best course of action for a given situation.
- Inferential: Makes inferences and predictions about a population based on a sample of data. It often involves [Hypothesis testing](#hypothesis-testing) and [Confidence Interval](#confidence-interval).

# Data Analyst {#data-analyst}

Summary:
- Gathers and processes data to generate reports.
- Communicates insights and findings to management
- Conducts [Data Analysis](#data-analysis).

### Key responsibilities of a data analyst:

- Define Objectives: Clearly outline the goals of the analysis to guide the process.
  
- [Data Collection](#data-collection): Gather relevant data from various sources, ensuring accuracy, completeness, and timeliness.
  
- [Data Cleansing](#data-cleansing): Clean the data to remove errors, duplicates, and inconsistencies for reliable findings.
  
- Data Exploration: Perform exploratory data analysis ([EDA](#eda)) to understand data structure, [Distributions|distribution](#distributionsdistribution), and relationships.
  
- Choose the Right Tools: Utilize appropriate tools and software for analysis, such as Excel, R, Python, SQL, or specialized platforms.
  
- [Statistics](#statistics): Apply various statistical methods and techniques, such as regression analysis, clustering, and [hypothesis testing](#hypothesis-testing).
  
- [Data Visualisation](#data-visualisation): Use visualization techniques to effectively present findings and communicate insights.
  
- Interpret Results: Analyze results in the context of objectives and consider implications for decision-making.
  
- Documentation: Maintain thorough [Documentation & Meetings](#documentation--meetings) of the analysis process, including data sources, methodologies, and findings.
  
- Continuous Learning: Stay updated with the latest tools, techniques, and best practices in the evolving field of data analysis.

# Data Architect {#data-architect}

Data Architect
  - Designs and manages the data infrastructure.
  - Ensures data is stored, organized, and accessible for analysis.

# Data Archive Graph Analysis {#data-archive-graph-analysis}


Use the following to 

[Dataview](#dataview)
[Graph View](#graph-view)

Check out [Graph Analysis Plugin](#graph-analysis-plugin)

Convert Dataview to CSV


# Data Asset {#data-asset}



# Data Cleansing {#data-cleansing}


Data cleansing is the process of correcting or removing inaccurate, incomplete, or inconsistent data to improve its [Data Quality](#data-quality) for analysis. Involves:

- [standardised/Outliers|Handling Outliers](#standardisedoutliershandling-outliers)
- [Handling Missing Data](#handling-missing-data)
- [Handling Different Distributions](#handling-different-distributions)

In [DE_Tools](#de_tools) see:
- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Cleaning/Dataframe_Cleaing.ipynb
- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Cleaning

Related terms:
- [Data Selection](#data-selection)

Follow-up questions:
- [Deleting rows or filling them with the mean is not always best ](#deleting-rows-or-filling-them-with-the-mean-is-not-always-best)



# Data Collection {#data-collection}

Determine the [Data Quality](#data-quality) and quantity of data required and get it.

[Imbalanced Datasets](#imbalanced-datasets)

# Data Contract {#data-contract}

### [Data Contract](#data-contract)

pattern to handle schema changes

Pattern to apply to organisation using tools they have.

Tooling:
- [dbt](#dbt)

Data contracts help prevent preventable data issues while increasing collaboration and reducing costs.

A data contract is an agreed interface between
the generators of data and its consumers. It sets
the expectations around that data, defines how
it should be governed, and facilitates the
explicit generation of quality data that meets
the business requirements.

Interfaces:
- [API](#api) for data.

A document to codify what has been agreed.

Q: How does the Data Contract allow for contextual rules? Example the same schema can support multiple products in our org but the DQ rules can be different for different Products
A: Data contracts are particular business. Could use <mark>inheritance</mark> of rules in data contracts - basic template. To get standardisation across products.

[Data Contract](#data-contract)
By establishing a [data contract](#data-contract) and building interfaces based on it, organizations can improve data quality. Implementing structured agreements and automated change management processes can help business users, who may not be data experts, produce higher-quality data ([Data Quality](#data-quality)).

### Images

![Pasted image 20250312163351.png](../content/images/Pasted%20image%2020250312163351.png)

# Data Distribution {#data-distribution}

Data distribution refers to the process of making processed and analyzed data available for downstream applications and systems. 

This can involve supplying data to
- business applications, 
- reporting systems, 
- or other data-driven processes, 
- ensuring that stakeholders

have access to the information they need for decision-making and operations.

# Data Drift {#data-drift}

Data drift refers to changes in the statistical properties of input data that a machine learning (ML) model encounters during production. Such shifts can lead to decreased model performance, as the model may struggle to make accurate predictions on data that differ from its training set. 

Regular monitoring and prompt response to data drift are essential to maintain the effectiveness of ML models in dynamic production environments.

Concepts:

- Data drift involves changes in input data distributions
- Concept drift pertains to alterations in the relationship between inputs and outputs.
- [Performance Drift](#performance-drift) drift relates to changes in model outputs. 

**Training-Serving Skew:** This refers to discrepancies between training data and production data, which can arise from data drift or other factors, leading to performance issues. 

<mark>**Detecting Data Drift:**</mark>

Identifying data drift is crucial for maintaining model accuracy. Techniques include:

- **Statistical Hypothesis Testing:** Assessing whether differences between training and production data distributions are statistically significant.

- **Distance Metrics:** Quantifying the divergence between data distributions using measures like Kullback-Leibler divergence or Kolmogorov-Smirnov tests.

- **Monitoring Summary Statistics:** Regularly reviewing key statistical indicators (e.g., mean, variance) of input features to detect anomalies.

**Addressing Data Drift:**

Once detected, strategies to manage data drift include:

1. **Data Quality Checks:** Ensure that the drift isn't due to data quality issues, such as errors in data collection or processing. 

2. **Investigate the Drift:** Analyze the source and nature of the drift to understand its implications.

3. **Model Retraining:** Update the model using recent data to help it adapt to new patterns.

4. **Model Rebuilding:** In cases of significant drift, it may be necessary to redesign the model architecture or feature engineering processes.

5. **Fallback Strategies:** Implement alternative decision-making processes, such as rule-based systems or human judgment, when the model's reliability is compromised.





 

# Data Engineer {#data-engineer}


The primary responsibility of a data engineer is to take data from its source and make it available for analysis. They focus on
- automating the data collection, 
- processing, 
- and analysis workflows,
- solving how systems manage and handle the flow of data. 

<mark>Develops data pipelines and ensures data flow between systems.</mark>

Resources:
- [Link](https://www.youtube.com/watch?v=qWru-b6m030)

### Key Responsibilities:

1. Infrastructure Design and Maintenance:  
   Data engineers design, build, and maintain the necessary infrastructure to collect, process, and store large amounts of data. This infrastructure is crucial for ensuring data is accessible and usable for analysis and reporting.

2. [Data Pipeline](#data-pipeline): 

3. Support Role:  
   Data engineers act as a bridge between <mark>data producers and consumers</mark>, ensuring smooth and reliable data flow. They support business operations through scalable and efficient [Data Management](#data-management) solutions, contributing indirectly to product delivery and decision-making.

### Core Activities:

What engineers do & interact with: see [Data Engineering Portal](#data-engineering-portal)

Stakeholders they interact with see [Data Roles](#data-roles)

Tools they use: [Data Engineering Tools](#data-engineering-tools)

Tasks They Are Usually Given
  - Project Management: Tracking tasks, bugs, and progress through Azure Boards.
  - Collaboration: Facilitating teamwork with shared repositories and [continuous integration](#continuous-integration) workflows.
  - Continuous Learning: Keeping up-to-date with the latest technologies and updating pipelines due to obsolescence of tech
  - [Documentation & Meetings](#documentation--meetings) and Security: Creating documentation, implementing security measures, and exploring system upgrades for enhanced efficiency.



# Data Engineering Portal {#data-engineering-portal}


Databases manage large data volumes with scalability, speed, and flexibility. Key systems include:

- [MySql](#mysql)
- [PostgreSQL](#postgresql)


They facilitate efficient [CRUD.md](obsidian://open?vault=content&file=standardised%2FCRUD.md) operations and transactional processing ([OLTP.md](obsidian://open?vault=content&file=standardised%2FOLTP.md)), structured by a [Database Schema.md](obsidian://open?vault=content&file=standardised%2FDatabase%20Schema.md) that organizes data into tables and relationships.

## Key Features

- **[Structured Data](#structured-data)**: Organized for efficient CRUD operations, allowing reliable access.
- **Relational Databases**: Use SQL to manage data in tables with relationships expressed through foreign keys and joins, minimizing redundancy.

Structure
- Data is organized into tables (like spreadsheets) with columns (fields) and rows (records), enabling efficient storage and retrieval.

Flexibility
- Databases have a flexible schema that adapts to evolving requirements, unlike static solutions like spreadsheets.

Related Ideas:
- [Spreadsheets vs Databases](#spreadsheets-vs-databases)
- [Database Management System (DBMS)](#database-management-system-dbms)
- [Components of the database](#components-of-the-database)
- [Relating Tables Together](#relating-tables-together)
- [Turning a flat file into a database](#turning-a-flat-file-into-a-database)
- [Database Techniques](#database-techniques)

# Data Engineering Tools {#data-engineering-tools}


  - **Snowflake:** [Cloud](#cloud)-based data warehousing for scalable storage and processing.
  - **Microsoft SQL Server:** [SQL](#sql)-based relational database management.
  - **[Azure](#azure) SQL Database:** Managed relational database service on Azure.
  - **Azure Data Lake Storage:** Scalable storage for big data analytics.
  - **SQL and T-SQL:** Query languages for managing and querying relational databases.
  - **AWS [Amazon S3|S3](#amazon-s3s3):** Storage for data lakes.

[Data Ingestion](#data-ingestion) Tools and Technologies:
- [Apache Kafka](#apache-kafka)
- AWS Kinesis: A cloud service for real-time data processing, enabling the collection and analysis of streaming data.
- Google Pub/Sub: A messaging service that allows for asynchronous communication between applications, supporting real-time data ingestion.

[Data Storage](#data-storage)
Tools: Amazon S3, Google BigQuery, Snowflake.

[dbt](#dbt)

### Tags
- **Tags**: #data_tools, #data_management

# Data Engineering {#data-engineering}


The definition from the [Fundamentals of Data Engineering](https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/), as it’s one of the most recent and complete: 

> Data engineering is the development, implementation, and maintenance of systems and processes that take in raw data and produce high-quality, consistent information that supports downstream use cases, such as analysis and machine learning. Data engineering intersects security, [Data Management](#data-management), DataOps, data architecture, orchestration, and software engineering.

A [Data Engineer](#data-engineer) today oversees the whole data engineering process, from collecting data from various sources to making it available for downstream processes. The role requires familiarity with the multiple stages of the [Data Engineering Lifecycle](Data%20Lifecycle%20Management.md) and an aptitude for evaluating data tools for optimal performance across several dimensions, including price, speed, flexibility, scalability, simplicity, reusability, and interoperability.

Data Engineering helps also overcome the bottlenecks of [Business Intelligence](term/business%20intelligence.md):
- More transparency as tools are open-source mostly
- More frequent data loads
- Supporting [Machine Learning](Machine%20Learning.md) capabilities 

Compared to existing roles it would be a **software engineering plus business intelligence engineer including big data abilities** as the [Hadoop](term/apache%20hadoop.md) ecosystem, streaming, and computation at scale. Business creates more reporting artifacts themselves but with more data that needs to be collected, cleaned, and updated near real-time and complexity is expanding every day.

With that said more programmatic skills are needed similar to software engineering. **The emerging language at the moment is [Python](term/python.md)** which is used in engineering with tools alike [Apache Airflow](#apache-airflow), [dagster](dagster.md), [Prefect](#prefect) as well as data science with powerful libraries.

As a data engineer, you use mainly [SQL](SQL.md) for almost everything except when using external data from an API. Here you'd use [ELT](term/elt.md) tools or write some [Data Pipeline](#data-pipeline) with the tools mentioned above.

# Data Governance {#data-governance}


[**Data governance**](https://www.talend.com/resources/what-is-data-governance/) **is a collection of processes, roles, policies, standards, and metrics that ensure the effective and efficient use of information in enabling an organization to achieve its goals.**

It establishes the processes and responsibilities that ensure the [Data Quality](Data%20Quality.md) and security of the data used across a business or organization. Data governance defines who can take what action, upon what data, in what situations, and using what methods.

**Data Governance**: Focuses on ensuring that data is managed consistently and adheres to policies, often working in tandem with [Data Observability](#data-observability) to enforce quality standards.

# Data Hierarchy Of Needs {#data-hierarchy-of-needs}


![Pasted image 20241005170237.png|500](../content/images/Pasted%20image%2020241005170237.png|500)

The **Data Hierarchy of Needs** is a framework that outlines the stages required to effectively use data in organizations. It resembles Maslow’s hierarchy, progressing from basic data needs to advanced capabilities:

1. **Data Collection**:  (bottom)
   Start by collecting raw data from various sources, ensuring it's stored securely and reliably.

2. **Data Storage and Access**:  
   Organize and store data so it's easily accessible for those who need it, using databases or data warehouses.

3. **Data Cleaning and Preparation**:  
   Clean, preprocess, and transform data to ensure it’s accurate, consistent, and ready for analysis.

4. **Data Analytics**:  
   Analyze the prepared data to generate insights, identify patterns, and create reports.

5. **Data-Driven Decision Making**:  
   Use the insights from data analytics to inform and improve decision-making across the organization.

6. **Advanced Data Capabilities (AI/ML)**:  (top)
   Once the foundation is in place, apply advanced techniques like machine learning and artificial intelligence for predictive and prescriptive insights.


# Data Ingestion {#data-ingestion}


Data ingestion is the process of collecting and importing raw data from various sources ([Database](#database), [API](#api), [Data Streaming](#data-streaming) services) into a system for processing and analysis, and can be performed in batch and realtime ingestion. The goal is to gather raw data that can be processed and analyzed.

Used for building [Data Pipeline](#data-pipeline)

Challenges
- [Data Quality](#data-quality): Ensuring that the ingested data is accurate, complete, and consistent.
- [Scalability](#scalability): Handling large volumes of data efficiently as the data sources grow.
- [Latency](#latency): Minimizing the delay between data generation and processing, especially in real-time scenarios.

Use Cases:
- Data ingestion is used in various applications, including: [business intelligence](#business-intelligence), [Machine Learning](#machine-learning)

Related to:
- [Data Engineering Tools](#data-engineering-tools)



[Data Ingestion](#data-ingestion)
   **Tags**: #data_collection, #data_management

# Data Integration {#data-integration}


Data integration is the process of combining data from disparate source systems into a single unified view, moving data to a [Single Source of Truth](#single-source-of-truth).

## Manual Integration
Manual integration involves analysts manually logging into source systems, analyzing and/or exporting data, and creating reports. 

### Disadvantages of Manual Integration:
- **Time-consuming**: The process requires significant time investment.
- **Security Risks**: Analysts need access to multiple operational systems.
- **Performance Issues**: Running analytics on non-optimized systems can interfere with their functioning.
- **Outdated Reports**: Data changes frequently, leading to quickly outdated reports.

## [Data Virtualization](#data-virtualization)

Data virtualization is a method that allows access to data without needing to replicate it, providing a unified view of data from multiple sources.

## Application Integration
Application integration links multiple applications to move data directly between them. 

### Methods of Application Integration:
- **Point-to-Point Communications**: Direct connections between applications.
- **Middleware Layer**: Using tools like an Enterprise Service Bus (ESB).
- **Application Integration Tools**: Specialized tools for integrating applications.

### Disadvantages of Application Integration:
- **Data Redundancy**: May result in multiple copies of the same data across systems.
- **Increased Costs**: Managing multiple copies can lead to higher costs.
- **Point-to-Point Traffic**: Can create excessive traffic between systems.
- **Performance Impact**: Executing analytics on operational systems may interfere with their functioning.

# Data Integrity {#data-integrity}

Data integrity refers to the 
- accuracy, 
- consistency, and 
- reliability of data

throughout its lifecycle. It ensures that data remains <mark>unaltered</mark> and <mark>trustworthy</mark>, whether it is being 
- stored, 
- processed, 
- or transmitted. 

Maintaining data integrity involves implementing measures to prevent unauthorized access, corruption, or loss of data.

In the context of [Database](#database) and information systems, data integrity can be enforced through:

1. **Validation Rules**: Ensuring that data entered into a system meets certain criteria.
2. **Access Controls**: Limiting who can view or modify data.
3. **Backups**: Regularly saving copies of data to prevent loss.
4. **Error Checking**: Using [Checksum](#checksum) or [Hash](#hash) to verify data integrity during transmission.



[Data Integrity](#data-integrity)
   **Tags**: #data_quality, #data_management

# Data Lake {#data-lake}


A Data Lake is a storage system with vast amounts of [unstructured data](#unstructured-data) and [structured data](#structured-data), stored as-is, without a specific purpose in mind, that can be built on multiple technologies such as Hadoop, NoSQL, Amazon Simple Storage Service, a relational database, or various combinations and different formats (e.g. Excel, CSV, Text, Logs, etc.).

**Definition**: A repository that <mark>stores diverse data types</mark>, including structured, semi-structured, and unstructured data. If cant fit into a database.

Features:
- **Versatility**: Can accommodate various data formats, including videos, images, documents, and more.
- **Raw Data Storage**: Preserves data in its raw form, suitable for advanced analytics, particularly in machine learning and AI.
- **Data Usability**: Raw data <mark>may require cleaning and transformation for analytical use</mark>, often transferred to databases or data warehouses.
- **Use Case**: Valuable for storing large volumes of raw data, especially in contexts requiring advanced analytics and experimentation.

[unstructured data](#unstructured-data) for predictive modeling and analysis. This leads to the creation of a **data lake**, which stores raw data without predefined schemas. 

The data lake supports the following capabilities:
-   To capture and store raw data at scale for a low cost
-   To store many types of data in the same repository
-   To perform [Data Transformation](Data%20Transformation.md) on the data where the purpose may not be defined
-   To perform new types of data processing
-   To perform single-subject analytics based on particular use cases

Components of a data lake
		1. [Storage Layer](term/storage%20layer%20object%20store.md)
		2. [Data Lake File Format](term/data%20lake%20file%20format.md)
		3. [Data Lake Table Format](term/data%20lake%20table%20format.md) with [Apache Parquet](term/apache%20parquet.md), [Apache Iceberg](term/apache%20iceberg.md), and [Apache Hudi](term/apache%20hudi.md)

# Data Lakehouse {#data-lakehouse}


A Data Lakehouse open [Data Management](#data-management) architecture that combines the flexibility, cost-efficiency, and scale of [Data Lake](Data%20Lake.md) with the data management and ACID transactions of [Data Warehouse](Data%20Warehouse.md) with Data Lake Table Formats ([Delta Lake](term/delta%20lake.md), [Apache Iceberg](term/apache%20iceberg.md) & [Apache Hudi](term/apache%20hudi.md)) that enable Business Intelligence (BI) and Machine Learning (ML) on all data.

A **data lakehouse** is an emerging architectural approach that combines the best features of data lakes and data warehouses to provide a unified platform for storing, processing, and analyzing large volumes of structured and unstructured data. Here’s a breakdown of its key characteristics and benefits:

The data lakehouse architecture represents a significant evolution in [Data Management](#data-management), addressing the limitations of traditional data lakes and [Data Warehouse|Warehouse](#data-warehousewarehouse) by providing a unified platform for all data types.
### Key Characteristics

1. **Unified Storage**:
   - Data lakehouses store data in a single repository, accommodating both structured data (like tables in a database) and unstructured data (like images, videos, and text). This eliminates the need for separate systems, simplifying data management.

2. **Support for Multiple Data Types**:
   - They can handle various data formats, such as **CSV**, **JSON**, **Parquet**, and **Avro**, enabling flexibility in how data is ingested and stored.

3. [ACID Transaction](#acid-transaction):
   - Unlike traditional data lakes, data lakehouses provide [ACID Transaction](#acid-transaction) which ensure reliable data operations and integrity, even in concurrent processing environments.

4. **Schema Enforcement**:
   - Data lakehouses can enforce [Database Schema|schema](#database-schemaschema) at the time of data write, allowing users to define data structures while still benefiting from the flexibility of a data lake.

5. **Performance Optimization**:
   - They incorporate various optimization techniques, such as indexing and caching, to improve query performance and provide faster access to data.

6. **Integration with BI Tools**:
   - Data lakehouses are designed to work seamlessly with business intelligence (BI) tools and data analytics platforms, enabling users to derive insights without needing extensive data preparation.

### Benefits

1. **Cost-Effectiveness**:
   - By merging the functionalities of data lakes and data warehouses, organizations can reduce the costs associated with maintaining separate systems for structured and unstructured data.

2. **Scalability**:
   - Data lakehouses leverage cloud storage solutions, allowing for scalable data storage that can grow with the organization’s needs.

3. **Data Accessibility**:
   - With a unified architecture, data from different sources can be accessed and analyzed together, breaking down silos and fostering a more holistic view of the organization’s data landscape.

4. **Simplified Data Pipelines**:
   - Data lakehouses streamline the data ingestion process, enabling organizations to build more efficient data pipelines that accommodate a variety of data sources.

5. **Support for Advanced Analytics**:
   - They provide a robust foundation for advanced analytics, including machine learning and real-time data processing, allowing organizations to extract actionable insights more effectively.

Platforms that implement the data lakehouse architecture include:
- **Databricks Lakehouse Platform**: Combines data engineering, data science, and BI capabilities with a focus on collaboration.
- **Apache Iceberg**: A high-performance table format for large analytic datasets that supports ACID transactions and schema evolution.




# Data Leakage {#data-leakage}

**Data Leakage** refers to the unintentional inclusion of information in the training data that would not be available in a real-world scenario, leading to overly optimistic model performance. It occurs when the model has access to data it shouldn't during training, such as future information or test data, which can result in misleading evaluation metrics and poor generalization to new data.

# Data Lifecycle Management {#data-lifecycle-management}


This is the comprehensive process of managing data from its initial ingestion to its final use in downstream processes. 

Used for maintaining [data integrity](#data-integrity), optimizing performance, and ensuring that data-driven decisions are based on accurate and timely information. 

Not the same as the [Software Development Life Cycle](#software-development-life-cycle)

Key Stages of Full Lifecycle Management

1. [Data Ingestion](#data-ingestion)
2. [Data Storage](#data-storage)
3. [Preprocessing](#preprocessing)
4. [Data Analysis](#data-analysis)
5. [Data Visualisation](#data-visualisation)
6. [Data Distribution](#data-distribution)

Data engineers must evaluate and select tools and technologies based on several [Performance Dimensions](#performance-dimensions)

# Data Lineage {#data-lineage}


Data lineage uncovers the [Data Lifecycle Management](#data-lifecycle-management) life cycle of data. It aims to show the complete data flow from start to finish. 

Data lineage is the process of understanding, recording, and visualizing data as it flows from data sources to consumption. 

This includes all [Data Transformation](Data%20Transformation.md) (what changed and why).

# Data Literacy {#data-literacy}


Data literacy is the ability to read, work with, analyze, and argue with data in order to extract meaningful information and make informed decisions. This skill set is crucial for employees across various levels of an organization, especially as data-driven decision-making becomes increasingly important.

Organizations should invest in data literacy training programs to empower their employees with the necessary skills to effectively engage with data. A data-literate employee can read charts, draw correct conclusions, recognize when data is being used inappropriately or misleadingly, and gain a deeper understanding of the business domain. This enables them to communicate more effectively using a common language of data, spot unexpected operational issues, identify root causes, and prevent poor decision-making due to data misinterpretation.

Examples of data literacy in action include:

* Implementing the Adoptive Framework to create a Data Literacy Program.
* Employees working with spreadsheets to understand the rationale behind data-driven decisions and advocating for alternative courses of action.
* Work teams identifying areas where data needs clarification for a project.

By nurturing a data-literate workforce, businesses can improve their ability to make informed decisions, drive innovation, and achieve better outcomes.


# Data Management {#data-management}


Data management involves overseeing processes to maintain data integrity and quality. It includes:

- **Responsibility**: Identifying accountable individuals or teams.
- **Issue Resolution**: Mechanisms for detecting and addressing data-related problems.

Data management ensures that a [Data Pipeline](#data-pipeline) operates efficiently, focusing on monitoring errors, performance issues, and [data quality](#data-quality).

**Tools**:
- [Apache Airflow](#apache-airflow)
- Prefect
- [Dagster](#dagster)

Related Concepts:
- [Database Management System (DBMS)](#database-management-system-dbms)
- [Master Data Management](#master-data-management)
- [Data Distribution](#data-distribution)



[Data Management](#data-management)
   **Tags**: #data_management, #data_quality

# Data Modelling {#data-modelling}


Data modelling is the process of creating a visual representation of a system's data and the relationships between different data elements. 

This helps in organizing and structuring the data so it can be efficiently managed and utilized.

Data modelling ensures that data is logically structured and organized, making it easier to store, retrieve, and manipulate in a database.

Workflow of Data Modeling:
1) [Conceptual Model](#conceptual-model)
2) [Logical Model](#logical-model)
3) [Physical Model](#physical-model)


Types of Modeling:
- Relational: Organizes data into tables.
- Object-Oriented: Focuses on objects and their state changes, e.g., robots in a car factory.
- Entity: Uses [ER Diagrams](#er-diagrams) to represent data entities and relationships.
- Network: An extension of hierarchical models.
- Hierarchical: Organizes data in a tree-like structure.





[Data Modelling](#data-modelling)
   **Tags**: #data_modeling, #database_design

# Data Observability {#data-observability}


Data observability refers to the continuous monitoring and collection of metrics about your data to ensure its [Data Quality](#data-quality), reliability, and availability. 

It covers various aspects, such as data quality, pipeline health, metadata management, and infrastructure performance. By tracking key metrics and [standardised/Outliers|anomalies](#standardisedoutliersanomalies), it helps detect issues like data freshness problems, schema changes, or pipeline failures before they impact downstream processes or users.
### Categories of Observability

Auto-profiling Data:

Automatically tracks data attributes, such as row count, column types, data distributions, and schema changes.
 - Bigeye: Provides ML-driven threshold tests and automatic alerts when data drifts beyond expected ranges.
 - Datafold: Integrates with GitHub to run data diffs between environments, offering insights into differences between datasets during development.
 - Monte Carlo: Enterprise-focused with data lake integrations for comprehensive observability.
 - Metaplane: Offers a high level of configuration and both out-of-the-box and custom tests.

Pipeline Testing:

Ensures that data transformation pipelines are functioning correctly by verifying the quality and accuracy of data as it moves through different stages.
 - Great Expectations: An open-source tool that allows you to define tests and automatically generate documentation for those tests, promoting transparency in data quality checks.
 - Soda: Offers pipeline testing with the flexibility of a self-hosted option for more control over data quality monitoring.
 - [dbt](#dbt)tests: Integrated with [dbt](#dbt) Core and dbt Cloud, allowing testing during the transformation process in a dbt project.

 Infrastructure Monitoring:
 
Monitors the health and performance of the underlying data infrastructure, such as databases, pipelines, and servers, to prevent failures and bottlenecks.
 - DataDog: Provides deep monitoring capabilities, including for Airflow, containers, and custom metrics, allowing visibility at various layers of the data stack.

### Managing Metadata

Managing metadata is critical for observability, as it provides context and lineage for your data. Metadata can include:

- Technical Metadata: Information about the dataset’s structure, such as table schema, data types, and column descriptions.
- Operational Metadata: Information about the dataset’s freshness, when it was last updated, and the number of records processed.
- Business Metadata: Describes the meaning of data, such as field definitions and business rules, helping stakeholders understand the context and usage of the dataset.

How to Manage Metadata:

- Manual Documentation: Teams may manually document metadata, but this can be prone to human error and inconsistency.
- Automated Metadata Management: Many modern data tools, such as data catalogs (e.g., Atlan, Alation), automatically track and manage metadata, offering insights into data lineage, schema changes, and data usage.
- Integration with Data Pipelines: Tools like dbt also generate metadata about transformations, which can be included in downstream monitoring systems to ensure consistency and traceability.

[Data Observability](#data-observability)
- Tracking the issues.
- Alerting and ensuring data owners fix it.

# Data Pipeline To Data Products {#data-pipeline-to-data-products}


The journey from [Data Pipeline](#data-pipeline) to [Data Product](#data-product) involves transforming raw data into valuable insights or applications that can be used to drive business decisions. This process typically includes several stages, each with its own set of tasks and objectives.

Read more on [Data Orchestration Trends: The Shift From Data Pipelines to Data Products](https://airbyte.com/blog/data-orchestration-trends).
### Workflow

1. **Define Objectives**:
   - Understand the business goals and what insights or products are needed.

2. **Design the Pipeline**:
   - Plan the architecture and select appropriate tools for each stage of the pipeline.

3. **Implement and Test**:
   - Build the pipeline, ensuring data flows smoothly from ingestion to product delivery.
   - Test for accuracy, performance, and reliability.

4. **Deploy and Monitor**:
   - Deploy the pipeline in a production environment.
   - Continuously monitor for performance and make adjustments as needed.

5. **Iterate and Improve**:
   - Gather feedback and refine the pipeline and products to better meet business needs.
### Example

Imagine a retail company wants to create a recommendation system for its online store:

1. **Data Ingestion**: Collect customer browsing and purchase data from the website.
2. **Data Processing**: Clean and transform the data to identify patterns in customer behavior.
3. **Data Storage**: Store the processed data in a data warehouse for easy access.
4. **Data Analysis**: Use machine learning algorithms to analyze the data and generate recommendations.
5. **Data Visualization**: Create dashboards to visualize customer trends and recommendation performance.
6. **Data Products**: Deploy the recommendation system on the website to enhance customer experience.



# Data Pipeline {#data-pipeline}


A data pipeline is a series of processes that automate the movement and transformation of data from various sources to a destination where it can be stored, analyzed, and used to generate insights. 

It ensures that data flows smoothly and efficiently through different stages, maintaining data quality and [Data Integrity](#data-integrity).

By implementing a data pipeline, organizations can automate data workflows, reduce manual effort, and ensure timely and accurate data delivery for decision-making.
### Workflow

1. [Data Ingestion](#data-ingestion)
2. [Data Transformation](#data-transformation)
3. [Data Storage](#data-storage)
4. [Preprocessing|Data Preprocessing](#preprocessingdata-preprocessing)
5. [Data Management](#data-management)
#### Other steps:

Design:
   - Define the objectives and requirements of the data pipeline.
   - Choose appropriate tools and technologies.

Development:
   - Build the pipeline components and integrate them into a cohesive system.

Testing:
   - Validate the pipeline to ensure data accuracy and performance.

Deployment:
   - Deploy the pipeline in a production environment.

Monitoring and Maintenance:
   - Continuously monitor the pipeline and make necessary adjustments to improve performance and reliability.

### Related Notes

- [Data Pipeline to Data Products](#data-pipeline-to-data-products)




[Data Pipeline](#data-pipeline)
   **Tags**: #data_workflow, #data_management

# Data Principles {#data-principles}


Data principles are essential for ensuring that data is managed, used, and maintained effectively and ethically.

1. [Data Quality](#data-quality) Ensure data is accurate, complete, reliable, and up-to-date. High-quality data is crucial for making informed decisions.

2. [Data Governance](#data-governance): Establish clear policies and procedures for data management, including roles and responsibilities, to ensure [data integrity](#data-integrity) and compliance with regulations.

3. Data Privacy: Protect personal and sensitive information by adhering to privacy laws and regulations, such as GDPR or CCPA, and implementing appropriate security measures.

4. Data Security: Safeguard data against unauthorized access, breaches, and other security threats through encryption, access controls, and regular [security](#security) audits.

5. Data Accessibility: Ensure that data is easily accessible to those who need it while maintaining appropriate security and privacy controls. This includes providing the necessary tools and training for data access.

6. Data Transparency: Maintain transparency about data collection, usage, and sharing practices. This helps build trust with stakeholders and ensures accountability.

7. Data Consistency: Standardize data formats and definitions across the organization to ensure consistency and interoperability.

8. Data Stewardship: Assign data stewards to oversee [data management](#data-management) practices, ensuring data quality, compliance, and proper usage.

9. [Data Lifecycle Management](#data-lifecycle-management) Manage data throughout its lifecycle, from creation and storage to archiving and deletion, ensuring that data is retained only as long as necessary.

10. Ethical Data Use: Use data ethically and responsibly, considering the potential impact on individuals and society. Avoid biases and ensure fairness in data-driven decisions.

11. Data [Documentation & Meetings](#documentation--meetings): Maintain thorough documentation of data sources, definitions, and processes to facilitate understanding and reproducibility.

12. Data Sharing and Collaboration: Encourage data sharing and collaboration within and across organizations to maximize the value of data, while respecting privacy and security constraints.
    
13. DRY

Related:
- [Performance Dimensions](#performance-dimensions)

# Data Product {#data-product}


A data product is

"a product that facilitates an end goal through data".

Delivering the final output, which could be dashboards, reports, or machine learning models. For example Recommendation systems or predictive analytics dashboards.

It applies more product thinking, whereas the "Data Product" essentially is a dashboard, report, and table in a [Data Warehouse](Data%20Warehouse.md) or a Machine Learning model.

Sometimes Data Products are also called [data asset](#data-asset).

# Data Quality {#data-quality}


Data quality is the process of ensuring that data meets established expectations. High-quality data is crucial for effective decision-making and analysis.

**Definition**: Data quality refers to the <mark>accuracy, consistency, and reliability of data.</mark> It is essential for maintaining trust in data-driven processes and outcomes. 

**Importance**: The principle of "garbage in, garbage out" highlights that poor-quality data leads to poor model performance.

Related terms:
- [Data Observability](#data-observability)
- [Change Management](#change-management)
- [Prevention Is Better Than The Cure](#prevention-is-better-than-the-cure)


Related terms:
- [Data Observability](#data-observability)
- [Data Contract](#data-contract)
- [Change Management](#change-management)

# Data Reduction {#data-reduction}

Reducing the volume of data through techniques:

[Dimensionality Reduction](#dimensionality-reduction)

[Sampling](#sampling): Use subsets of data for training to speed up the process and address issues like imbalanced data representation.

Remove features with zero or low [variance](#variance) and redundant features to improve model performance.

# Data Roles {#data-roles}

A data team is a specialized group within an organization responsible for managing, analyzing, and leveraging data to drive business decisions and strategies. 

The team collaborates across various functions to ensure data integrity, accessibility, and usability.

## Key Roles and Responsibilities

| Role                         | Focus Area                    | Key Responsibilities                                                                           |
| ---------------------------- | ----------------------------- | ---------------------------------------------------------------------------------------------- |
| **[Data Steward](#data-steward)**         | [Data quality](#data-quality) & governance | Enforces data policies, resolves [data quality](#data-quality) issues, manages metadata.                    |
| **[Data Governance](#data-governance) Team** | Policy & compliance           | Defines [data management](#data-management) rules, ensures regulatory adherence.                               |
| **[Data Engineer](#data-engineer)**        | Data infrastructure           | Builds data pipelines, integrates data sources, and ensures data flow.                         |
| **[Data Scientist](#data-scientist)**       | [Data analysis](#data-analysis) & modeling  | Utilizes BI tools, analyzes data, develops and deploys ML models.                              |
| **[ML Engineer](#ml-engineer)**          | Machine learning              | Configures and optimizes ML models, monitors performance in production.                        |
| **[Data Architect](#data-architect)**       | Data architecture             | Designs and manages data infrastructure, ensures data accessibility.                           |
| **[Data Analyst](#data-analyst)**         | Reporting & visualization     | Gathers and processes data, generates reports, communicates insights using tools like Tableau. |

#### Other Stakeholders
- **Business Analysts:** Ensure data is structured and accessible for analysis and reporting.
- **Senior Stakeholders and Business Ambassadors:** Communicate requirements, progress, and solutions to align with business goals.
- **Software Engineers and Data Teams:** Coordinate on data production and integration processes.

# Data Science {#data-science}


A field that uses the [Scientific Method](#scientific-method), algorithms, and systems to <mark>extract knowledge</mark> and insights from structured and [unstructured data](#unstructured-data). It combines techniques from [statistics](#statistics), computer science, and domain expertise to analyze and interpret complex data sets, enabling informed decision-making and predictive modeling.

Resources:
- https://scikit-learn.org/stable/auto_examples/index.html


# Data Scientist {#data-scientist}

Data Scientist
  - Utilizes [Business Intelligence](#business-intelligence) (BI) tools to analyze data.
  - Works with data lakes to extract insights.
  - Develops and deploys production Machine Learning (ML) models for predictions.

# Data Selection In Ml {#data-selection-in-ml}

When selecting data for machine learning models, several important considerations can significantly impact the model's performance/[Model Optimisation](#model-optimisation) and the insights you can derive from it. Here are key factors to consider:

1. Relevance:
   - Ensure that the features (input variables) you select are relevant to the problem you are trying to solve. Irrelevant features can introduce noise and reduce model accuracy.

2. Quality: [Data Quality](#data-quality)
   - Assess the quality of the data, including checking for missing values, outliers, and errors. Poor quality data can lead to inaccurate models.

3. Quantity:
   - Consider the size of your dataset. More data can lead to better models, but it also requires more computational resources. Ensure you have enough data to train your model effectively.

4. Balance: [Imbalanced Datasets](#imbalanced-datasets)
   - Check for [Imbalanced Datasets|class imbalance](#imbalanced-datasetsclass-imbalance) in classification problems. An imbalanced dataset can bias the model towards the majority class. Techniques like resampling, synthetic data generation, or using different evaluation metrics can help address this.

5. Feature Distribution: [Distributions](#distributions)
   - Analyze the distribution of your features. Features with skewed [distributions](#distributions) may need transformation ([Data Transformation](#data-transformation)) (e.g., log transformation) to improve model performance.

6. [Correlation](#correlation):
   - Examine the correlation between features. Highly correlated features can lead to [multicollinearity](#multicollinearity), which can affect model stability and interpretability. Consider removing or combining correlated features.

7. Dimensionality: [Dimensionality Reduction](#dimensionality-reduction)
   - High-dimensional data can lead to overfitting. Techniques like [feature selection](#feature-selection), dimensionality reduction (e.g., PCA), or regularization can help manage this.

8. Temporal Considerations:
- For time series data, ensure that the temporal order is maintained. Avoid data leakage by ensuring that future information is not used in training.

9. Domain Knowledge:
   - Leverage domain expertise to select features that are known to be important for the problem. This can guide feature engineering and selection.

10. Data Leakage:
  - Be cautious of [Data Leakage](#data-leakage), where information from the test set is inadvertently used in training. This can lead to overly optimistic performance estimates.

11. Scalability:
- Consider the scalability of your data selection process. As datasets grow, ensure that your methods can handle larger volumes efficiently.

# Data Selection {#data-selection}


Data selection is a crucial part of data manipulation and analysis. Pandas provides several methods to select data from a DataFrame.

In [DE_Tools](#de_tools)  we explore how to do Data Selection with Pandas
- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/selection.ipynb

Related:
- [Data Selection in ML](#data-selection-in-ml)
## Examples
### Selecting Columns

You can select a single column from a DataFrame using either bracket notation or dot notation:

```python
df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})
column_a = df['A']  # or df.A
```
### Selecting Rows by Index

To select rows by their index position, you can use slicing:
```python
rows_0_to_2 = df[0:3]  # Selects the first three rows
```
### Selecting Rows by Date Range

If your DataFrame has a DateTime index, you can select rows within a specific date range:

```python
date_rng = pd.date_range(start='2013-01-01', end='2013-01-06', freq='D')
df = pd.DataFrame(date_rng, columns=['date'])
df.set_index('date', inplace=True)
selected_dates = df['2013-01-02':'2013-01-04']
```
### Label-based Selection

Use `.loc` or `.at` to select rows by label:

```python
df = pd.DataFrame({'Weather': ['Sunny', 'Rain', 'Cloudy'], 'Temp': [30, 22, 25]})
df.set_index('Weather', inplace=True)
rain_row = df.loc['Rain']  # or df.at['Rain']
```
### Position-based Selection

Use `.iloc` or `.iat` to select rows by position:

```python
third_row = df.iloc[2]  # Selects the third row
specific_value = df.iat[1, 1]  # Selects the value at row 1, column 1
```
### Conditional Selection

Create a new DataFrame based on a condition:
```python
df_new = df[df['var1'] >= 999]  # Selects rows where 'var1' is greater than or equal to 999
```
The condition `df["var1"] >= 999` creates a boolean Series that filters the rows of `df`.



# Data Steward {#data-steward}

A **Data Steward** is responsible for ensuring the quality, integrity, and governance of an organization's data assets. They act as a bridge between business users, IT teams, and data governance policies, ensuring that data is well-defined, accurate, and used appropriately.

### **Key Responsibilities of a Data Steward**

1. **Data Quality Management** – Ensuring data accuracy, completeness, consistency, and reliability across systems.
2. **Metadata Management** – Documenting data definitions, relationships, and lineage.
3. **Data Governance Compliance** – Implementing policies, standards, and best practices for data handling.
4. **Master Data Management (MDM)** – Managing critical business data entities like customers, products, and suppliers.
5. **Collaboration with Stakeholders** – Acting as a liaison between business units, data engineers, and data governance teams.
6. **Issue Resolution** – Identifying and resolving data-related issues such as duplicates, missing values, and inconsistencies.
7. **Data Security & Privacy** – Ensuring compliance with regulations (e.g., GDPR, HIPAA) by monitoring access and usage.

### **Why is a Data Steward Important?**

- Enhances **data trustworthiness**, leading to better decision-making.
- Reduces **data inconsistencies** and errors in analytics and reporting.
- Supports **regulatory compliance** and risk management.
- Enables **efficient data integration** across systems and departments.





[Data Steward](#data-steward)
  - Responsible for [data governance](#data-governance) and quality.
  - Ensures that data policies and standards are adhered to across the organization.
  - Acts as a liaison between data users and IT to facilitate [data management](#data-management).

# Data Storage {#data-storage}


Data storage is a fundamental aspect of [Data Engineering](#data-engineering), influencing processes such as 
- (occurring after [Data Ingestion](#data-ingestion))
- [Data Transformation](#data-transformation)
- [Querying](#querying)
- [data management](#data-management).

Storing the [Data Transformation](#data-transformation) data in a database or [Data Warehouse](#data-warehouse) for easy access and analysis.
## Types of Storage

Data storage encompasses various methods and technologies for storing, retrieving, and managing data. The choice of storage method significantly impacts <mark>data retrieval efficiency</mark> and consistency

| Storage Type                                 | Description                                                                                           |
| -------------------------------------------- | ----------------------------------------------------------------------------------------------------- |
| [storage layer object store\|Object Store](#storage-layer-object-storeobject-store) | The gold standard for data lakes, ideal for unstructured data such as images, audio, and text.        |
| [Database](#database)                                 | The most widely deployed database globally is [SQLite](#sqlite). Suited for transaction recording.           |
| [NoSQL](#nosql)                                    |                                                                                                       |
| [Data Warehouse](#data-warehouse)                           | Excels in analytics and reporting.                                                                    |
| [Data Lake](#data-lake)                                | Offers versatility for storing raw data, particularly beneficial for advanced analytics applications. |
## Follow-Up Questions
- How do different data storage methods impact data retrieval speed in large datasets?
- What are the trade-offs between using relational versus [NoSQL](#nosql) databases in specific applications?
## Related Resources
- [Cloud Providers](#cloud-providers)
- [Amazon S3](#amazon-s3)
- [Data Governance](#data-governance)
- [Data Engineering Tools](#data-engineering-tools)



# Data Streaming {#data-streaming}


Data Streaming is used for real-time data processing, allowing continuous flow and processing of data as it arrives. This is different from [batch processing](#batch-processing), which handles data in chunks.

The key to data streaming is the [Publish and Subscribe](#publish-and-subscribe)
  
[Apache Kafka](#apache-kafka)

Example:
  - Companies like Netflix use Kafka to handle billions of messages daily, powering real-time recommendations, analytics, and user activity tracking.

[Alternatives to Batch Processing](#alternatives-to-batch-processing)



[Data Streaming](#data-streaming)
   **Tags**: #data_workflow

# Data Terms {#data-terms}



# Data Transformation In Data Engineering {#data-transformation-in-data-engineering}

Data transformation in [Data Engineering](#data-engineering) is a key step in data pipelines, often part of:  

- [ETL (Extract, Transform, Load)](ETL.md) [ETL](#etl): Data is transformed before loading into the target system.  
- [ELT (Extract, Load, Transform)](term/elt.md) [ELT](#elt): Data is loaded first, then transformed for analysis.  
- EtLT (Extract, “tweak”, Load, Transform: A hybrid approach combining elements of ETL and ELT.  

Related:
- [ETL vs ELT](#etl-vs-elt)for a comparison.

# Data Transformation In Machine Learning {#data-transformation-in-machine-learning}

Transforming raw data into a meaningful format is necessary for building effective models.  

- [Supervised Learning](#supervised-learning): Annotating datasets with correct labels (e.g., labeling images of apples vs. other fruits).  
- Manual & Automated Labeling: Using human annotators or leveraging existing labeled datasets (e.g., Google reCAPTCHA).  
- Feature Scaling & Encoding: Applying normalization and encoding to categorical variables.  
- [Encoding Categorical Variables](#encoding-categorical-variables): Converting categorical data into numerical format for machine learning models.  

# Data Transformation With Pandas {#data-transformation-with-pandas}


Using [pandas](#pandas) we can do the following:


- [Merge](#merge)
- [Concatenate](#concatenate)
- [Joining Datasets](#joining-datasets) 
- [Pandas join vs merge](#pandas-join-vs-merge)
- [Multi-level index](#multi-level-index)

- [Aggregation](#aggregation)

- [Pandas Stack](#pandas-stack)
- [Crosstab](#crosstab)

A summary of transformations steps can be helpful:

|Step|Operation|Result|
|---|---|---|
|1|`set_index`|Rows get hierarchical keys|
|2|`stack`|Wide → long with 3-level row index|
|3|`reset + extract`|Parse variable names into fields|
|4|`pivot`|Tidy format with metric columns|
|5|`unstack`|Wide format with MultiIndex columns|

In [DE_Tools](#de_tools) see:
- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Investigating/Transformation

Related terms:

### Split-Apply-Combine

![Pasted image 20250323081817.png](../content/images/Pasted%20image%2020250323081817.png)


# Data Transformation {#data-transformation}


Data transformation is the process of converting data from one format to another. 

Data transformation may involve:  
- [Data Cleansing](#data-cleansing)
- [Structuring and organizing data](#structuring-and-organizing-data)
- [Aggregation](#aggregation)
- [Data Selection](#data-selection)
- [Joining Datasets](#joining-datasets)
- [Normalisation of data](#normalisation-of-data)
- [Normalised Schema](#normalised-schema)

Others:
- Sorting: Arranging data in a logical order.  
- Validating: Ensuring data integrity and accuracy.  
- Data Type Conversion: Changing data types (e.g., converting strings to integers).  
- Schema Normalization: Ensuring a consistent data structure for efficiency.  

Related:
- [Data Transformation with Pandas](#data-transformation-with-pandas)  
- [Data transformation in Data Engineering](#data-transformation-in-data-engineering)
- [Data transformation in Machine Learning](#data-transformation-in-machine-learning)
- [Benefits of Data Transformation](#benefits-of-data-transformation)



# Data Validation {#data-validation}

Data Validation:

- **Error Prevention**: It ensures data accuracy by preventing incorrect or inappropriate data entries.
- **Consistent Data Entry**: Helps maintain consistency across large datasets by controlling what users can input.
- **Efficiency**: By providing drop-down lists or constraints, it reduces the chances of manual errors.
- **Better [Data Quality](#data-quality): Validating input ensures that your data is clean and ready for analysis or reporting without requiring additional checks.
- [type checking](#type-checking)
- [TypeScript](#typescript)

--- 
[Pydantic](#pydantic)

# Data Virtualization {#data-virtualization}

Organizations may also consider adopting a data virtualization solution to integrate their data. 

In this type of [data integration](#data-integration), data from multiple sources is left in place and is <mark>accessed</mark> via a virtualization layer so that it <mark>_appears_</mark> as a single data store. 

This virtualization layer makes use of adapters that translate queries executed on the virtualization layer into a format that each connected source system can execute. 

The virtualization layer then combines the responses from these source systems into a single result. This data integration strategy is sometimes used when a BI tool like Tableau needs to access data from multiple data sources.

One disadvantage of data virtualization is that analytics workloads are executed on operational systems, which could interfere with their functioning. Another disadvantage is that the virtualization layer may act as a bottleneck on the performance of analytics operations.

# Data Visualisation {#data-visualisation}


Data visualization involves presenting data in a visual format, enabling stakeholders to quickly grasp insights and make informed decisions. Effective visualization tools include dashboards and reports.

Can generate reports using:
- [Tableau](#tableau)
- [PowerBI](#powerbi)
- [Looker Studio](#looker-studio)

# Data Warehouse {#data-warehouse}


A Data Warehouse (DWH) is a centralized repository designed for [Querying](#querying) and analysis, storing large volumes of structured data from various sources within an organization. It supports reporting and decision-making by providing a consolidated view of data.
### Key Features

[Data Ingestion](#data-ingestion) Integration: Combines data from diverse sources (e.g., transactional databases, CRM systems) into a single repository, ensuring consistency.
  
Subject-Oriented: Organizes data around key business areas (e.g., sales, finance) rather than operational processes.

Non-Volatile: Data remains unchanged once entered, preserving historical data for long-term analysis.

Time-Variant: Stores data with a time dimension, enabling historical analysis and trend identification.

### Components

Data Sources: Internal (e.g., ERP systems) and external (e.g., market research data) origins of data.

[ETL](#etl)

[Data Storage](#data-storage)

Metadata/[Documentation & Meetings](#documentation--meetings): Information about the data, including definitions and transformation rules, aiding in data management.

Access Tools: Tools for querying and analyzing data, such as SQL clients and business intelligence tools.

##### Resources
- [Designing a Data Warehouse](https://www.youtube.com/watch?v=patBYUGwsHE)
- [Why a Data Warehouse?](https://www.youtube.com/watch?v=jmwGNhUXn_o)


# Database Index {#database-index}


In [DE_Tools](#de_tools) see: 
- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Indexing/Indexing.ipynb

Related terms:
- [Covering Index](#covering-index)
- Partial Index (Index with where clause)

Indexing is a technique used to <mark>speed up data retrieval</mark> in [database](#database). It achieves this by creating a separate structure, known as an index, that organizes specific columns of data for faster access. Better than scanning.

Commonly created on <mark>primary keys</mark> (unique for item) and foreign keys.

Indexes can also be created across multiple tables to enhance the performance of complex queries, especially those that involve joins. A special type of index, called a <mark>covering index</mark>, includes all the necessary data within the index itself, further improving efficiency.

Example:  
For instance, creating an index on the "title" column in the "movies" table can significantly reduce the time it takes to execute [Querying|queries](#queryingqueries) that search for movie titles.

## Using Indexes

Keep in mind that indexes consume additional storage space.

Creating Indexes: To improve search performance, create indexes on relevant columns. For example:
  
```sql
  CREATE INDEX idx_title ON movies(title);
  
```

Analyzing Queries: Use the `EXPLAIN QUERY PLAN` command to check if a query is utilizing an index effectively.

Dropping Indexes: If an index is no longer needed, it can be removed using:
  
```sql
  DROP INDEX idx_title;
  
```

## Space and Time Trade-offs

Space: Indexes require extra storage because they are built using B-Trees, which are hierarchical data structures.
Time: While indexes speed up data retrieval, creating and updating them can slow down data insertion and modification processes.

## How Indexes Work

- Data Structure: Indexes typically use a [B-tree](#b-tree) data structure, which allows for efficient searching.
- Node Structure: A B-tree organizes data into nodes, where each node contains links to the corresponding rows in the table. The data is sorted, enabling quick access.
- Search Mechanism: When searching, a binary search method is employed. This involves checking the middle of the data and deciding which side to search next, taking advantage of the ordered nature of B-trees for efficiency.


# Database Management System (Dbms) {#database-management-system-dbms}


A **Database Management System** (DBMS) is software that allows you to interact with and manage databases.
Easiest to use:
- [SQLite](#sqlite)
- [PostgreSQL](#postgresql)

Others:
- [MySql](#mysql)
- [MongoDB](#mongodb)
- [Oracle](#oracle)

These systems enable users to perform [CRUD](#crud) operations while maintaining data integrity and providing tools for backup, security, and optimization.

Can be proprietary (paid, with support) or Open source (free, self-supported).


# Database Schema {#database-schema}


A [Database Schema|schema](#database-schemaschema) is the structure that defines how data is organized in a [Database](#database), used in [Data Management](#data-management). It specifies the tables, columns, relationships, and constraints within the database. The schema is used for ensuring data is stored consistently and can be queried efficiently.

1. Definition and Components: A database schema represents the <mark>structure</mark> around the data, including tables, views, fields, relationships, and various other elements like indexes and triggers. It provides a framework for organizing and understanding data.

2. Importance of <mark>Structure</mark>: Without a schema, data can be chaotic and difficult to <mark>interpret</mark>. A well-defined schema organizes data, making it <mark>manageable and meaningful.</mark>

3. Schema on Read vs. Schema on Write: 
   - Schema on Read: Structure is applied when the data is read, useful for unstructured data stores.
   - Schema on Write: Structure is enforced when data is written, typical of traditional databases.

1. Design Influences: The design of a schema impacts database behavior. For example, schemas designed with tables connected by primary keys are optimized for transactional applications, while star schemas are designed for efficient read operations in data warehouses.

2. Performance Impact: A good schema can significantly <mark>improve query performance</mark>, reducing processing <mark>time</mark> and <mark>cost</mark>, and simplifying query complexity.

3. [Data Modelling](#data-modelling): Despite being considered an old concept, data modeling remains crucial for creating effective schemas, particularly in the context of big data and analytics.

4. Iterative Process: Developing a data warehouse schema involves iterative refinement, starting with interviews to create a [conceptual data model](#conceptual-data-model), which is then tested and refined through multiple iterations before being implemented.

5. Strategic Importance: The strategic design and deployment of a database schema are vital for efficient data warehousing and analytics. Intracity specializes in this process, helping organizations define and execute their data strategies.

Related to: 
- [Types of Database Schema](#types-of-database-schema)
- [Implementing Database Schema](#implementing-database-schema)

#### Resources
[link](https://www.youtube.com/watch?v=3BZz8R7mqu0)

# Database Storage {#database-storage}


Methods and optimizations for storing, retrieving, and processing data in [database](#database) systems. 

[Columnar Storage](#columnar-storage)

[Row-based Storage](#row-based-storage)

[Vectorized Engine](#vectorized-engine)



# Database Techniques {#database-techniques}


Techniques:
- [Soft Deletion](#soft-deletion)
- [Concurrency](#concurrency) 
	- [Race Conditions](#race-conditions)
- [Querying](#querying)
	- [SQL Joins](#sql-joins)
	- [Stored Procedures](#stored-procedures)
	- Cleaning: Use **Levenshtein Distance** (if SQLite extension is available) to group similar entries.
- [Database Index|Indexing](#database-indexindexing)
	- [Query Plan](#query-plan)
	- [Vacuum](#vacuum)

# Database {#database}


Databases manage large data volumes with scalability, speed, and flexibility. Key systems include:

- [MySql](#mysql)
- [PostgreSQL](#postgresql)
- [MongoDB](#mongodb)

They facilitate efficient [CRUD](#crud) operations and transactional processing ([OLTP](#oltp)) structured by [Database Schema|schema](#database-schemaschema) that organizes data into tables and relationships.

Key Features
- **[Structured Data](#structured-data)**: Organized for efficient CRUD operations, allowing reliable access.
- **Relational Databases**: Use SQL to manage data in tables with relationships expressed through foreign keys and joins, minimizing redundancy.

Structure
- Data is organized into tables (like spreadsheets) with columns (fields) and rows (records), enabling efficient storage and retrieval.

Flexibility
- Databases have a flexible schema that adapts to evolving requirements, unlike static solutions like spreadsheets.

Related Ideas:
- [Spreadsheets vs Databases](#spreadsheets-vs-databases)
- [Database Management System (DBMS)](#database-management-system-dbms)
- [Components of the database](#components-of-the-database)
- [Relating Tables Together](#relating-tables-together)
- [Turning a flat file into a database](#turning-a-flat-file-into-a-database)
- [Database Techniques](#database-techniques)

# Databricks Vs Snowflake {#databricks-vs-snowflake}


Comparison between **[Databricks](#databricks)** and **[Snowflake](#snowflake)**:

- **Databricks** is a versatile platform that emphasizes collaborative data science and engineering through interactive notebooks, making it suitable for advanced analytics and machine learning applications.
- **Snowflake**, on the other hand, focuses on [Data Warehouse](#data-warehouse) and offers a robust SQL interface for analytics, making it a preferred choice for organizations prioritizing data storage and reporting capabilities.

| Feature                      | **Databricks**                                         | **Snowflake**                                      |
|------------------------------|-------------------------------------------------------|---------------------------------------------------|
| **Primary Functionality**     | Unified analytics platform for big data processing and machine learning. | Cloud-based data warehousing and analytics platform. |
| **Data Processing**           | Built on **Apache Spark**, optimized for large-scale data processing and machine learning workflows. | Uses its own SQL-based engine for data warehousing; excels in querying structured data. |
| **Collaboration**             | Emphasizes collaboration through **notebooks** (e.g., Jupyter `.ipynb` files) that allow for interactive data analysis and coding. | Provides features for data sharing and collaboration but lacks the notebook interface. |
| **Data Structure**            | Supports both structured and unstructured data, integrating seamlessly with data lakes (e.g., Delta Lake). | Primarily designed for structured data and semi-structured data (like JSON) stored in tables. |
| **Scalability**               | Uses clusters to scale up compute resources dynamically; suitable for big data workloads. | Offers automatic scaling of compute and storage resources, focusing on cost-effective scaling. |
| **Machine Learning Support**  | Integrated support for ML libraries (e.g., MLlib, MLflow) to build and deploy machine learning models. | Limited built-in support for machine learning, primarily used for data storage and querying. |
| **Query Language**            | Supports multiple programming languages (Python, R, Scala, SQL) within notebooks. | Primarily uses SQL for querying data, providing a familiar interface for data analysts. |
| **Deployment**                | Available on major cloud platforms (AWS, Azure, GCP); allows for more customization and flexibility in deployment. | Also cloud-native, designed for seamless deployment in the cloud, with less emphasis on infrastructure management. |
| **Use Cases**                 | Ideal for big data analytics, data engineering, and data science projects requiring complex processing. | Best suited for traditional data warehousing, business intelligence, and analytics use cases. |




# Databricks {#databricks}


### **Databricks Overview**

>[!Summary]  
Databricks is a cloud-based platform for [big data](#big-data) processing built on [Apache Spark](#apache-spark). It provides an integrated workspace for collaboration among [data engineer](#data-engineer)s, data scientists, and analysts. Databricks on Azure simplifies Spark deployment by offering auto-scaling clusters, real-time analytics, and integration with various Azure services, such as Azure [Data Lake](#data-lake) for large-scale data storage.


**Cloud Platform Compatibility**: 
  - Supports the big three cloud providers (AWS, Azure, GCP).
- **Integration with Other Technologies**: 
  - Combines capabilities of:
    - **Apache Spark**
    - **Delta Lake**
    - **MLflow**
- **Data Lakehouse Architecture**:
  - Represents a combination of a **data [Data Warehouse|warehouse](#data-warehousewarehouse)** and a **data lake**.

Core Components:
1. **Tables**: 
   - Represents files and data sources.
2. **Clusters**: 
   - Provides computing power for data processing.
3. **Notebooks**: 
   - Similar to Jupyter notebooks; support multiple programming languages and allow for productionization of code.
4. **Workspaces**: 
   - Collaborative environments for teams to work together.

Scalability
- Leverages the scalability of **[Hadoop](#hadoop)** while integrating advanced features for big data processing.

[Databricks vs Snowflake](#databricks-vs-snowflake)

# Datasets {#datasets}

This note collects notes on datasets that are good examples for exploring various concepts.
## Heart Failure Prediction Dataset
- **Link**: [Heart Failure Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)
- **Useful for**: Exploring predictive modeling in healthcare.

## Time Series Exploration
- **Description**: There is a dataset with seasonality, bikes, which can be used to explore [Time Series](#time-series) concepts.

## Numenta Anomaly Benchmark (NAB)
- **Link**: [Numenta Anomaly Benchmark (NAB)](https://github.com/numenta/NAB?ref=hackernoon.com)
- **Columns**: timestamp, value
- **Description**: NAB is used to evaluate and compare the performance of different anomaly detection algorithms on a diverse set of time series data. It includes real-world and artificial time series data covering domains such as finance, transportation, and environmental monitoring.

## U.S. Census Bureau's International Data Base (IDB)
- **Link**: [International Data Base (IDB)](https://www.census.gov/data-tools/demo/idb?ref=hackernoon.com)
- **Useful for**: Researchers, policymakers, and businesses studying population dynamics, forecasting future population growth, monitoring economic development, and comparing demographic and economic characteristics of different countries.

## Wikipedia Web Traffic Time Series Dataset
- **Link**: [Wikipedia Web Traffic Time Series Dataset](https://www.kaggle.com/code/muonneutrino/wikipedia-traffic-data-exploration/data?ref=hackernoon.com)
- **Useful for**: Examining the dynamics of website traffic, understanding interactions with Wikipedia, and identifying patterns and trends in online behavior. It can be used to compare traffic across languages, analyze the popularity of articles, and track the evolution of articles over time.



# Dbscan {#dbscan}


**DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) is a [Clustering](#clustering) algorithm that groups together data points <mark>based on density</mark>. It is particularly useful when K-means doesn't work well, such as in datasets with complex shapes or when there are outliers.

- **Used when [K-means](#k-means) doesn't work**: DBSCAN handles datasets with <mark>irregular cluster shapes</mark> and is not sensitive to outliers like K-means.
- **When you have nesting of clusters**: It can identify clusters of varying shapes and sizes without needing to predefine the number of clusters, unlike K-means.
- **Groups core points to make clusters**: DBSCAN identifies core points, which have many nearby points, and groups them together.
- **Can identify [standardised/Outliers](#standardisedoutliers)**: It detects noise points (outliers) that don't belong to any cluster.

### Python Example:

```python
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# Create sample data
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)

# Apply DBSCAN
dbscan = DBSCAN(eps=0.3, min_samples=5)
clusters = dbscan.fit_predict(X)

# Plot results
plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='plasma')
plt.show()
```

This will cluster the data and visualize it, highlighting core points and marking outliers as separate clusters.

## 🌐 Sources
1. [hex.tech - When and Why To Choose Density-Based Methods](https://hex.tech/blog/comparing-density-based-methods/#:~:text=DBSCAN%20is%20a%20density%2Dbased)
2. [newhorizons.com - DBSCAN vs. K-Means: A Guide in Python](https://www.newhorizons.com/resources/blog/dbscan-vs-kmeans-a-guide-in-python)

# Dbt {#dbt}


Data build tool is an open-source framework designed for [Data Transformation](#data-transformation) within a modern data stack. 

It enables analysts and engineers to transform, model, and manage data using [SQL](#sql) while <mark>adhering to software engineering best practices</mark> like version control, testing, and [Documentation & Meetings](#documentation--meetings). 

### Key Concepts of dbt:
1. **SQL-based Transformation**: dbt allows users to write SQL queries to define transformations and models, making it accessible for analysts who are already familiar with SQL. It doesn't handle extraction or loading of data, but focuses purely on transforming data that is already in a data warehouse.

2. **Modular and Reusable Models**: dbt encourages the creation of modular, <mark>reusable SQL "models."</mark> Each model represents a transformation, and these models can be built on top of each other. A model is essentially a SQL query stored as a `.sql` file that dbt uses to transform raw data into a refined dataset. Models are run in sequence, with dbt handling dependencies between models.

3. **Version Control and Collaboration**: dbt integrates with Git for version control, making it easy for teams to collaborate, track changes, and roll back to previous versions if needed. This promotes transparency and accountability within the data team.

4. **Testing**: dbt allows users to write and run tests to ensure data integrity and consistency. You can define tests for specific models or fields, like checking for non-null values or ensuring data uniqueness.

5. **Documentation**: dbt auto-generates documentation from your models, providing a clear overview of your data transformations, lineage, and dependencies. You can also add descriptions for models and fields to improve the clarity of your data pipelines.

6. **[Data Lineage](#data-lineage)**: dbt automatically tracks the lineage of your data by mapping dependencies between models. This makes it easy to understand how data flows through the pipeline and where any upstream or downstream issues might originate.

7. **Extensibility**: dbt has a plugin architecture that allows users to extend functionality. For example, there are adapters for popular data warehouses like Snowflake, BigQuery, Redshift, and others, making dbt highly flexible in different data stack environments.

8. **Cloud and Core Versions**: 
   - **dbt Core** is the open-source version that you run locally or in your cloud infrastructure.
   - **dbt Cloud** is a fully managed service that adds features like scheduling, logging, and a web-based IDE for dbt workflows.

### Workflow with dbt:
1. **Data Loading**: First, data is loaded into a data warehouse from various sources using ELT tools (e.g., Fivetran, Stitch).
2. **Transform with dbt**: Using dbt, you write SQL models to clean, transform, and aggregate the raw data into useful, analytical datasets.
3. **Build Data Models**: You organize your models into layers, often referred to as staging, intermediate, and final models.
4. **Testing and Documentation**: Run tests to validate data, generate lineage diagrams, and create documentation.
5. **Deploy**: Schedule or trigger dbt jobs to run in production environments, ensuring consistent and accurate data transformations.

### Example of a dbt Model:
```sql
-- models/staging_orders.sql
WITH raw_orders AS (
    SELECT * FROM {{ ref('raw_orders_data') }}
)
SELECT 
    order_id,
    customer_id,
    order_date,
    amount
FROM raw_orders
WHERE order_status = 'completed';
```
In this model:
- `ref('raw_orders_data')` is referencing another model that contains raw order data.
- The model selects and transforms only the completed orders.

### Benefits of Using dbt:
1. **Analyst Empowerment**: dbt empowers data analysts to own the transformation process using SQL, reducing dependency on data engineers for transformations.
2. **Version Control and Testing**: Built-in version control and testing improve data reliability and reduce risks of errors in production.
3. **Modularity and Scalability**: The modular nature of dbt models makes it easier to scale transformations and manage complex pipelines.
4. **Transparency and Documentation**: dbt creates clear documentation and lineage automatically, improving visibility across teams.

### Tools Integrating with dbt:
- **Data Warehouses**: Redshift, Snowflake, BigQuery, Postgres.
- **[ELT](#elt) Tools**: Stitch, Fivetran, Airbyte (for the extraction and loading phase).
- **Version Control**: GitHub, GitLab, Bitbucket (for managing dbt code).
  
### Resources:
https://www.getdbt.com/blog/what-exactly-is-dbt
[dbt](https://docs.getdbt.com/docs/introduction) 



[dbt](#dbt)
   **Tags**: #data_transformation, #data_tools

# Debugging Ipynb {#debugging-ipynb}

debugging jupyter cells

https://www.youtube.com/watch?v=CY6uZIoF_kQ

Sometimes dissapears:
https://stackoverflow.com/questions/72671709/vs-code-debug-cell-disappears-arbitrarily-in-jupyter-notebook-view



# Debugging {#debugging}


Debugging is the process of identifying, analyzing, and resolving bugs or defects in software while [Testing](#testing). It is a critical part of the [Software Development Life Cycle](#software-development-life-cycle), ensuring that applications function correctly and efficiently. Debugging involves several techniques and tools to pinpoint the source of errors and fix them.

In [ML_Tools](#ml_tools) see:
- [Debugging.py](#debuggingpy)
- [Testing_unittest.py](#testing_unittestpy)
- [Testing_Pytest.py](#testing_pytestpy)
#### Key Concepts in Debugging

1. [Types of Computational Bugs](#types-of-computational-bugs): Understanding the types of bugs, such as cumulative rounding errors, integer overflow, and race conditions, is essential for effective debugging.

2. **How to Manage/View Bugs**:
   
   - **Console Log/Dir**: Use console logging to output variable values and program states to the console, helping to trace the flow of execution.
    
   - **Availability in VSCode**: Visual Studio Code provides powerful debugging features like breakpoints, watch expressions, and call stacks to help developers inspect and modify code execution.
     
   - **Sample Script**: Creating a minimal script that reproduces the bug can simplify the debugging process by isolating the problem.
     
   - **Logging in Python**: Python's logging module allows developers to record events, errors, and informational messages, which can be crucial for diagnosing issues.
     
   - **Run and Debug**: Step through code execution using debugging tools to observe the program's behavior and identify where it deviates from expected results.
     
   - **Log Point/Break Point**: Set breakpoints to pause execution at specific lines of code, allowing inspection of variables and program state at that moment.

#### Solution Attempts

1. **Reproduce the Bug**: Simplifying the code to reproduce the bug helps in understanding its cause and facilitates easier sharing with others for collaborative debugging. Sharing on platforms like [StackBiz](#stackbiz) can help others contribute to the solution.

2. **Automated Testing**: Implementing automated tests ensures that code changes do not introduce new bugs and that existing functionality remains intact.

3. **Test-Driven Development (TDD)**: Writing tests before the actual code helps define expected behavior and ensures that the code meets these expectations.

4. **Static Analysis**: Tools like [TypeScript](#typescript) and ESLint analyze code for potential errors without executing it, helping to catch issues early in the development process.

#### Debugging Tools and Techniques

- **Integrated Development Environments (IDEs)**: IDEs like Visual Studio Code, IntelliJ IDEA, and Eclipse offer built-in debugging tools that streamline the debugging process.
  
- **Version Control Systems**: Tools like [Git](#git) allow developers to track changes and revert to previous versions if a bug is introduced.
  
- **Profilers**: These tools analyze program performance and help identify bottlenecks or inefficient code paths.
  
- **Memory Analyzers**: Tools like Valgrind help detect memory leaks and other memory-related issues.





# Debugging.Py {#debuggingpy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Utilities/Debugging.py

This script includes examples of logging, using breakpoints, and reproducing a simple bug for practice
### Key Concepts Demonstrated in the Script

1. **Logging in Python**: The script uses Python's logging module to record debug, info, error, and warning messages. This helps track the flow of execution and diagnose issues.

2. **Reproduce the Bug**: The script intentionally includes a division by zero bug to demonstrate how to identify and fix it.

3. **Breakpoints**: You can set a breakpoint in your IDE at the line where `result = divide_numbers(num1, num2)` to inspect the values of `num1` and `num2`.

4. **Automated Testing**: The script includes a simple assertion to test the `divide_numbers` function, ensuring it behaves as expected.

5. **Static Analysis**: The commented line `unused_variable = 42` can be used to simulate a static analysis warning for an unused variable.



# Decision Tree {#decision-tree}


A Decision Tree is a type of [Supervised Learning](#supervised-learning) algorithm used to predict a target variable based on input features. It involves splitting data into subsets to create a tree-like model.

Decision Tree Structure are a flowchart-like model where each internal node represents a decision based on a feature, branches represent outcomes, and leaf nodes represent final predictions.

Splits data recursively based on feature importance, forming a tree-like structure.

The decision tree algorithm calculates the [Gini impurity](#gini-impurity) for each possible split and selects the one with the lowest impurity. Use to make predictions on new data, the algorithm traverses the decision tree from the root node to a leaf node, following decision rules based on input features. Once it reaches a leaf node, it assigns the corresponding class label or prediction.

![Pasted image 20240404154526.png|500](../content/images/Pasted%20image%2020240404154526.png|500)
### Key Concepts

1. Objective: Predict a target variable using input features.
2. Splitting: Identify the best feature to split the data into subsets, aiming for homogeneous groups.
3. Impurity Calculation: Use metrics like [Gini Impurity](#gini-impurity) or [Cross Entropy](#cross-entropy) ([Gini Impurity vs Cross Entropy](#gini-impurity-vs-cross-entropy))to evaluate splits. Choose the split that minimizes impurity.
4. Purity: A node is pure if it perfectly classifies the data, requiring no further splits.
5. Leaf Node Output: Assigns the most common class label or average value in the node.
6. [Overfitting](#overfitting): Can occur if the tree is too complex. Mitigate with [pruning](#pruning) and limiting tree depth.
7. [Cross Validation](#cross-validation): Refine the model to better generalize to new data.

### Splitting Process

The splitting process in a Decision Tree involves dividing the dataset into subsets to create a tree-like structure. This process is crucial for building an effective model that can predict target variables accurately.

Splitting Criteria:
- The algorithm evaluates various features to determine the best split at each node.
- It selects the feature and split point that minimize impurity in the resulting child nodes.
- The split that most effectively reduces impurity is chosen, ensuring that each subset is as homogeneous as possible.

#### Building Process

1. **Initial Splitting**:
   - Begin at the root node and select the best feature to split the data. This selection is based on impurity measures such as Gini impurity or entropy for [classification](#classification) tasks, and variance reduction for regression tasks.
   - The goal is to create subsets that are as homogeneous as possible with respect to the target variable.

2. **Recursive Partitioning**:
   - After the initial split, each subset becomes a child node.
   - The algorithm recursively applies the splitting process to each child node.
   - Continue splitting until stopping criteria are met, such as reaching a maximum tree depth, having a minimum number of samples per node, or achieving insufficient improvement in purity.

3. **Leaf Nodes**:
   - The process continues until reaching leaf nodes, which have no further splits.
   - At each leaf node, assign a class label (for classification) or predict a continuous value (for regression) based on the majority class or average value of the samples in that node.

### Refinement

Pruning:
  - Pre-pruning: Stop tree growth early based on criteria like maximum depth or minimum impurity improvement.
  - Post-pruning: Allow the tree to grow fully, then prune back based on performance metrics.

### [Hyperparameter](#hyperparameter)

Can use [GridSeachCv](#gridseachcv) to pick the best paramaters.

| **Parameter**       | **Purpose**                      | **Effect**                  | **Example**                                      |
|---------------------|----------------------------------|-----------------------------|--------------------------------------------------|
| `criterion`         | Splitting criteria               | Impacts decision logic.     | `criterion='gini'` or `criterion='entropy'`      |
| `max_depth`         | Maximum tree depth               | Prevents overfitting.       | `max_depth=5` limits the tree depth to 5.        |
| `min_samples_split` | Min samples to split a node      | Limits tree growth.         | `min_samples_split=10` requires at least 10 samples to split a node. |
| `min_samples_leaf`  | Min samples at leaf node         | Reduces overfitting.        | `min_samples_leaf=5` ensures every leaf has at least 5 samples. |
| `max_features`      | Features considered for splitting| Adds randomness.            | `max_features='sqrt'` or `max_features=3`.       |
| `max_leaf_nodes`    | Max leaf nodes allowed           | Reduces overfitting.        | `max_leaf_nodes=20` caps the tree at 20 leaves.  |
| `class_weight`      | Adjusts for imbalanced data      | Improves fairness.          | `class_weight='balanced'` or `class_weight={0:1, 1:2}`. |
| `ccp_alpha`         | Pruning parameter                | Simplifies tree.            | `ccp_alpha=0.01` prunes weak splits based on complexity. |


### Advantages and Disadvantages of Decision Trees

Advantages:
- Simple and [interpretability|interpretable](#interpretabilityinterpretable) model.
- Minimal data preparation required.
- Transparent decision-making process.

Disadvantages:
- Prone to overfitting, especially with complex datasets.
- Sensitive to small changes in data.
- Can become complex with many features.

[Decision Tree](#decision-tree)



# Declarative {#declarative}


In a **declarative data pipeline**, the focus is on *what* needs to be achieved, not *how* it should be executed. You define the desired outcome or the data products, and the system takes care of the underlying execution details, such as the order in which tasks are performed. This is in contrast to an **imperative** pipeline, where the developer explicitly specifies the steps and the order in which they should be executed. Here's a breakdown of the key aspects:

### **Declarative Programming**:
- Focuses on *<mark>what</mark>* needs to be done.
- Describes the desired state or result without dictating the control flow or step-by-step process.
- In a data pipeline context, a declarative approach might involve specifying the desired data products and letting the system optimize how and when different parts of the pipeline are executed.
- Example: [SQL](#sql) is often considered declarative because you specify the result you want (e.g., the output of a query) without explicitly stating the steps for how the database engine should retrieve it.

### **Imperative Programming**:
- Focuses on *<mark>how</mark>* tasks should be done.
- Specifies the <mark>control flow</mark> explicitly, dictating the exact steps to be performed and the order of operations.
- In a data pipeline, this would involve writing scripts that detail each step in the transformation and loading process in the sequence they must be executed.
- Example: A series of Python scripts that process data in a specific sequence.

### **Advantages of Declarative Pipelines**:
1. **Easier to Debug**: Since the desired state is clearly defined, it is easier to identify discrepancies between the intended outcome and the current state. This can help pinpoint issues in the pipeline.
   
2. **Automation**: Declarative systems often enable better automation since the system has the flexibility to determine the most efficient way to achieve the defined goals.

3. **Simplicity and Intent**: Declarative approaches focus on the *<mark>intent</mark>* of the program, making it easier for others to understand what the program is supposed to do without having to dive into implementation details. 

4. **Reactivity**: The pipeline can automatically adjust when inputs or dependencies change. For example, if certain data dependencies change, the system can rerun the necessary parts of the pipeline to maintain consistency.

### **Example in Data Engineering**:

A declarative approach to data engineering would involve **Functional Data Engineering** principles. This involves treating data as immutable and focusing on defining the desired transformations and outputs in a declarative manner. Instead of writing imperative scripts for each data transformation step, you'd define the desired outputs, and the system would optimize the execution.

### **Use Cases**:
Declarative pipelines are particularly useful in [data lineage](#data-lineage), **[Data Observability](#data-observability)**, and [Data Quality](#data-quality) monitoring**. By defining *what* data products should exist and what their properties should be, it's easier to track changes and ensure the consistency and quality of data. It also makes systems more resilient to changes, as the declarative nature enables the system to adjust the execution order or method dynamically, based on current conditions.

# Deep Learning Frameworks {#deep-learning-frameworks}


[Watch Overview Video](https://www.youtube.com/watch?v=MDP9FfsNx60)

### TensorFlow

**Focus**: 
  TensorFlow is a comprehensive open-source platform for machine learning. It provides a flexible and comprehensive ecosystem of tools, libraries, and community resources that lets researchers push the state-of-the-art in ML, and developers easily build and deploy ML-powered applications.
  
**Integration**: 
  TensorFlow can implement a wide range of machine learning algorithms, including those available in [Sci-kit Learn](#sci-kit-learn), making it versatile for various applications.
  
**Modularity**: 
  Its modular architecture allows users to deploy computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices.
  
**Parallelization**: 
  TensorFlow is optimized for high-performance numerical computation, making it suitable for large-scale machine learning tasks that require parallel processing.
  
**Use Cases**: 
  TensorFlow is widely used in both academic research and industry for tasks such as image and speech recognition, natural language processing, and more.


### Sci-kit Learn

**Focus**: 
  Sci-kit Learn is a simple and efficient tool for data mining and data analysis, built on NumPy, SciPy, and matplotlib. It is primarily used for traditional machine learning techniques such as classification, regression, clustering, and dimensionality reduction.
  
**Limitations**: 
  While excellent for classical machine learning tasks, Sci-kit Learn is not designed for deep learning or neural network architectures, which require more specialized frameworks like TensorFlow or PyTorch.

### Keras

**API Level**: 
  Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It allows for easy and fast prototyping through user-friendly, modular, and extensible code.
  
**Integration**: 
  Keras is tightly integrated with TensorFlow 2.0, providing a simplified interface for building and training deep learning models.
  
**Purpose**: 
  Designed to enable fast experimentation, Keras is ideal for beginners and researchers who need to quickly prototype and test new ideas.
  
**Performance**: 
  While Keras simplifies model building, it may not be as performant as lower-level frameworks like TensorFlow when it comes to fine-tuning and optimizing models for production.


# Deep Learning {#deep-learning}


>[!Summary]
> Deep learning is a subset of machine learning that uses neural networks to process large-scale data for tasks like image and speech recognition, natural language processing, and recommendation systems. 
> 
> A neural network consists of layers of nodes where each node performs weighted sums of its inputs, applies activation functions like ReLU or sigmoid, and produces an output. 
> 
> [Backpropagation](#backpropagation) is the primary algorithm for training neural networks by minimizing error through [Gradient Descent](#gradient-descent). Regularization techniques, such as dropout, prevent overfitting. 
> 
> Popular frameworks like [PyTorch](#pytorch) and [TensorFlow](#tensorflow) facilitate deep learning model development.

Questions:
- [What is the role of gradient-based optimization in training deep learning models. ](#what-is-the-role-of-gradient-based-optimization-in-training-deep-learning-models)
- [Explain different gradient descent algorithms, their advantages, and limitations.](#explain-different-gradient-descent-algorithms-their-advantages-and-limitations)

Areas of Deep Learning:
- [LLM](#llm)
- [Neural network|Neural Network](#neural-networkneural-network)


>[!Follow up questions]
> - How does the choice of activation function affect the performance of deep learning models across different tasks?
> - What are the trade-offs between different gradient descent algorithms (e.g., [Stochastic Gradient Descent|SGD](#stochastic-gradient-descentsgd) vs. Adam) in training neural networks? See [Optimisation techniques](#optimisation-techniques).

>[!Related Topics]
> - [Transfer Learning](#transfer-learning): Applying pre-trained models to new tasks.



# Deep Q Learning {#deep-q-learning}


Deep [Q-Learning](#q-learning) is a type of [reinforcement learning](#reinforcement-learning) algorithm that combines Q-Learning with [Neural network](#neural-network). Necessary when Q-Table grows too large.

Updates the weights in the model.

![Pasted image 20250220133838.png](../content/images/Pasted%20image%2020250220133838.png)

## Key Concepts

### Target Network

- **Purpose**: The target network is used to stabilize the training process in Deep Q-Learning.
- **When is it needed?**: It is needed when updating the Q-values to prevent oscillations and divergence during training.
- **How it works**: The target network is a copy of the main Q-network and is used to generate target Q-values. It is updated less frequently than the main network, often using a technique called a "soft update," where the target network is slowly adjusted towards the main network over time.

### Experience Replay

- **Purpose**: Experience replay is used to break the correlation between consecutive experiences, which can lead to inefficient learning and instability.
- **Issue it resolves**: When an agent learns from sequential experiences, the strong correlations between them can cause problems such as oscillations and instability in learning.
- **How it works**: 
  - Experiences (state, action, reward, next state) are stored in a memory buffer.
  - During training, random mini-batches of experiences are sampled from this buffer to update the network.
  - This random sampling helps to generate uncorrelated experiences, improving stability and efficiency.
  - It also allows the agent to reuse experiences for multiple updates, increasing data efficiency.


# Deepseek {#deepseek}


[LLM](#llm) example 

open source

optimising for preformance vs efficency.

Deepseek leading in efficency

o3 mini

[Chain of thought](#chain-of-thought)
can see it - ui choice

[Distillation](#distillation) - use gpt output and trains on it. 

[security](#security)

#drafting 

[jevon paradox](#jevon-paradox) - as cost decreases usage increases 

edge inference [Edge Machine Learning Models](#edge-machine-learning-models)

[The Genius of DeepSeek’s 57X Efficiency Boost](https://www.youtube.com/watch?v=0VLAoVGf_74)
key value caching 
impacts attention block compute scaling: linear
increased memory usage
Solution: 
multi-query attention vs mutli - head attention
Grouped-query attention
Multi-head latent attention - deepseek uses
uses compresses latent space
linear algebra 
absorbed weights at training




- **Access to Advanced AI Features Without Payment**: You can now access powerful reasoning models like DeepSeek's R1 and ChatGPT's 03 Mini for free. These models are good at complex math, programming, and step-by-step reasoning.

- **Privacy Protection**: If you are concerned about privacy, you have options to protect your data by using platforms like Perplexity, Venice AI, or Cursor to access DeepSeek models, which keeps data in the US. For full privacy, you can run DeepSeek models locally using LM Studio or oLama, but this may limit access to more powerful models due to hardware constraints.

- **Smart Choices About Switching**: Consider if DeepSeek offers clear advantages for your specific needs before changing your current AI tools or workflows. DeepSeek is beneficial for developers focused on cost minimization. If you are an everyday user already paying for ChatGPT and concerned about data storage, switching may not be necessary unless DeepSeek significantly improves your workflow.

# Deleting Rows Or Filling Them With The Mean Is Not Always Best {#deleting-rows-or-filling-them-with-the-mean-is-not-always-best}



# Demand Forecasting {#demand-forecasting}


- **Overview**: Demand response programs encourage consumers to adjust their energy usage during peak periods in response to time-based rates or other incentives. RL can optimize how these programs are implemented.
- **Applications**:
    - **Incentive Management**: [Reinforcement learning|RL](#reinforcement-learningrl) models can dynamically adjust incentives for consumers to reduce usage during peak times based on real-time grid conditions and consumer behavior.
    - **Behavioral Adaptation**: By learning from historical consumer response data, RL systems can predict how different consumers will react to incentives, allowing for more tailored and effective demand response strategies.

How can we model the effects of energy consumption patterns on demand forecasting

- **Dynamic Programming**: Useful in solving multi-stage decision problems, such as optimal scheduling of power plants.


- **Linear Programming**: Used for optimizing resource allocation in energy production and distribution, such as maximizing output while minimizing costs.
-

# Dendrograms {#dendrograms}

Dendrograms show **close** vectors is the data where taken as a vector.

Can tell which <mark>features are the most similar</mark> with [Dendrograms](#dendrograms)

![Pasted image 20240405173403.png](../content/images/Pasted%20image%2020240405173403.png)






# Dependency Manager {#dependency-manager}

[Virtual environments](#virtual-environments)

[requirements.txt](#requirementstxt)

[TOML](#toml)

[Poetry](#poetry)

# Design Thinking Questions {#design-thinking-questions}

- "What is the user’s need?"
- "What constraints are at play?"
- "How might we…?" — a classic starter for idea generation.

# Determining Threshold Values {#determining-threshold-values}

In [Binary Classification](#binary-classification) problems, a threshold value is used to convert predicted probabilities into discrete class labels. The choice of threshold significantly impacts the model's performance, affecting [Evaluation Metrics](#evaluation-metrics).

Important Considerations:
* [Imbalanced Datasets|Class Imbalance](#imbalanced-datasetsclass-imbalance): If the classes are imbalanced, the choice of threshold can be significantly affected. Techniques like oversampling, undersampling, or using weighted loss functions can help mitigate the impact of class imbalance.
* [Data Quality](#data-quality): The quality of the training data can also influence the choice of threshold. If the data is noisy or contains outliers, the chosen values may not be optimal.
* Choose [Evaluation Metrics](#evaluation-metrics) that are appropriate for the specific problem and the desired trade-off between different types of errors.

Here are common methods for determining the optimal threshold value:
- Receiver Operating Characteristic (ROC) Curve Analysis : [ROC (Receiver Operating Characteristic)](#roc-receiver-operating-characteristic)
- [Precision-Recall Curve](#precision-recall-curve) Analysis
- [Cost-Sensitive Analysis](#cost-sensitive-analysis)

# Devops {#devops}


DevOps refers to practices for collaboration and automation between [Software Development Portal](#software-development-portal) (Dev) and IT operations (Ops) teams, aiming for faster, more reliable software delivery.

**Integration**: It integrates the work of software development and operations teams by fostering a culture of collaboration and shared responsibility.

**Principles**: 
- Emerges from Agile principles.
- Emphasizes collaboration between development and operations teams.

**Approach**: 
- Utilizes continuous integration and continuous delivery ([CI-CD](#ci-cd)) to ensure frequent code changes and quick feedback loops.
- Enables rapid and reliable updates with high levels of automation and efficiency.

**Goals**: 
 - Ensures existing processes are optimized and streamlined.

 Related to:
- [DataOps](#dataops)

# Difference Between Databricks Vs. Snowflake {#difference-between-databricks-vs-snowflake}



# Difference Between Snowflake To Hadoop {#difference-between-snowflake-to-hadoop}


Snowflake and Hadoop are both [Data Management](#data-management) systems, but they serve different purposes and have distinct architectures and functionalities. 

In summary, Snowflake and Hadoop are both powerful tools for managing and analyzing data, but they are optimized for different types of workloads and use cases. Snowflake excels in <mark>cloud-based data warehousing</mark> and real-time analytics, while Hadoop is suited for <mark>large-scale data processing</mark> and storage in a distributed environment.

[Snowflake](#snowflake)

[Hadoop](#hadoop)
### **Key Differences**

1. **Deployment**:
   - **Snowflake**: Cloud-based, requires no hardware or infrastructure management by users.
   - **Hadoop**: Can be deployed on-premises or in the cloud, but typically requires more hands-on management.

2. **Ease of Use**:
   - **Snowflake**: User-friendly with a simple SQL interface, automated maintenance, and optimization.
   - **Hadoop**: Requires more technical expertise to set up, manage, and optimize.

3. **Performance and Scalability**:
   - **Snowflake**: Excels in performance for analytical queries with the ability to scale compute resources independently.
   - **Hadoop**: Scales horizontally by adding more nodes, suitable for large-scale data processing but may have higher query latency.

4. **Cost**:
   - **Snowflake**: Pay-as-you-go model based on compute and storage usage.
   - **Hadoop**: Costs depend on the infrastructure (hardware or cloud resources) and maintenance overhead.

### Example Use Case Scenarios

- **Snowflake**: A retail company wanting to perform real-time analytics and reporting on sales data would benefit from Snowflake’s high performance and ease of use for SQL-based queries and dashboards.

- **Hadoop**: A tech company needing to process and analyze massive amounts of log data for machine learning models might use Hadoop due to its ability to handle large-scale data processing and diverse data types.



# Differentation {#differentation}


# Forward Mode Automatic Differentiation

uses dual numbers

implemented in tensor flow

see also Reverse Mode Automatic Differentiation

Fast,Flexible,Exact

# Digital Transformation {#digital-transformation}



<mark>"Digital transformation starts with data centralisation"</mark>

To digitally transform your department, you'll need to approach the process in a structured and strategic way that addresses both technological and organizational changes.

### [Data Audit](#data-audit)
   - **Understand Department Processes:** Identify current workflows, key processes, and technologies being used.
   - **Identify Pain Points:** Collect feedback from employees on inefficiencies, bottlenecks, and areas for improvement.
   - <mark>**Data Collection and Analysis</mark>:** Review existing data and determine how it's being used (or underutilized) in decision-making processes.
   
### 2. **Define Clear Objectives**
   - **Set Transformation Goals:** Align transformation efforts with the broader goals of the organization. For example, improving efficiency, enhancing customer experience, or better data-driven decision-making.
   - **Quantifiable Metrics:** Define success metrics (e.g., reduced processing time, increased customer satisfaction, or data accuracy).

### 3. **Engage Stakeholders**
   - **Get Leadership Buy-In:** Ensure leadership is aligned with the transformation and committed to driving it forward.
   - **Collaborate with Employees:** Involve employees in planning to understand their needs and gain their support for changes.
   - **Map Stakeholders:** Identify key influencers, decision-makers, and implementers within the department.

### 4. **Evaluate and Select Technologies**
   - **Research Tools and Platforms:** Evaluate technologies such as cloud platforms, automation tools, analytics solutions, and collaboration software.
   - **Pilot New Technologies:** Test small-scale pilots to validate the value of proposed solutions before large-scale rollouts.
   - **Interoperability:** Ensure that new technologies can integrate smoothly with existing systems and data.

### 5. **Build a Transformation Roadmap**
   - <mark>**Prioritize Initiatives</mark>:** Focus on high-impact areas that align with the department's goals.
   - **Create a Timeline:** Develop a step-by-step implementation timeline, including milestones and deadlines.
   - **Allocate Resources:** Assign teams, budgets, and technology resources for each phase of the transformation.

### 6. **Change Management and Training**
   - **Implement Change Management Strategies:** Proactively manage resistance to change by communicating the benefits of digital transformation.
   - **Provide Training and Support:** Train staff on new technologies and provide ongoing support to ensure a smooth transition.
   - **Foster a Digital Culture:** Encourage a mindset of innovation, experimentation, and continuous improvement.

### 7. **Implement New Technologies and Processes**
   - **Roll Out in Phases:** Implement technology solutions gradually, allowing time for adjustments and feedback.
   - **Monitor and Iterate:** Regularly check the implementation process and adjust based on feedback and performance metrics.

### 8. **Measure and Optimize**
   - **Track Key Metrics:** Use the pre-defined success metrics to measure the impact of digital transformation.
   - **Continuously Improve:** Adjust and optimize workflows, tools, and processes based on performance and evolving needs.

### 9. **Ensure Long-Term Sustainability**
   - **Encourage Innovation:** Promote continuous learning and adoption of new technologies to keep the department adaptable to future changes.
   - **Monitor Industry Trends:** Stay updated on new digital trends and opportunities that can benefit the department.
   - **Review and Update Goals:** Periodically revisit your transformation goals and adjust them based on organizational needs and market shifts.

### Example Focus Areas:
   - **Automation:** Streamline repetitive processes using Robotic Process Automation (RPA) or AI.
   - **Data-Driven Decision Making:** Introduce analytics tools and dashboards for real-time insights.
   - **Cloud Adoption:** Shift to cloud-based platforms for better scalability, collaboration, and remote work capabilities.
   - **Collaboration Tools:** Implement communication and project management platforms like Microsoft Teams or Slack to enhance collaboration.

[Digital Transformation](#digital-transformation)

Businesses have data, but need to evaluate, organise, clean , and prepare before using.

[Digital Transformation](#digital-transformation) is a [Change Management|Change Program](#change-managementchange-program).

Where a business can benefit. Areas that can be improved:
- reporting and financial
- data quality / backend
- network management
- Customer services

What we want to move away from/towards
- Silos in our work
- No KPIs
- Multiple spreadsheets doing the same thing
- One-time reporting (get a standarised process in place)
- Systems that are not understood or replicatible
- Poor data quality
- Missed opportunities
- Small, reactive team to issues.

Client focues:
- What does the client want? A fuller understanding of the network. The ability to dive deeper if they wanted to. The ability to understand their bills (AccM). To be able to plan their projects better knowing the information we have?
- What can the water usage tell us about a site/all sites, that could be beneficial to the client? Stress levels on the system - predictive maintenance? Does high usage result in higher leakage.
- What are the reasons for leaks? - non error - what happened on the system that caused it?

---

Digital transformation aims to provide innovation to business processes.  
- save time
- do more, 
- save money.

Digital transformation aims to:
- Automate repetitive tasks.  
- Create systematic improvements to business sops, through strategies, methodologies, technologies.  
- How can we prepare digital assets for further automation?  

To consider when conducting:
 - Conduct a Data audit company wide, what is the format of data and where is it? is the data accessible?
 - Reporting mechanisms what can you do with the data 
 - How to handle data debt/minimise it.
 - What does leadership want as a data/product pathway.
 - How can we continuously improve the quality of our data? 
 - Need to track by design, i.e. what do we need to know to benefit in the future.   
  




  


  



# Digital Twin {#digital-twin}




>[!Summary]
> A **digital twin** is a virtual representation of a physical object, system, or process that mirrors its real-world counterpart in real-time. This digital model is used to simulate, monitor, analyze, and optimize the physical entity by continuously updating based on data collected from sensors, devices, or other inputs. The concept is widely applied in industries such as manufacturing, healthcare, [Energy](#energy), smart cities, and more to improve decision-making, predictive maintenance, and efficiency. A digital twin is a powerful tool for enhancing real-time decision-making, optimizing processes, and predicting future performance by bridging the physical and digital worlds. Its applications continue to expand across various industries, helping organizations to reduce costs, improve efficiency, and innovate faster.

>[!Example]
>Consider a **digital twin of a wind turbine**. Sensors installed on the turbine gather data on operational conditions such as wind speed, blade position, temperature, and vibration. This data is continuously transmitted to the digital twin, which mirrors the turbine’s state in real-time. The digital twin runs simulations to predict when parts of the turbine might fail due to wear and tear. Maintenance teams can use this information to schedule repairs before a breakdown occurs, minimizing downtime and improving the turbine's efficiency.

### Key Components of a Digital Twin:

1. **Physical Object or Process**:
   - The real-world entity (such as a machine, a building, a production line, or even a human body) that the digital twin replicates.

2. **Digital Model**:
   - A virtual replica of the physical entity, designed using data models, physics-based simulations, and other analytical tools. The digital model reflects the structure, behavior, and function of the physical object or process.

3. **Data Integration**:
   - The digital twin <mark>relies on real-time data from sensors,</mark> IoT devices, or historical databases connected to the physical counterpart. This data flow enables the twin to reflect current operating conditions and states.

4. **Analytics and Simulation**:
   - Advanced analytics (e.g., machine learning, artificial intelligence) and simulations are applied to the digital twin to gain insights into the performance, predict future behavior, and test scenarios that would be difficult or expensive to replicate in the real world.

5. **Feedback Loop**:
   - A digital twin allows for continuous interaction between the physical and digital worlds. Insights or predictions from the digital twin can inform changes to the physical system, and any updates in the physical system feed back into the digital twin, maintaining accuracy and alignment.

### Types of Digital Twins:

1. **Component/Asset Twin**:
   - Represents individual components or parts of a larger system (e.g., the digital twin of a jet engine or an electric motor).

2. **System or Unit Twin**:
   - Models entire systems or units, such as a production line in a factory or the electrical system of a building.

3. **Process Twin**:
   - Focuses on simulating and optimizing processes, such as a manufacturing workflow or supply chain operations.

4. **Environment Twin**:
   - Used to simulate larger, more complex systems like cities, ecosystems, or large-scale infrastructure (e.g., smart city initiatives or environmental monitoring).

### Applications of Digital Twins:

1. **Manufacturing**:
   - In smart factories, digital twins are used to simulate production processes, predict machine failures, optimize maintenance schedules, and improve product design by running real-time simulations of manufacturing conditions.

2. **Healthcare**:
   - Digital twins of patients are being developed to model individual health profiles, allowing for personalized treatment plans and predictive diagnostics. A digital twin of a human organ, for example, could simulate medical treatments before they are applied to the patient.

3. **[Energy](#energy)**:
   - In energy systems, digital twins help optimize the operation of power plants, monitor grid performance, and simulate the impacts of renewable energy integration, improving reliability and efficiency.

4. **Smart Cities**:
   - Urban planners use digital twins to model traffic flow, infrastructure usage, or environmental conditions. This allows them to simulate different scenarios and optimize city operations, reduce congestion, and improve public services.

5. **Aerospace and Automotive**:
   - Digital twins are used extensively in designing, testing, and maintaining complex systems like aircraft, satellites, and autonomous vehicles. Engineers can simulate operational conditions to identify potential problems before they occur in the physical system.

6. **Building Management**:
   - Digital twins of buildings or infrastructure monitor and control systems like HVAC, lighting, and security, improving energy efficiency and safety. They are also used for simulating how a building will perform under different conditions (e.g., weather events or occupancy changes).

### Benefits of Digital Twins:

1. **Real-time Monitoring**:
   - Provides live feedback from the physical entity, which enables organizations to make faster, more informed decisions.

2. **Predictive Maintenance**:
   - Predicts when equipment or systems are likely to fail based on real-time data and simulations, reducing downtime and maintenance costs.

3. **Optimization**:
   - Enables the continuous improvement of processes by testing scenarios in a virtual environment without disrupting real-world operations.

4. **Improved Design and Innovation**:
   - Digital twins allow engineers and designers to experiment with different configurations, materials, or processes virtually, leading to faster, cheaper, and more innovative solutions.

5. **Reduced Risk**:
   - By simulating potential failures or dangerous scenarios in the digital world, organizations can assess risk and plan mitigation strategies without putting the physical system at risk.

### Challenges:

1. **[Data Management](#data-management)**:
   - Digital twins require a large amount of real-time data to maintain accuracy. Collecting, managing, and processing this data efficiently can be complex and costly.

2. **Integration**:
   - Integrating the digital twin with physical systems, particularly in legacy environments, can be challenging due to compatibility issues and the need for IoT infrastructure.

3. **Security**:
   - Because digital twins rely on real-time data transmission, they are vulnerable to cyberattacks, which can lead to compromised systems or intellectual property theft.

4. **Scalability**:
   - Scaling digital twin models to encompass entire cities or large systems involves high computational and infrastructural requirements.






# Dimension Table {#dimension-table}

A dimension table is a key component of a [star schema](#star-schema) or snowflake schema in a data warehouse. It provides descriptive attributes (or dimensions) related to the [Facts](#facts) stored in a fact table.

They provide the context and descriptive information necessary for analyzing the quantitative data stored in fact tables (e.g., product names, customer demographics, time periods).

1. **Descriptive Attributes**: Dimension tables contain qualitative data that describe the entities involved in the business process. For example, a product dimension table might include attributes such as product name, category, brand, and manufacturer.

2. **Primary Key**: Each dimension table has a primary key that uniquely identifies each record in the table. This primary key is used as a foreign key in the [Fact Table](#fact-table) to establish relationships between the two.

3. **Hierarchies**: Dimension tables often include hierarchies that allow for data to be analyzed at different levels of granularity. For example, a time dimension might include attributes for year, quarter, month, and day, allowing users to drill down or roll up in their analysis.

4. **Smaller Size**: Compared to fact tables, dimension tables are typically smaller in size, as they contain descriptive data rather than large volumes of transactional data.

5. **Static Data**: Dimension tables usually contain relatively static data that does not change frequently, such as product details or customer information. However, they can be updated as needed to reflect changes in the business.

6. **Support for Filtering and Grouping**: Dimension tables enable users to filter and group data in reports and analyses. For example, users can analyze sales data by different dimensions such as time, geography, or product category.

Examples
  - **TimeDimension**: Contains information about the time period.
    - Columns: `DateKey`, `Year`, `Quarter`, `Month`, `Day`
  - **ProductDimension**: Contains product details.
    - Columns: `ProductKey`, `ProductName`, `ProductCategory`
  - **RegionDimension**: Contains regional information.
    - Columns: `RegionKey`, `RegionName`, `Country`





[Dimension Table](#dimension-table)
   **Tags**: #data_modeling, #data_warehouse

# Dimensional Modelling {#dimensional-modelling}


Dimensional modeling is a design technique used in [Data Warehouse](#data-warehouse)used to structure data for efficient <mark>retrieval</mark> and analysis. It is particularly well-suited for organizing data in a way that supports complex [queries](#queries) and reporting, making it easier for business users to understand and interact with the data. 

Dimensional modeling is a foundational technique in building data warehouses and is often associated with methodologies like the <mark>Kimball</mark> approach, which emphasizes the use of [Star Schema](#star-schema) and the importance of understanding business processes and user requirements.

Key Concepts in Dimensional Modeling

 - [Fact Table](#fact-table) & [Facts](#facts)
- [Dimension Table](#dimension-table)
- [Grain](#grain)

Benefits of Dimensional Modeling: [Performance Dimensions](#performance-dimensions)





[Dimensional Modelling](#dimensional-modelling)
   **Tags**: #data_modeling, #data_warehouse

# Dimensionality Reduction {#dimensionality-reduction}


Dimensionality reduction is a step in the [Preprocessing](#preprocessing) phase of machine learning that helps simplify models, enhance interpretability, and improve computational efficiency.

Its a technique used to reduce the number of input variables (features) in a dataset while retaining as much information as possible. This process is essential for several reasons:

1. **Improves Model Performance**: Reducing the number of features can help improve the performance of machine learning models by minimizing overfitting and reducing noise.

2. **Enhances Visualization**: It allows for easier [Data Visualisation](#data-visualisation) of high-dimensional data by projecting it into lower dimensions (e.g., 2D or 3D).

3. **Reduces Computational Cost**: Fewer features mean less computational power and time required for training models.

### Common Techniques
- **Principal Component Analysis ([Principal Component Analysis](#principal-component-analysis))**: A statistical method that transforms the data into a new coordinate system, where the greatest variance by any projection lies on the first coordinate <mark>(principal component/orthogonal components )</mark>, the second greatest variance on the second coordinate, and so on.

- **t-Distributed Stochastic Neighbor Embedding ([t-SNE](#t-sne))**: A technique particularly well-suited for visualizing high-dimensional data by reducing it to two or three dimensions while preserving the local structure of the data. t-SNE is a non-linear technique used for visualization and dimensionality reduction by preserving pairwise similarities between data points, making it suitable for exploring high-dimensional data.

- [Linear Discriminant Analysis](#linear-discriminant-analysis) method used for both classification and dimensionality reduction, which finds a linear combination of features that best separates two or more classes.

### [Explain the curse of dimensionality](#explain-the-curse-of-dimensionality)


# Dimensions {#dimensions}


Dimensions are the categorical buckets that can be used to segment, filter, or group—such as sales amount region, city, product, color, and distribution channel. 

Traditionally known from [OLAP (online analytical processing)|OLAP](#olap-online-analytical-processingolap)cubes with Bus Matrixes, and [Dimensional Modeling](Dimensional%20Modelling.md). 

They provide context to the [Facts](#facts).

# Directed Acyclic Graph (Dag) {#directed-acyclic-graph-dag}


DAG stands for **Directed Acyclic Graph**. 

A DAG is a graph where information must travel along with a finite set of nodes connected by vertices. There is no particular start or node and also no way for data to travel through the graph in a loop that circles back to the starting point.

It's a popular way of building data pipelines in tools like [Apache Airflow](#apache-airflow), [dagster](#dagster), [Prefect](#prefect). It clearly defines the data lineage. As well, it's made for a functional approach where you have the [idempotency](term/idempotency.md) to restart pipelines without side-effects.

![](dag.png)

# Directory Structure {#directory-structure}


[To make a file tree](https://faun.pub/create-a-repository-tree-and-print-it-to-a-file-f376f103f169)

├── README.md          <- The top-level README for developers using this project.
├── data
│   ├── external       <- Data from third party sources.
│   ├── interim        <- Intermediate data that has been transformed.
│   ├── processed      <- The final, canonical data sets for modeling.
│   └── raw            <- The original, immutable data dump.
│
├── models             <- Trained and serialized models, model predictions, or model summaries
│
├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
│                         the creator's initials, and a short `-` delimited description, e.g.
│                         `1.0-jqp-initial-data-exploration`.
│
├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
│   └── figures        <- Generated graphics and figures to be used in reporting
│
├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
│                         generated with `pip freeze > requirements.txt`
│
├── src                <- Source code for use in this project.
    ├── data           <- Scripts to download or generate data
    │   └── make_dataset.py
    │
    ├── features       <- Scripts to turn raw data into features for modeling
    │   └── build_features.py
    │
    ├── models         <- Scripts to train models and then use trained models to make
    │   │                 predictions
    │   ├── predict_model.py
    │   └── train_model.py
    │
    └── visualization  <- Scripts to create exploratory and results oriented visualizations
        └── visualize.py

# Distillation {#distillation}

training smaller models with larger.

[Small Language Models](#small-language-models)

![Pasted image 20250130074219.png](../content/images/Pasted%20image%2020250130074219.png)



# Distributed Computing {#distributed-computing}


**Distributed Computing** is essential for managing **massive data volumes** by distributing tasks across multiple servers or machines. This enables scalability and efficient data processing.

**[Hadoop](#hadoop)** is a framework that handles both data storage and processing across **clusters of servers**:
  - It ensures **scalability**: can easily grow as more data is added.
  - Provides **redundancy**: data is replicated across servers to prevent loss in case of failures.

Tools like [Apache Spark](#apache-spark) are built to process data in these **distributed environments**, allowing for fast, parallel processing across the cluster.

Distributed computing is central to modern data handling, driven by frameworks like Spark and Hadoop, supported by cloud infrastructure, and expanding into real-time and [edge computing](#edge-computing).
### Distributed Computing: Current State

Distributed computing enables the processing of massive datasets and computational tasks by distributing them across multiple machines. This approach increases [scalability](#scalability), parallelism, and fault tolerance, making it essential for modern data processing.
#### Key Frameworks

- **Hadoop**: An early pioneer, Hadoop introduced distributed storage via **HDFS** and data processing with **MapReduce**, allowing tasks to be split across clusters of servers.
- **Apache Spark**: A faster alternative to MapReduce, Spark uses in-memory computing for real-time, iterative tasks, improving speed and efficiency. It has become the leading tool for distributed data processing.

#### Distributed Storage

- **HDFS** and cloud storage systems like **[Amazon S3](#amazon-s3)** break data into smaller parts and distribute them across multiple servers. This setup provides high throughput, redundancy, and fault tolerance.
- **Distributed databases** such as **Cassandra** and **Bigtable** offer scalable storage for structured data, ensuring availability across nodes.

#### Real-Time Processing ([Data Streaming](#data-streaming))

- Frameworks like **Apache Flink** and **Kafka Streams** are critical for real-time data processing, enabling continuous data handling as it is generated. They are commonly used in applications requiring instant processing, such as fraud detection or live analytics.

#### Cloud-Native Computing

- **Cloud platforms** (e.g., AWS, Google Cloud, Azure) have made distributed computing accessible through services like **Amazon EMR** and **Google Dataproc**. These services simplify the deployment and management of distributed applications.
- [kubernetes](#kubernetes) has become the standard for orchestrating distributed applications, managing containers and ensuring high availability across clusters.

#### Trends and Challenges

- **Edge computing** is gaining momentum, enabling data to be processed closer to the source (e.g., IoT devices), reducing latency and bandwidth usage.
- Challenges include **fault tolerance**, **network [latency](#latency)**, and **data consistency**. Innovations in consensus algorithms and fault-tolerant storage systems are working to mitigate these issues.


# Distribution_Analysis.Py {#distribution_analysispy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Distribution_Analysis.py

The goodness-of-fit results represent the **p-values** from the **Kolmogorov-Smirnov (KS) test**, which assesses how well the data fits each distribution. Here's how to interpret these values:

1. **Higher p-value** → The distribution is a better fit.
2. **Lower p-value** → The distribution is a poor fit (likely not the correct model for the data).
3. **Threshold**: A common significance level is **0.05**.
    - If **p > 0.05**, we do **not** reject the hypothesis that the data follows this distribution.
    - If **p < 0.05**, we reject the hypothesis, meaning the data **likely does not follow** that distribution.

Example using penguins.csv column "bill_depth_mm"

Goodness-of-fit results for bill_depth_mm
Gaussian: 0.026308596409291618
T: 0.025906678848475195
Chi-squared: 1.4504381882536289e-15
Exponential: 2.8020502445188308e-14
Logistic: 0.05019989765502264

- **Gaussian (0.0263)** → **Poor fit** (p < 0.05). The data likely does not follow a normal distribution.
- **T (0.0259)** → **Poor fit** (p < 0.05). The data does not fit a t-distribution well.
- **Chi-squared (1.45e-15)** → **Very poor fit** (extremely low p-value). The data is **highly unlikely** to follow a chi-squared distribution.
- **Exponential (2.80e-14)** → **Very poor fit** (extremely low p-value). The data is **not** exponentially distributed.
- **Logistic (0.0502)** → **Acceptable fit** (p ≈ 0.05). The data could **potentially** follow a logistic distribution.

The **Logistic distribution** has the **highest p-value (0.0502)**, making it the **best candidate** among the tested distributions. However, since it's **borderline (≈0.05)**, you may want to visualize the distribution and compare the fits.

# Distributions {#distributions}


In [ML_Tools](#ml_tools) see:
- [Distribution_Analysis.py](#distribution_analysispy)
#### Discrete Distributions
These distributions have probabilities concentrated on specific values.

- [Uniform](#uniform) Distribution: All outcomes are equally likely. Example: Drawing a card from a shuffled deck. A boxplot can be meaningful if there’s variation in the distribution. Since the values are discrete, the boxplot will show the range and quartiles effectively.
- [Bernoulli](#bernoulli) Distribution: Represents two possible outcomes. Example: Coin flip (heads or tails), true/false scenarios. A bar chart or frequency plot would be better for visualizing the proportions. or rolling a dice.
- [Binomial](#binomial) Distribution: Represents the number of successes in a sequence of Bernoulli trials. Example: Number of heads in 10 coin flips,
- [Poisson](#poisson) Distribution: Models the frequency of events in a fixed interval. Example: Number of website visits per hour. A boxplot is suitable for this distribution, showing central tendencies, spread, and potential outliers.

#### Continuous Distributions
These distributions have probabilities spread over a continuous range.

- [Gaussian Distribution](#gaussian-distribution): Characterized by a bell-shaped curve, symmetric with thin tails. Example: Heights, exam scores.
- T Distribution: Similar to the normal distribution but with fatter tails, useful with limited data.
- Chi-squared Distribution: Asymmetric and non-negative, commonly used in [hypothesis testing](#hypothesis-testing).
- Exponential Distribution: Models the time between events. Example: Time between website traffic hits, radioactive decay.
- Logistic Distribution: S-shaped curve, often used in forecasting and modeling growth.
  ![Pasted image 20250308191945.png](../content/images/Pasted%20image%2020250308191945.png)
  
#### Q-Q plots (Quantile-Quantile Plots)

A Q-Q (quantile-quantile) plot is a graphical tool used to compare the distribution of a dataset against a theoretical distribution (e.g., normal, logistic, exponential). It helps assess how well a given distribution fits the data.

How Q-Q Plots Work:

1. Sort your dataset → Compute the sample quantiles (percentiles).
2. Compute the theoretical quantiles → Take the same number of points from the theoretical distribution (e.g., normal, logistic).
3. Plot sample quantiles vs. theoretical quantiles:
    - If the points lie on a straight diagonal line, the data follows the theoretical distribution.
    - If the points deviate significantly, the data does not fit that distribution.

Interpreting a Q-Q Plot:

- Straight diagonal line → Data follows the chosen distribution.
- Curved S-shape → Data has skewness.
    - Upward curve (right tail high) → Right-skewed.
    - Downward curve (left tail high) → Left-skewed.
- Heavy tails (outliers) → Points at the ends deviate from the line.
- Light tails (thin-tailed distribution) → Points at the ends fall below the line.
### Practical Applications

Feature Distribution: Understanding the distribution of numerical/ categortical feature values across samples can provide insights into data characteristics.

  - Observation: Analyze the spread and central tendency of data.
  - Decision: Determine appropriate statistical methods or transformations.

### Related Notes

In [ML_Tools](#ml_tools) see: [Feature_Distribution.py](#feature_distributionpy)

- [Violin plot](#violin-plot)
- [Boxplot](#boxplot)


# Docker Image {#docker-image}

A Docker image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including the code, runtime, libraries, environment variables, and configuration files. Docker images are used to create Docker containers, which are instances of these images running in an isolated environment.

Docker images are used for:

1. **Consistency**: They ensure that software runs the same way regardless of where it is deployed, whether on a developer's laptop, a test server, or in production.

2. **Portability**: Docker images can be easily shared and moved across different environments, making it easier to deploy applications.

3. **Version Control**: Images can be versioned, allowing developers to track changes and roll back to previous versions if necessary.

4. **Isolation**: Each Docker container runs in its own isolated environment, which helps in avoiding conflicts between different applications or services running on the same host.

5. **Scalability**: Docker images can be used to quickly scale applications by running multiple containers from the same image.
### Example: Running a Simple Web Server (dont do unless required)

1. **Install Docker**: First, ensure Docker is installed on your machine. You can download it from the [Docker website](https://www.docker.com/products/docker-desktop).

2. **Pull a Docker Image**: Use a pre-built Docker image from Docker Hub. For this example, we'll use the official Nginx image, which is a popular web server.

   
```bash
   docker pull nginx
   
```

3. **Run a Docker Container**: Start a container using the Nginx image. This will run the web server.

   
```bash
   docker run --name my-nginx -d -p 8080:80 nginx
   
```

   - `--name my-nginx`: Names the container "my-nginx".
   - `-d`: Runs the container in detached mode (in the background).
   - `-p 8080:80`: Maps port 80 in the container to port 8080 on your host machine.

4. **Access the Web Server**: Open a web browser and go to `http://localhost:8080`. You should see the default Nginx welcome page, indicating that the web server is running inside the Docker container.

5. **List Running Containers**: Check which containers are running.

   
```bash
   docker ps
   
```

6. **Stop the Container**: When you're done, you can stop the container.

   
```bash
   docker stop my-nginx
   
```

7. **Remove the Container**: If you no longer need the container, you can remove it.

   
```bash
   docker rm my-nginx
   
```

### Key Features Demonstrated:

- **Portability**: The Nginx server runs the same way on any machine with Docker installed.
- **Isolation**: The web server runs in its own environment, separate from other applications.
- **Ease of Use**: Starting and stopping services is straightforward with simple commands.
- **Resource Efficiency**: Docker containers are lightweight compared to virtual machines.

This example gives you a taste of how Docker can be used to quickly deploy and manage applications. As you become more familiar with Docker, you can explore building your own images, managing multi-container applications with Docker Compose, and more.

# Docker {#docker}

Utilizes Docker images [Docker Image](#docker-image) to set up containers for consistent development and testing environments.

Containers can include necessary dependencies like Python and pip.

Tutorial:
- [The Only Docker Tutorial You Need To Get Started](https://www.youtube.com/watch?v=DQdB7wFEygo)

Docker Volumes - storing data

Docker Compose

Multi use containers

`docker init` command
- generated Dockerfile - image
- compose.yaml
- .dockerignore





# Documentation & Meetings {#documentation--meetings}

## Tools  
- [pdoc](#pdoc) – Auto-generate Python API documentation  
- [Mermaid](#mermaid) – Create diagrams and flowcharts from text in a Markdown-like syntax  
## Templates  

### Project & Technical Meetings  

- [Technical Design Doc Template](#technical-design-doc-template) – Document system architecture, components, data flow  
- [Pull Request Template](#pull-request-template) – Checklist and summary for PRs (code, tests, reviewers, notes)  
- [Experiment Plan Template](#experiment-plan-template) – Hypothesis, variables, metrics, and analysis plan  
- [Retrospective Template](#retrospective-template) – Guide for reviewing past sprint/project cycles  
### Data & Analytics Meetings  
- [Data Request Template](#data-request-template) – Intake form for new analysis or data extract needs  
- [One Pager Template](#one-pager-template) – Summarize a project, idea, or proposal on a single page  
- [Meeting Notes Template](#meeting-notes-template) – Standard format for taking concise and structured notes  
### Cross-functional / Stakeholder Meetings  
- [One Pager Template](#one-pager-template) – Summarize a project, idea, or proposal on a single page  
- [Meeting Notes Template](#meeting-notes-template) – Standard format for taking concise and structured notes  
- [Data Request Template](#data-request-template) – Intake form for new analysis or data extract needs  
### Reporting & Strategic Meetings  
- [Postmortem Template](#postmortem-template) – Structure for documenting incidents and learnings  
- [Retrospective Template](#retrospective-template) – Guide for reviewing past sprint/project cycles  
- [One Pager Template](#one-pager-template) – Summarize a project, idea, or proposal on a single page  
### Collaborative & Feedback Sessions  
- [1-on-1 Template](#1-on-1-template) – Agenda for 1-on-1 check-ins with manager or reports  
- [Feedback Template](#feedback-template) – Structure for giving/receiving peer or performance feedback  
## Meeting Types

### Project & Technical Meetings  
- **Sprint Planning / Stand-ups (Agile):**  
  Define priorities, plan tasks, and report blockers on a daily/weekly basis  

- **Design & Architecture Reviews:**  
  Evaluate technical designs—e.g., pipeline architecture, schemas, model design  

- **Code Reviews / Pair Programming:**  
  Collaborative code sessions for learning and validation  

- **Pipeline/Model Monitoring & Debugging:**  
  Diagnose failures, resolve data/model performance issues in production  

### Data & Analytics Meetings  
- **Exploratory Data Analysis (EDA) Discussions:**  
  Share insights, anomalies, or feature engineering results  

- **Model Review & Evaluation:**  
  Present metrics, discuss trade-offs (e.g., ROC-AUC, fairness, overfitting)  

- **Data Quality & Validation:**  
  Ensure schema consistency, missing data checks, rule-based validations  

- **Experiment Review:**  
  Discuss A/B test results, statistical significance, and business impact  

### Cross-functional / Stakeholder Meetings  

- **Product or Domain Team Syncs:**  
  Discuss project goals, KPIs, data availability  

- **Ad Hoc Analysis Requests:**  
  Clarify requirements for exploratory or one-off reporting  

- **User Feedback / Data Product Reviews:**  
  Gather user input on dashboards, ML outputs, or data access tools  

- **Requirements Grooming:**  
  Translate business requirements into data/technical specifications  

### Reporting & Strategic Meetings  

- **Quarterly Business Reviews / OKR Alignment:**  
  Evaluate progress against metrics and strategic initiatives  

- **Dashboard Walkthroughs / Reporting Demos:**  
  Demonstrate key metrics, performance trends, and data tooling  

- **Metrics Definition Meetings:**  
  Align teams on KPI definitions, calculation logic, and data sources  

### Collaborative Initiatives  

- **Knowledge Sharing / Lunch & Learns:**  
  Internal sessions to demo tools, share best practices, or teach concepts  

- **Workshops (e.g., dbt, Airflow, MLOps):**  
  Hands-on learning sessions for technical upskilling  

- **Data Governance / Compliance:**  
  Ensure responsible data use, privacy adherence, and lineage tracking  

- **Hackathons / Innovation Days:**  
  Time-boxed events for experimentation and cross-team collaboration  


# Dropout {#dropout}


**Dropout** is a [Regularisation](#regularisation) technique used in [Neural network](#neural-network) training to prevent [overfitting](#overfitting). It works by randomly dropping units (neurons) during training, which helps the network to not rely too heavily on any single neuron.

Purpose
- The main goal of dropout is to improve the generalization of the model by reducing over-reliance on specific neurons. This encourages the network to learn more robust features that are useful in different contexts.

How It Works
- During each training iteration, a subset of neurons is randomly selected and ignored (dropped out). This means their contribution to the activation of downstream neurons is temporarily removed on the forward pass, and any weight updates are not applied to the neuron on the backward pass.

Implementation
```python
from tensorflow.keras.layers import Dropout
# Add a dropout layer with a dropout rate of 0.5
# The dropout rate (e.g., 0.5) specifies the fraction of neurons to drop during training. A rate of 0.5 means that half of the neurons will be dropped at each iteration.
Dropout(0.5)
```

# DS & ML Portal {#ds--ml-portal}

### Machine Learning Fundamentals

- [ML_Tools](#ml_tools)
- [Supervised Learning](#supervised-learning)
- [Unsupervised Learning](#unsupervised-learning)
- [Reinforcement learning](#reinforcement-learning)
- [Deep Learning](#deep-learning)

### Model Training and Optimisation

- [Learning rate](#learning-rate)
- [Overfitting](#overfitting)
- [Regularisation](#regularisation)
- [Hyperparameter](#hyperparameter)
- [Hyperparameter Tuning](#hyperparameter-tuning)
- [Model Optimisation](#model-optimisation)
- [Model Selection](#model-selection)
- [Vanishing and exploding gradients problem](#vanishing-and-exploding-gradients-problem)

### Feature Engineering and Data Handling

- [Feature Selection](#feature-selection)
- [Feature Engineering](#feature-engineering)
- [Imbalanced Datasets](#imbalanced-datasets)
- [Outliers](#outliers)
- [Anomaly Detection](#anomaly-detection)
- [Multicollinearity](#multicollinearity)
- [Dimensionality Reduction](#dimensionality-reduction)
- [Clustering](#clustering)
### Machine Learning Models

Classification Models

- [Classification](#classification)
- [Binary Classification](#binary-classification)
- [Support Vector Machines](#support-vector-machines)
- [Decision Tree](#decision-tree)
- [Random Forests](#random-forests)
- [K-nearest neighbours](#k-nearest-neighbours)
- [Logistic Regression](#logistic-regression)

Regression Models

- [Regression](#regression)
- [Linear Regression](#linear-regression)

Boosting and Optimisation

- [Gradient Descent](#gradient-descent)
- [Gradient Boosting](#gradient-boosting)
- [XGBoost](#xgboost)

### Deep Learning and Neural Networks
 
- [BERT](#bert)
- [LSTM](#lstm)
- [Recurrent Neural Networks](#recurrent-neural-networks)
- [Transformer](#transformer)
- [Attention mechanism](#attention-mechanism)
- [Neural network](#neural-network)

### Model Evaluation and Metrics

- [Cost Function](#cost-function)
- [Loss function](#loss-function)
- [Cross Entropy](#cross-entropy)
- [Evaluation Metrics](#evaluation-metrics)
- [Model Evaluation](#model-evaluation)
- [Accuracy](#accuracy)
- [Precision](#precision)
- [Recall](#recall)

### Algorithms and Frameworks

- [Machine Learning Algorithms](#machine-learning-algorithms)
- [Optimisation techniques](#optimisation-techniques)
- [Optimisation function](#optimisation-function)
- [Model Ensemble](#model-ensemble)
- [Batch Processing](#batch-processing)
- [Apache Spark](#apache-spark)
- [Sklearn](#sklearn)

### Statistical and Data Analysis Concepts

- [Distributions](#distributions)
- [Statistics](#statistics)
- [Correlation](#correlation)
- [Data Analysis](#data-analysis)
- [Data Quality](#data-quality)
- [Principal Component Analysis](#principal-component-analysis)

### Misc

- [Interpretability](#interpretability)
- [RAG](#rag)

# Duckdb In Python {#duckdb-in-python}

To use **DuckDB** in Python, you can follow these steps to install the DuckDB library and perform basic operations such as creating a database, running queries, and manipulating data. Here's a simple guide:

### Step 1: Install DuckDB

You can install DuckDB using pip. Open your terminal or command prompt and run:

```bash
pip install duckdb
```
### Example: Full Code

Here’s a complete example that incorporates all the steps:

```python
import duckdb

# Step 1: Connect to an in-memory database
conn = duckdb.connect(database=':memory:')

# Step 2: Create a table
conn.execute("""
CREATE TABLE users (
    id INTEGER,
    name VARCHAR,
    age INTEGER
)
""")

# Step 3: Insert data
conn.execute("""
INSERT INTO users VALUES
(1, 'Alice', 30),
(2, 'Bob', 25),
(3, 'Charlie', 35)
""")

# Step 4: Query data
result = conn.execute("SELECT * FROM users").fetchall()

# Print the results
for row in result:
    print(row)

# Step 5: Close the connection
conn.close()
```

### Additional Features

DuckDB also supports advanced features such as:

- **Reading from CSV and Parquet files**: You can load data directly from these formats using commands like `READ_CSV` or `READ_PARQUET`.
- **Integration with Pandas**: You can easily convert DuckDB query results to Pandas DataFrames for further analysis.

### Example of Reading from a CSV

```python
# Load data from a CSV file into a DuckDB table
conn.execute("CREATE TABLE my_data AS SELECT * FROM read_csv_auto('path/to/your/file.csv')")
```

# Duckdb Vs Sqlite {#duckdb-vs-sqlite}

Choosing between **[DuckDB](#duckdb)** and **[SQLite](#sqlite)** for data processing in [Python](#python) depends on your specific use case and requirements.

While **SQLite** is an excellent choice for lightweight applications, local data storage, and simple transactional workloads.

**DuckDB** shines in scenarios involving complex analytical queries, large datasets, and data science workflows.

If your primary focus is on data analysis and you need high performance for analytical tasks, DuckDB may be the better option. However, if you need a simple, lightweight database for small-scale applications, SQLite could be more appropriate.

### 1. **Performance for Analytical Queries**
- **DuckDB** is optimized for analytical workloads and can handle complex queries involving large datasets more efficiently than SQLite. It uses a [columnar storage](#columnar-storage) format, which is particularly beneficial for aggregation and analytical operations.
- **SQLite**, while fast for transactional workloads, may not perform as well with large-scale analytical queries.

### 2. **In-Memory Processing**
- DuckDB can operate entirely in-memory, allowing for faster data processing and query execution. This is especially useful for data science tasks where speed is critical.
- SQLite can also work in-memory, but its performance may not match that of DuckDB for analytical tasks.

### 3. **Support for Complex Data Types**
- DuckDB supports more complex data types and operations, such as nested data structures and advanced analytical functions, which can be advantageous for data analysis.
- SQLite has a more limited set of data types and may not support some advanced analytical functions.

### 4. **Integration with Data Science Tools**
- DuckDB is designed to integrate seamlessly with data science libraries like Pandas, making it easier to perform data analysis and manipulation directly within your Python workflows.
- While SQLite can also be used with Pandas, DuckDB's integration is often more straightforward for analytical tasks.

### 5. **Cloud and Big Data Compatibility**
- DuckDB is designed to work well with cloud-based data warehouses and big data environments, making it suitable for modern data workflows that involve large datasets stored in cloud storage.
- SQLite is more suited for lightweight applications and local data storage.

### 6. **Columnar Storage Format**
- DuckDB's columnar storage format allows for more efficient data compression and faster query performance on analytical workloads, as it reads only the necessary columns for a query.
- SQLite uses a row-based storage format, which can be less efficient for certain types of analytical queries.

### 7. **Ease of Use for Data Transformation**
- DuckDB simplifies data transformation processes, allowing you to perform transformations directly within the database after loading the data. This can streamline workflows and reduce the need for additional data processing steps.
- SQLite requires more manual handling for data transformations, especially when dealing with large datasets.


# Duckdb {#duckdb}

**DuckDB** is an open-source analytical database management system designed for efficient data processing and analysis. It is optimized for running complex queries on large datasets and is particularly well-suited for data science and analytics tasks. Here are some key features and characteristics of DuckDB:

Resources:
- https://duckdb.org/
- [DuckDB in python](#duckdb-in-python)
- [DuckDB vs SQLite](#duckdb-vs-sqlite) 

### Key Features

1. **In-Memory Processing**: DuckDB operates primarily in-memory, which allows for fast query execution and data manipulation.

2. [Columnar Storage](#columnar-storage) It uses a columnar storage format, which is efficient for analytical queries that often involve aggregations and scans over large datasets.

3. **SQL Support**: DuckDB supports SQL as its query language, making it accessible to users familiar with SQL syntax.

4. **Integration with Data Science Tools**: DuckDB can be easily integrated with popular data science tools and programming languages, such as Python and R, allowing for seamless data analysis workflows.

5. **Lightweight and Easy to Use**: It can be embedded in applications and does not require a separate server, making it lightweight and easy to deploy.

6. **Compatibility**: DuckDB can read from various data formats, including CSV, [Parquet](#parquet)

### Use Cases
- [Data Analysis](#data-analysis)
- [Data Transformation](#data-transformation)
- [Querying](#querying)


# Dummy variable trap {#dummy-variable-trap}

### Key Takeaways:

- The dummy variable trap occurs due to [multicollinearity](#multicollinearity), where <mark>one dummy variable can be perfectly predicted from others.</mark>
- Dropping one dummy variable avoids this issue and ensures that the model has a reference category against which the other categories are compared.
- This approach leads to a well-conditioned model and allows for more interpretable regression coefficients.
### Dummy Variable Trap

The dummy variable trap refers to a scenario in which there is multicollinearity in your dataset when you create dummy variables for categorical features. Specifically, it occurs when one of the dummy variables in a set of dummy variables can be perfectly predicted by a linear combination of the others.

This situation arises when you create dummy variables for a categorical feature with $n$ categories, leading to $n$ binary columns. However, if you include all $n$ dummy variables in your regression model, <mark>the model will face redundancy because knowing the values of $n-1$ dummy variables will already give you the value of the last one (since all the categories must add up to 1 for each observation)</mark>. This results in perfect multicollinearity.

### Why Do We Need to Drop One of the Dummy Variables?

In a regression model, multicollinearity can cause problems because it makes the estimation of coefficients unstable, leading to unreliable statistical inferences. Specifically, the model can't determine which of the correlated variables is truly responsible for explaining the variation in the target variable.
### Example:

Suppose you have a categorical feature `town` with three categories: `West Windsor`, `Robbinsville`, and `Princeton`. When you apply [one-hot encoding](#one-hot-encoding), you create three dummy variables:

|town|West Windsor|Robbinsville|Princeton|
|---|---|---|---|
|West Windsor|1|0|0|
|Robbinsville|0|1|0|
|Princeton|0|0|1|

Now, if you include all three dummy variables in a linear regression model, the columns `West Windsor`, `Robbinsville`, and `Princeton` will be perfectly correlated. For example, if the values of `West Windsor` and `Robbinsville` are both 0, then `Princeton` must be 1, and vice versa.

This creates multicollinearity because you can predict one dummy variable perfectly by knowing the others. Hence, you need to drop one of the dummy variables—usually, you drop one category, which becomes the reference group.

If you drop the `West Windsor` dummy column, your table would look like this:

|town|Robbinsville|Princeton|
|---|---|---|
|West Windsor|0|0|
|Robbinsville|1|0|
|Princeton|0|1|

Now, your model will use the `West Windsor` category as the baseline. The coefficients of `Robbinsville` and `Princeton` in the regression model will indicate how much higher or lower their prices are compared to `West Windsor`.

# Eda {#eda}


Exploratory [Data Analysis](#data-analysis) (EDA) is an approach to analyzing datasets to summarize their main characteristics, often utilizing visual methods. EDA helps users to:

- Understand the Data's Structure: Gain insights into the organization and format of the data.
- Detect Patterns: Identify trends and patterns within the data.
- Decide on Statistical Techniques: Choose appropriate statistical methods by examining [distributions](#distributions) and [correlation](#correlation).
- Select Variables: Determine which variables to include in further analysis.
- Handle [Data Quality](#data-quality): Address issues related to data quality and integrity.
- Spot Anomalies and [standardised/Outliers](#standardisedoutliers): Identify unusual data points that may affect analysis.
- Generate and Test Hypotheses: Formulate hypotheses and validate them using statistical methods.
- Check Assumptions: Verify assumptions through statistical summaries and graphical representations.
### Common Techniques Used in EDA

- Descriptive Statistics: Calculating measures such as mean, median, mode, [standard deviation](#standard-deviation), and percentiles to summarize data.
- [Data Visualisation](#data-visualisation): Using plots and charts like histograms, box plots, scatter plots, and bar charts to visually explore data.
- Correlation Analysis: Assessing relationships between variables using [correlation](#correlation) coefficients and scatter plots.
- [Data Transformation](#data-transformation): Applying transformations to data, such as normalization or log transformation, to better understand its characteristics.
### Implementation 

In [ML_Tools](#ml_tools) see:
- [EDA_Pandas.py](#eda_pandaspy)

# Eda_Pandas.Py {#eda_pandaspy}



# Edge Machine Learning Models {#edge-machine-learning-models}

**Edge ML** refers to deploying machine learning models directly on edge devices, such as IoT sensors, smartphones, or embedded systems, instead of relying on cloud-based processing. This is crucial in scenarios requiring low-latency, real-time decision-making, or environments with limited connectivity.

#### Key Characteristics of Edge ML Models:

1. **Low Latency**
   - Models running at the edge can make real-time decisions with minimal delay. This is critical in applications like autonomous vehicles, industrial automation, or real-time health monitoring, where delays can have serious consequences.

2. **Reduced Bandwidth Usage**
   - By processing data locally, edge ML models reduce the need to send large amounts of data to the cloud for analysis. This is particularly valuable in environments with limited or expensive connectivity (e.g., remote locations or bandwidth-constrained networks).
   
3. **Privacy Preservation**
   - Processing sensitive data on-device, instead of sending it to the cloud, enhances privacy and reduces the risk of data breaches. This is important in healthcare, financial services, or any scenario involving personal data.

4. **Energy Efficiency**
   - Edge devices often have limited power resources. As a result, models deployed at the edge need to be optimized for low energy consumption, ensuring they can operate for extended periods without requiring frequent battery replacements or recharging.

#### Common Applications of Edge ML:
   
1. **Autonomous Systems (e.g., Drones, Robots, Vehicles)**
   - Autonomous systems rely on real-time decision-making for navigation, obstacle detection, and control. Edge ML allows these systems to react instantaneously to their surroundings without depending on external servers.

3. **Smart Cities and Industrial IoT**
   - Edge ML powers applications such as **traffic monitoring**, **environmental sensing**, and **predictive maintenance** in smart factories. For example, sensors in factories can use edge models to predict equipment failure before it occurs, ensuring smooth operations without cloud reliance.

#### Challenges in Edge ML:

1. **Model Compression**
   - Since edge devices often have limited storage and computational power, ML models need to be compressed or optimized (e.g., using techniques like quantization, pruning, or knowledge distillation) to run efficiently while maintaining accuracy.

2. **On-Device Model Updates**
   - Keeping models updated without frequent cloud interactions is a challenge. Edge devices need mechanisms for **incremental learning** or efficient updates without disrupting normal operations.
#### Popular Frameworks for Edge ML:

- **TensorFlow Lite**: A lightweight version of TensorFlow, designed to run on mobile and embedded devices.
- **[PyTorch](#pytorch) Mobile**: PyTorch’s framework for deploying ML models on mobile devices.
- **[ONNX](#onnx) Runtime**: Optimized for running machine learning models on various platforms, including edge devices.
- **Edge Impulse**: A platform specifically for building ML models for edge devices, particularly for IoT applications.

Edge ML is driving innovation in industries requiring decentralized, real-time intelligence, enabling devices to make smart decisions locally while minimizing reliance on cloud resources.

# Education And Training {#education-and-training}

Adaptive Learning Systems

- **Overview**: Adaptive learning systems use technology to tailor educational experiences to individual student needs. RL is instrumental in personalizing these systems.
- **Applications**:
    - **Personalized Learning Paths**: RL algorithms can create customized learning paths for students based on their performance, preferences, and engagement levels, adapting content delivery in real-time.
    - **Feedback and Assessment**: Adaptive systems can provide immediate feedback based on student responses, reinforcing concepts through targeted exercises and adjusting difficulty levels as needed.
    - **Engagement Strategies**: By analyzing student interactions, RL can suggest motivational strategies, such as gamification elements or timely reminders, to keep students engaged and motivated.

# Elastic Net {#elastic-net}


This method combines both L1 ([Lasso](#lasso)) and L2 ([Ridge](#ridge)) regularization by adding both absolute and squared penalties to the loss function. It strikes a balance between Ridge and Lasso.

It is particularly useful when you have high-dimensional datasets with highly correlated features.

The Elastic Net loss function is:

    $$\text{Loss} = \text{MSE} + \lambda_1 \sum_{i=1}^{n} |w_i| + \lambda_2 \sum_{i=1}^{n} w_i^2$$
    
where $\lambda_1$ controls the L1 regularization and $\lambda_2$ controls the L2 regularization.

#### Code

```python
from sklearn.linear_model import ElasticNet

# Initialize an Elastic Net model
model = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio controls the L1/L2 mix
model.fit(X_train, y_train)
```

# Elt {#elt}


**ELT** (Extract, Load, Transform) is a data integration approach that involves three main steps:

1. **Extract (E)**: Data is extracted from a source system.
2. **Load (L)**: The raw data is loaded into a destination system, such as a data warehouse.
3. **Transform (T)**: Transformation of the data occurs within the destination system after the data has been loaded.

This approach contrasts with the traditional **ETL** (Extract, Transform, Load) method, where data is transformed before reaching the destination. For a detailed comparison, see [ETL vs ELT](#etl-vs-elt)

### Advantages of ELT
The shift from ETL to ELT has been facilitated by several factors:

- **Cost Efficiency**: The decreasing costs of cloud-based storage and computation have reduced the advantages of ETL's pre-loading data transformation.
  
- **Cloud-Based Data Warehouses**: The emergence of cloud-based data warehouses like Redshift, [BigQuery](#bigquery), and Snowflake has made the ELT approach more feasible and efficient.
### Historical Context
Historically, ETL was preferred for reasons that are now less relevant:

- **Cost Savings**: ETL was believed to save costs by filtering out unwanted data before loading. However, this is less significant with modern cloud solutions.
  
- **Complexity Management**: ETL minimizes the complexity of post-loading transformations. Yet, contemporary tools like [dbt](#dbt) simplify this process, making it easier to perform transformations after loading.



[ELT](#elt)
   **Tags**: #data_transformation, #data_integration

# Embedded Methods {#embedded-methods}

Embedded methods for [Feature Selection](#feature-selection) <mark>integrate feature selection directly into the model training process.</mark>

Embedded methods provide a convenient and efficient approach to feature selection by seamlessly integrating it into the model training process, ultimately leading to models that are more parsimonious and potentially more interpretable.

1. **Incorporated into Model Training**: Unlike [Filter method](#filter-method) and [Wrapper Methods](#wrapper-methods), which involve feature selection as a separate step from model training, embedded methods perform feature selection simultaneously with model training. This means that feature importance or relevance is determined within the context of the model itself.

2. **Regularization Techniques**: Embedded methods commonly use [Regularisation](#regularisation) techniques to penalize the inclusion of unnecessary features during model training. 

3. **Automatic Feature Selection**: Embedded methods automatically select the most relevant features by learning feature importance during the training process. The model adjusts the importance of features iteratively based on their contribution to minimizing the [Loss function](#loss-function).

4. **Examples of Embedded Methods**:
   - **[Lasso](#lasso) (L1 Regularization)**:
   - [Elastic Net](#elastic-net): Elastic Net combines L1 ([Lasso](#lasso)) and L2 ([Ridge](#ridge)) regularization .
   - **Tree-based Methods**: [Decision Tree](#decision-tree) and ensemble methods like [Random Forests](#random-forests) and [Gradient Boosting](#gradient-boosting) inherently perform feature selection during training by selecting the most informative features at each split node of the tree.

5. **Advantages**:
   - Simplicity: Embedded methods simplify the feature selection process by integrating it into model training, reducing the need for additional preprocessing steps.
   - Efficiency: Because feature selection is performed during model training, embedded methods can be more computationally efficient compared to wrapper methods, which require training multiple models.

6. **Considerations**:
   - Hyperparameter Tuning: Tuning regularization parameters or other model-specific parameters may be necessary to optimize feature selection performance.
   - Model [interpretability](#interpretability): While embedded methods can automatically select features, interpreting the resulting model may be challenging, especially for complex models like ensemble methods.



# Embeddings For Oov Words {#embeddings-for-oov-words}

Can you find words in a [Vector Embedding|word embedding](#vector-embeddingword-embedding) that where not used to creates the embedding?

Yes, but with important caveats. If a word is not in the [spaCy](#spacy) model’s vocabulary with a vector, then:

### ✅ What you can do

#### Option 1: Filter out words without vectors (what you're doing now)
This is the cleanest option:
```python
if token.has_vector:
    embeddings.append(token.vector)
    valid_words.append(word)
```

#### Option 2: Fallback to character-level embeddings (optional)
If you're using `en_core_web_lg`, spaCy sometimes provides approximate vectors for out-of-vocabulary (OOV) words using subword features. But with `en_core_web_md`, OOV words truly lack vector meaning.

#### Option 3: Use a different embedding model
Use FastText or transformer-based models (e.g., Sentence Transformers), which can produce [embeddings for OOV words](#embeddings-for-oov-words) based on subword information or context.

Example with [FastText](#fasttext) (using gensim):
```python
from gensim.models import KeyedVectors

model = KeyedVectors.load_word2vec_format("cc.en.300.vec")  # or download from FastText
embedding = model.get_vector("unseenword")  # FastText will synthesize it
```

### 💡 Summary

| Approach                     | Handles OOV? | Notes |
|-----------------------------|--------------|-------|
| spaCy `en_core_web_md`      | ❌            | Skips words without vectors (recommended) |
| spaCy `en_core_web_lg`      | ⚠️ Sometimes  | May infer vectors using subword info |
| FastText / GloVe            | ✅            | Good for unseen words |
| Sentence Transformers (BERT)| ✅            | Contextualized, ideal for phrases/sentences |

#NLP #ml_process #ml_optimisation

# Emergent Behavior {#emergent-behavior}



# Encoding Categorical Variables {#encoding-categorical-variables}


### Overview

Categorical variables need to be converted into numerical representations to be used in models, particularly in [Regression](#regression) analysis. This process is essential for transforming categorical results into a format that algorithms can interpret.

### Label Encoding

This method assigns a unique integer to each category in the variable.

```python
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
var1_cat = df['var1']  # Replace df with your DataFrame
var1_encoded = label_encoder.fit_transform(var1_cat)
```
For example, if `df[col]` contains the categories `['apple', 'banana', 'orange']`, the `LabelEncoder` would transform them into `[0, 1, 2]`.

However, keep in mind that this encoding can imply an order or hierarchy in the data, which might not be intended. In some cases, you might want to use `OneHotEncoder` instead, which creates a binary vector for each category.{}

Given a term in the df you can transform it without needing to look up its value.
```python
company="google"
company_n = LabelEncoder().transform([company])
```
### One-Hot Encoding

This technique creates a binary column for each category, allowing the model to treat each category as a separate feature.

```python
from sklearn.preprocessing import OneHotEncoder

binary_encoder = OneHotEncoder(categories='auto')
var1_1hot = binary_encoder.fit_transform(var1_encoded.reshape(-1, 1))
var1_1hot_mat = var1_1hot.toarray()
var1_DF = pd.DataFrame(var1_1hot_mat, columns=['cat1', 'cat2', 'cat3'])  # Adjust column names as needed
var1_DF.head()
```

Understanding OneHotEncoder:

The `OneHotEncoder` from `sklearn.preprocessing` is used to convert categorical integer values into a format that can be provided to machine learning algorithms to do a better job in prediction. It creates a binary column for each category and returns a sparse matrix or dense array.

### Converting All Categorical Variables to Dummies

To convert all categorical variables in a DataFrame to dummy variables, you can use the following loop:

```python
for col in df.columns:
    if df[col].dtype == 'object':
        dummies = pd.get_dummies(df[col], drop_first=False)
        dummies = dummies.add_prefix(f'{col}_')
        df.drop(col, axis=1, inplace=True)
        df = df.join(dummies)
```

Dummy Variable Trap:
When using one-hot encoding, it's important to avoid the **dummy variable trap**, which occurs when one category can be perfectly predicted from the others. To prevent this, you can drop one of the dummy variables, as one column is sufficient to represent a binary choice (0 or 1).

### Alternative Encoding Method
Another way to encode categorical variables is by mapping them directly to integers:

```python
dataset['var1'] = dataset['var1'].map({'A': 0, 'B': 1, 'C': 2}).astype(int)
```
### Related Topics
- **[Regression](#regression)**: Understanding how regression models utilize encoded variables.
- **[Feature Engineering](#feature-engineering)**: Techniques to enhance model performance through better feature representation.
### Overview

- Categorical variables need to be converted into numerical representations for use in models. This is essential for transforming categorical data into a format that algorithms can interpret.

### Methods

- Label Encoding: Assigns a unique integer to each category.
- One-Hot Encoding: Creates a binary column for each category, allowing the model to treat each category as a separate feature.

### Example Code

```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import pandas as pd

# Label Encoding
label_encoder = LabelEncoder()
var1_encoded = label_encoder.fit_transform(df['var1'])

# One-Hot Encoding
binary_encoder = OneHotEncoder(categories='auto')
var1_1hot = binary_encoder.fit_transform(var1_encoded.reshape(-1, 1))
var1_1hot_mat = var1_1hot.toarray()
var1_DF = pd.DataFrame(var1_1hot_mat, columns=['cat1', 'cat2', 'cat3'])
```

# Energy ABM {#energy-abm}

- **Complex Systems Understanding**: Energy systems involve numerous stakeholders (producers, consumers, regulators) with diverse interests and behaviors. [Agent-Based Modelling|ABM](#agent-based-modellingabm) helps capture this complexity, providing a clearer picture of system dynamics.
- **Adaptive Behavior**: Agents in ABM can adapt their behavior based on interactions, mirroring how consumers and producers might respond to incentives or changes in the market.
- **Scenario Analysis**: ABM allows for "what-if" analyses, enabling stakeholders to explore different scenarios, such as the impact of implementing new technologies or policies on energy systems.
- **Data-Driven Insights**: With the rise of smart meters and IoT devices, ABM can leverage real-time data to improve model accuracy and relevancy, enhancing decision-making processes.

# Energy Storage {#energy-storage}


## Energy Storage

Battery farms exist.

Stored energy can be traded.

Stored energy can be stored using distributed system such as EV cars.

# Energy {#energy}


Areas of interest:
- [Smart Grids](#smart-grids)
- [Energy Storage](#energy-storage)
- [Demand forecasting](#demand-forecasting)
- [Network Design](#network-design)
- [Energy ABM](#energy-abm)

Questions:
- [How to model to improve demand forecasting](#how-to-model-to-improve-demand-forecasting)
- What patterns can be identified in consumer behavior data to inform energy pricing strategies?
- How can predictive maintenance be implemented using data from smart sensors in energy infrastructure?

**Techniques:**
- **[Differential Equations](#differential-equations)**: Used to model dynamic systems in energy generation and consumption. For example, they can describe the behavior of power systems over time or the thermal dynamics of energy storage systems.
- **[Stochastic Modeling](#stochastic-modeling)**: Involves random variables to model uncertainties in energy production (e.g., variability in solar or wind energy) and consumption.
- [Agent-Based Modelling](#agent-based-modelling)Simulates interactions of agents (consumers, producers, regulators) to understand complex systems and emergent phenomena in energy markets.
- **Time Series Analysis**: Analyzing historical data to forecast future energy demand or production trends.
- **[Regression](#regression) Analysis**: Used to model relationships between different variables, such as energy prices and consumption patterns.
- [Neural network|Neural Network](#neural-networkneural-network) Particularly deep learning, is applied for complex pattern recognition in large datasets, such as detecting anomalies in energy consumption or predicting equipment failures.


Dymanic pricing, incentised load management, local generation 
  
Use green energy if on grid

# Environment Variables {#environment-variables}

Solution 1: Set Environment Variables Permanently (Recommended)
This ensures that environment variables persist across sessions.

On Windows (Permanent)
Open Control Panel → System → Advanced system settings → Environment Variables.

Under System Variables, click New.

Variable Name: PG_USER

Variable Value: postgres

Click New again.

Variable Name: PG_PASSWORD

Variable Value: your_password

Click OK and restart your computer.

Once restarted, Jupyter Notebook should be able to access the variables.

# Epoch {#epoch}


An epoch in machine learning is a single pass through the entire training dataset. The number of epochs, denoted as $N$, determines how many times the data is applied to the model.

Why Use Multiple Epochs?
- **Repetition for Learning:** The data is applied to the model $N$ times to improve learning and accuracy. For example, if $N = 10$, the model will see the entire dataset 10 times.

Example
```
Epoch 1/10
6250/6250 [<mark>=</mark><mark>=</mark><mark>=</mark><mark>=</mark><mark>=</mark><mark>=</mark>] - 6s 910us/step - loss: 0.1782
```

- **Epoch 1/10:** Indicates the model is currently on the first epoch out of a total of 10.
- **Batches:** For efficiency, the dataset is divided into smaller groups called 'batches'. In TensorFlow, the default batch size is 32. With 200,000 examples, this results in 6,250 batches.
- **Batch Execution:** The notation `6250/6250` shows the progress of batch execution within the current epoch.



# Epub {#epub}


An **EPUB** (short for *electronic publication*) file is a widely used **open eBook format** that is designed for **reflowable content**, meaning it can adapt its layout to fit various screen sizes—unlike PDFs, which preserve a fixed layout.

### Key Features of EPUB
- **Reflowable Text:** The content adjusts to screen size, font preferences, and orientation. This is ideal for smartphones, tablets, and e-readers like Kobo or Apple Books.
- **HTML + CSS Based:** Internally, an EPUB file is a compressed archive (`.zip`) that contains HTML files, images, stylesheets, metadata, and a manifest.
- **Navigation:** It supports **table of contents**, **internal links**, and **chapters** for easy navigation.
- **Supports Rich Media:** EPUB 3 can include audio, video, interactive elements, and MathML.

### How EPUB Shows “Pages”

EPUB doesn't have fixed "pages" like PDF. Instead:

- The **reading software** (like Apple Books, Calibre, or Kobo) dynamically **splits content into pages** based on screen size, font size, and user settings.
- Pages can vary in number depending on:
  - Device screen resolution
  - Font size or style
  - Margin settings

Because of this, you can't refer to a fixed page number universally across devices.
### EPUB vs PDF

| Feature                | EPUB                                 | PDF                                |
|------------------------|--------------------------------------|------------------------------------|
| Layout                 | Reflowable                           | Fixed                              |
| Usability on small screens | Excellent                         | Poor                               |
| Internal format        | HTML + CSS + XML                     | PostScript-based (binary)          |
| Navigation             | Flexible (TOC, links, metadata)      | Static (can have TOC, but fixed)   |


# Estimator {#estimator}

Given a sample an estimator is a formula that approximates a population parameter i.e feature

# Etl Pipeline Example {#etl-pipeline-example}


[Link](https://www.youtube.com/watch?v=uqRRjcsUGgk)

[Github](https://github.com/syalanuj/youtube/blob/main/de_fundamentals_python/etl.py

#### 1.  Extract using a API

Get data via api or download.

#### 2. Transform 

Put into a pandas df.

#### 3. Load

Save df as a database. Save SQLite database. Save as table.

Run functions


```python
"""
Python Extract Transform Load Example
"""

# %%
import requests
import pandas as pd
from sqlalchemy import create_engine

def extract()-> dict:
    """ This API extracts data from
    http://universities.hipolabs.com
    """
    API_URL = "http://universities.hipolabs.com/search?country=United+States"
    data = requests.get(API_URL).json()
    return data

def transform(data:dict) -> pd.DataFrame:
    """ Transforms the dataset into desired structure and filters"""
    df = pd.DataFrame(data)
    print(f"Total Number of universities from API {len(data)}")
    df = df[df["name"].str.contains("California")]
    print(f"Number of universities in california {len(df)}")
    df['domains'] = [','.join(map(str, l)) for l in df['domains']]
    df['web_pages'] = [','.join(map(str, l)) for l in df['web_pages']]
    df = df.reset_index(drop=True)
    return df["domains","country","web_pages","name"](#domainscountryweb_pagesname)

def load(df:pd.DataFrame)-> None:
    """ Loads data into a sqllite database"""
    disk_engine = create_engine('sqlite:///my_lite_store.db')
    df.to_sql('cal_uni', disk_engine, if_exists='replace')

# %%
data = extract()
df = transform(data)
load(df)


# %%
```

# Etl Vs Elt {#etl-vs-elt}


[ETL](ETL.md) (Extract, Transform, and Load) and [ELT](term/elt.md) (Extract, Load, and Transform) are two paradigms for moving data from one system to another.

<mark>ELT is most friendly for analysts</mark>

The main difference between them is that when an ETL approach is used, data is transformed before it is loaded into a destination system. On the other hand, in the case of ELT, any required transformations are done after the data has been written to the destination and are _then_ done _inside_ the destination -- often by executing SQL commands. The difference between these approaches is easier to understand by a visual comparison of the two approaches. 

The image below demonstrates the ETL approach to [data integration](term/data%20integration.md):

![](etl-tool.png)

While the following image demonstrates the ELT approach to data integration:

![](elt-tool.png)

[ETL](#etl) was originally used for [Data Warehousing](Data%20Warehouse.md) and ELT for creating a [Data Lake](Data%20Lake.md). 

## Disadvantages of ETL compared to ELT

**ETL** has several **disadvantages compared to ELT**, including the following:

- Generally, only transformed data is stored in the destination system, and so <mark>analysts must know beforehand every way</mark> they are going to use the data, and every report they are going to produce.  
- Modifications to requirements can be costly, and often require re-ingesting data from source systems.
- Every transformation that is performed on the data may obscure some of the underlying information, and analysts only see what was kept during the transformation phase. 
- Building an ETL-based data pipeline is often beyond the technical capabilities of analysts.

# Etl {#etl}


**ETL** (Extract, Transform, Load) is a data integration process that involves moving data from one system to another. It consists of three main stages:

1. **Extract**: Collecting data from various sources, such as databases, APIs, or flat files. This may involve setting up API connections to pull data from multiple sources.

2. **Transform**: Cleaning and converting the data into a usable format. This includes filtering, aggregating, and joining data to create a unified dataset. See [Data Transformation](#data-transformation).

3. **Load**: Inserting the transformed data into a destination system, such as a data warehouse (organized), database, or data lake (unorganized).

### Historical Context
The ETL paradigm emerged in the 1970s and was traditionally preferred for its ability to transform data before it reaches the destination. This approach ensures that data is standardized and passes quality checks, enhancing overall data quality.

### Transition to ELT
In recent years, the data movement paradigm has shifted towards [ELT](#elt) (Extract, Load, Transform). This approach emphasizes keeping raw data accessible in the destination system, allowing for more flexibility in data processing. For a comparison of ETL and ELT, see [ETL vs ELT](#etl-vs-elt).

Reasons for Change to see [ELT](#elt)

### Modern ETL Tools
Current ETL processes are often managed using tools like [Apache Airflow](#apache-airflow), [dagster](#dagster), and [Temporal](term/temporal.md).

### Enhancing the ETL Process
To improve an ETL process, consider the following enhancements:

- **Error Handling**: Implement error handling to manage exceptions and prevent silent failures.
- **Logging**: Include logging to track the process flow and facilitate debugging.
- **Parameterization**: Make scripts flexible by parameterizing file paths and database connections.
- **Data Validation and Cleaning**: Incorporate steps to validate and clean the data.
- **Database Indexing and Constraints**: Optimize database tables with proper indexing and constraints for better performance.
##### Related Notes
- [ETL Pipeline Example](#etl-pipeline-example)
- [ETL vs ELT](#etl-vs-elt)




[ETL](#etl)
   **Tags**: #data_transformation, #data_integration

# Etlt {#etlt}



>[!important]
> EtLT refers to Extract, “tweak”, Load, [Transform](Data%20Transformation.md), and can be thought of an extension to the [ELT](term/elt.md) approach to [data integration](term/data%20integration.md). 

When compared to ELT, the EtLT approach incorporates an additional light “tweak” (small “t”) transformation, which is done on the data after it is extracted from the source and before it is loaded into the destination.





# Evaluating Language Models {#evaluating-language-models}


The [LMSYS](#lmsys) Chatbot Arena is a platform where various large language models ([LLM](#llm)s), including versions of GPT and other prominent models like LLaMA or Claude, are compared through side-by-side interactions. The performance of these models is evaluated using human feedback based on the quality of their generated responses.

The arena employs several techniques to rank and compare models:

1. **[Elo Rating System](#elo-rating-system)**: Adapted from chess, this system rates models based on their relative performance in head-to-head competitions. When one model's response is preferred over another's, the winning model gains points while the losing model loses points. The rating difference helps determine the strength of models in future predictions. The system adjusts ratings gradually to avoid bias towards more recent results, ensuring stability over time.

2. [Bradley-Terry Model](#bradley-terry-model): This model goes beyond simple win-loss records by taking into account the <mark>difficulty of the task</mark> and the models' relative strengths. It helps fine-tune the ranking, especially when one model consistently performs better against tougher tasks.

In addition to these ranking systems, users can directly compare LLMs by giving them [Prompting](#prompting) to handle, such as writing articles, answering questions, or performing translations. Human voters then decide which model's output is better, or they can declare a tie if neither response is satisfactory.

These methods ensure continuous improvement of the rankings, providing a transparent and evolving leaderboard of the best generative models, including GPT versions

https://openlm.ai/chatbot-arena/

https://www.analyticsvidhya.com/blog/2024/05/from-gpt-4-to-llama-3-lmsys-chatbot-arena-ranks-top-llms/





# Evaluation Metrics {#evaluation-metrics}


## Description

[Confusion Matrix](#confusion-matrix)
[Accuracy](#accuracy)
[Precision](#precision)
[Recall](#recall)
[Precision or Recall](#precision-or-recall)
[F1 Score](#f1-score)
[Recall](#recall)
[Specificity](#specificity)


![Pasted image 20241222091831.png](../content/images/Pasted%20image%2020241222091831.png)

## Resources:
[Link to good website describing these](https://txt.cohere.com/classification-eval-metrics/)

In [ML_Tools](#ml_tools) see: [Evaluation_Metrics.py](#evaluation_metricspy)

## Types of predictions

Types of predictions in evaluating models. Also see [Why Type 1 and Type 2 matter](#why-type-1-and-type-2-matter)

**True Positive (TP)**:
- This occurs when the model correctly predicts the positive class. For example, if the model predicts that an email is spam and it actually is spam, that's a true positive.

**False Positive (FP)**:
- Also known as a <mark>"Type I error,"</mark> this occurs when the model incorrectly predicts the positive class. For example, if the model predicts that an email is spam but it is not, that's a false positive.

**True Negative (TN)**:
- This occurs when the model correctly predicts the negative class. For example, if the model predicts that an email is not spam and it actually is not spam, that's a true negative.

**False Negative (FN)**:
- Also known as a <mark>"Type II error,"</mark> this occurs when the model incorrectly predicts the negative class. For example, if the model predicts that an email is not spam but it actually is spam, that's a false negative.

## Evaluation metrics in practice

Having many evaluation metrics is hard to understand and optimise. Sometimes it is best to combine into one.

Use a single number i.e. accuracy or [F1 Score](#f1-score) . 

This speeds up development of ml projects.

In order to use metrics to evaluate a model we can:
- Can combine multiple metrics a formula, i.e. weighted average.
- If there is a metrics we are happy that the model passes a given level then we can have it "<mark>Satisfying</mark>". So the for the given metric it just needs to pass a given level.
- For metrics we are interested in we have it "<mark>Optimising</mark>", the one we want to be the best.
- Setup: Pick N-1 satisfying and 1 optimising.

![Pasted image 20241217073706.png](../content/images/Pasted%20image%2020241217073706.png)







# Event Driven Events {#event-driven-events}

Events can be stored in a [Data Lake](#data-lake) and analysed to find patterns/predictions.  

[Event Driven Microservices](#event-driven-microservices) allow for [Business observability](#business-observability)

[Monolith Architecture](#monolith-architecture)

[Event Driven Microservices](#event-driven-microservices)

[API Driven Microservices](#api-driven-microservices)

# Event Driven Microservices {#event-driven-microservices}


Event-driven microservices refer to a [software architecture](#software-architecture) pattern where [microservices](#microservices) communicate and coordinate their actions through the production, detection, consumption, and reaction to [Event Driven Events](#event-driven-events). This approach is designed to create a more decoupled and scalable system, where services can operate independently and react to changes in real-time.

Event-driven microservices are particularly useful for applications that require high scalability, real-time processing, and flexibility. 

They are commonly used in domains like e-commerce, IoT, financial services, and any system where real-time data processing and responsiveness are critical. 

However, they also introduce challenges in terms of event schema management, eventual consistency, and debugging, which need to be carefully addressed.

Key characteristics of event-driven microservices include:

1. **Event Producers and Consumers**: In this architecture, services can act as event producers, consumers, or both. An event producer generates events when something of interest happens (e.g., a new order is placed), and event consumers listen for these events to perform actions (e.g., processing the order).

2. **Asynchronous Communication**: Events are typically communicated asynchronously, meaning that the producer does not wait for the consumer to process the event. This allows services to operate independently and improves system responsiveness and scalability.

3. **Event Brokers**: An event broker or message broker (such as Apache Kafka, RabbitMQ, or AWS SNS/SQS) is often used to facilitate the distribution of events between services. The broker decouples producers from consumers, allowing them to evolve independently.

4. **Loose Coupling**: Services are loosely coupled because they do not need to know about each other directly. They only need to know about the events they produce or consume, which reduces dependencies and increases flexibility.

5. **Real-Time Processing**: Event-driven architectures are well-suited for real-time processing and can handle high volumes of events efficiently, making them ideal for applications that require immediate responses to changes.

6. **Scalability and Resilience**: The decoupled nature of event-driven microservices allows for independent scaling of services. If one service fails, it does not necessarily affect others, enhancing the system's resilience.

7. **Event Sourcing and CQRS**: Event-driven architectures often use patterns like event sourcing (storing the state of an entity as a sequence of events) and CQRS (Command Query Responsibility Segregation) to manage data consistency and separation of concerns.



# Event Driven {#event-driven}

Event-driven refers to a <mark>programming paradigm</mark> or architectural style where the flow of the program is determined by events—changes in state or conditions that trigger specific actions or responses. 

In this model, components of a system communicate through events, which can be generated by user interactions, system changes, or external sources.

### Key Concepts of Event-Driven Architecture:

1. Events: An event is a significant change in state or an occurrence that can trigger a response. For example, a user clicking a button, a file being uploaded, or a sensor detecting a change in temperature.

2. Event Producers: These are components or services that generate events. For instance, a web application might produce events when users perform actions like signing up or making a purchase.

3. Event Consumers: These are components or services that listen for and respond to events. They take action based on the events they receive, such as updating a database or sending a notification.

4. Event Channels: These are the pathways through which events are transmitted from producers to consumers. This can include message queues, event buses, or streaming platforms.

5. Loose Coupling: In an event-driven system, components are often loosely coupled, meaning that producers and consumers do not need to know about each other directly. This allows for greater flexibility and scalability.

Benefits of Event-Driven Architecture:
- [Scalability](#scalability)
- Responsiveness
- Flexibility

### Related topics

- [Event Driven Events](#event-driven-events)
- [Event-Driven Architecture](#event-driven-architecture)
- [Event Driven Microservices](#event-driven-microservices)
- **Tags**: #event_driven, #data_processing

# Event Driven Architecture {#event-driven-architecture}



# Everything {#everything}


Can we search with descriptions ? 

## Tips

use \ to match in paths i.e \playground 

can copy file 

new window crl+ n

Use | to get or search 

search syntax



# Excel & Sheets {#excel--sheets}


##### Links

[Google sheets example folder](https://drive.google.com/drive/folders/1F9GTIK-MARSjl6-BKb1AOID6EoRLe_zk?usp=drive_link)

see [standardised/GSheets|GSheets](#standardisedgsheetsgsheets)

Excel Example folder: Desktop/Example_Examples

## Tools common to Excel and Sheets

##### Vlookup

Table

| **Product ID** | **Product Name** | **Price** |
| -------------- | ---------------- | --------- |
| 1001           | Apple            | $1.00     |
| 1002           | Banana           | $0.50     |
| 1003           | Orange           | $0.80     |

$0.50=VLOOKUP(1002, Table, 3, FALSE)
##### Pivot table


## Excel specific

index-match

index-match-match
##### Excel: Evaluate Formula

**Evaluate Formula** is a feature in Excel that allows you to step through complex formulas to see how Excel calculates the result. This tool is helpful for debugging or understanding how nested formulas work.

**Example:**
Suppose you have a formula like this:
```excel
=IF(SUM(A1:A3) > 10, A4 * 2, A5 + 5)
```
To understand how Excel processes this formula, you can use **Evaluate Formula**. It will break down the formula into steps, showing how the `SUM(A1:A3)` is calculated, followed by the evaluation of the `IF` condition, and finally either the multiplication or addition operation based on the result.

**How to use it:**
1. Select the cell containing the formula.
2. Go to the "Formulas" tab on the ribbon.
3. Click on "Evaluate Formula".
4. Click "Evaluate" to step through each part of the formula.

##### Excel: What-If Analysis

**What-If Analysis** in Excel allows you to explore different scenarios by changing the inputs to your formulas. The three main tools within What-If Analysis are **Scenario Manager**, **Goal Seek**, and **Data Tables**.

- **Scenario Manager**: Lets you save different sets of input values and switch between them to see the impact on the result.
- **Goal Seek**: Finds the required input value to achieve a specific output.
- **Data Tables**: Shows how changing one or two variables affects your formulas.

**Example (Goal Seek):**
Imagine you have a loan payment formula:
```excel
=PMT(interest_rate, number_of_periods, loan_amount)
```
You want to find out what interest rate would result in a monthly payment of $500.

Steps:
1. Enter the formula with an initial guess for the interest rate.
2. Go to "Data" > "What-If Analysis" > "Goal Seek".
3. Set the cell with the formula to a value of 500.
4. Set the interest rate cell as the one to change.
5. Click "OK" and Excel will adjust the interest rate to meet the target.

##### Excel: Forecast Sheets

**Forecast Sheets** use historical data to predict future values. Excel creates a forecast chart based on the pattern in the data, using linear or exponential smoothing models. This is particularly useful for time series data, such as sales or financial data over time.

**Example:**
Suppose you have monthly sales data in a column:

| Month  | Sales |
|--------|-------|
| Jan    | 1000  |
| Feb    | 1100  |
| Mar    | 1200  |
| Apr    | 1300  |
You want to forecast future sales for the next 6 months.

Steps:
1. Select the range of historical data (both months and sales).
2. Go to the "Data" tab on the ribbon.
3. Click "Forecast Sheet".
4. Excel will automatically create a forecast for the future months based on the trend in the data.
5. You can adjust the forecast options (e.g., forecast length, confidence intervals) before creating the sheet.
##### Excel: Consolidate

In Excel, the **Consolidate** feature (found under the **Data** tab) allows you to combine data from multiple ranges or worksheets into a single summary. It is particularly useful when you have data spread across different locations and want to summarize it, such as calculating totals, averages, or other aggregate functions.

Key Features of Consolidate:
- **Multiple Ranges**: You can consolidate data from different ranges, even if they are in separate worksheets or workbooks.
- **Functions**: It provides several functions such as SUM, AVERAGE, COUNT, MIN, MAX, and more to aggregate the data.
- **Labels**: You can use row and column labels to match and consolidate the data correctly.

How to Use Consolidate:
1. **Prepare your data**: Ensure that your data is organized in a table format with similar structures (e.g., same columns and rows across different sheets or ranges).
2. **Navigate to Consolidate**: Go to the **Data** tab and click on **Consolidate** in the **Data Tools** group.
3. **Select Function**: In the Consolidate dialog box, select the function you want to use (e.g., **Sum**, **Average**, etc.).
4. **Add References**: Add the ranges you want to consolidate by clicking on **Add** after selecting the range. You can select ranges from different worksheets or workbooks.
5. **Use Labels**: If your data contains row or column labels, check the boxes for "Use labels in top row" or "Use labels in left column" to consolidate the data correctly based on these labels.
6. **Create links**: If you want the consolidated data to update automatically when the source data changes, check the box for **Create links to source data**.

Benefits:
- Saves time by avoiding manual data entry or copy-pasting from multiple sheets.
- Helps in summarizing large amounts of data quickly.
- Ensures accuracy in consolidation, especially with functions like **Sum** or **Average**.

Example:
Suppose you have sales data in two worksheets as follows:

**Sheet1 (Region A)**:

| Product | Sales |
|---------|-------|
| A       | 100   |
| B       | 200   |
| C       | 300   |

**Sheet2 (Region B)**:

| Product | Sales |
|---------|-------|
| A       | 150   |
| B       | 250   |
| C       | 350   |
You can use the **Consolidate** feature to combine the sales from both regions into a summary table:

**Consolidated Sheet**:

| Product | Sales |
|---------|-------|
| A       | 250   |
| B       | 450   |
| C       | 650   |

In this case, you would use the **Sum** function in the Consolidate dialog to add the sales from the two regions.
##### Excel: Text to Columns

The **Text to Columns** feature in Excel is used to split the data in one column into multiple columns, based on a delimiter (like commas, spaces, or tabs) or a fixed width. This is particularly helpful when you have data combined in a single column and you need to separate it into distinct parts.

Benefits:
- Efficiently splits combined data without manual editing.
- Helps with data cleaning when importing text-based data from sources like CSV files.
- Reduces errors by automating the process of separating values.

Key Features of Text to Columns:
- **Delimiters**: You can split text based on specific delimiters such as commas, spaces, semicolons, or custom characters.
- **Fixed Width**: If the data is in a consistent format, you can split the text based on fixed-width segments.

How to Use Text to Columns:
1. **Select the Data**: Highlight the column or cells that contain the text you want to split.
2. **Navigate to Text to Columns**: Go to the **Data** tab on the ribbon, then click **Text to Columns** in the **Data Tools** group.
3. **Choose the Split Type**:
   - **Delimited**: Choose this option if your data is separated by characters like commas, tabs, or spaces.
   - **Fixed Width**: Choose this if the data is aligned into specific columns with consistent spacing.
4. **Select Delimiters or Set Width**: 
   - For **Delimited**, choose the character that separates your data (e.g., comma, space, semicolon).
   - For **Fixed Width**, manually set where the splits should occur by clicking in the preview window.
5. **Select Destination**: Choose where you want the split data to appear (by default, it will overwrite the original data).
6. **Finish**: Click **Finish** to apply the split.

Example 1: Delimited (Comma-Separated Values)
Imagine you have a list of full names in one column:

| Full Name        |
|------------------|
| John,Smith       |
| Jane,Doe         |
| Robert,Johnson   |

You want to split these names into two separate columns: First Name and Last Name. Here's how you would use **Text to Columns**:
1. Select the column with the full names.
2. Go to **Data** > **Text to Columns**.
3. Choose **Delimited**, then select **Comma** as the delimiter.
4. Click **Finish**. The data will be split into two columns:

| First Name | Last Name |
|------------|-----------|
| John       | Smith     |
| Jane       | Doe       |
| Robert     | Johnson   |

Example 2: Fixed Width
Suppose you have a column with product codes where each section of the code has a fixed length:

| Product Code   |
|----------------|
| 12345ABC67890  |
| 67890DEF12345  |

If the first 5 characters are the product number, the next 3 characters are the product category, and the last 5 characters are the batch number, you can split them based on fixed widths:
1. Select the column with the product codes.
2. Go to **Data** > **Text to Columns**.
3. Choose **Fixed Width**.
4. In the preview window, set the breaks where you want to split the text (after the 5th and 8th characters).
5. Click **Finish**. The data will be split into separate columns like this:

| Product Number | Category | Batch Number |
|----------------|----------|--------------|
| 12345          | ABC      | 67890        |
| 67890          | DEF      | 12345        |

##### Excel: [Data Validation](#data-validation)
- **Restrict Data Types**: You can limit entries to specific data types, such as whole numbers, decimal numbers, dates, or times.
- **Set Input Rules**: You can set conditions (e.g., numbers between 1 and 100, dates in a certain range, or specific text lengths).
- **Create Drop-Down Lists**: You can provide users with a predefined list of options to select from.
- **Custom Validation**: You can use formulas for advanced validation rules.
- **Error Alerts**: You can display custom messages to users when they try to enter invalid data.

How to Use Data Validation:

1. **Select the Cell(s)**: Select the range of cells where you want to apply data validation.
2. **Go to Data Validation**: Navigate to the **Data** tab on the ribbon, and in the **Data Tools** group, click **Data Validation**.
3. **Set Validation Criteria**:
   - In the **Settings** tab, choose the type of data you want to allow (Whole Number, Decimal, List, Date, Time, Text Length, or Custom).
   - Specify the condition (e.g., a range of values or a list of items).
4. **Input Message (Optional)**: In the **Input Message** tab, you can set a message that will appear when the user selects the cell, providing guidance on what they should enter.
5. **Error Alert**: In the **Error Alert** tab, define what happens if invalid data is entered. You can show an error message and choose whether to stop the entry, give a warning, or provide information.

**Restricting Input to a Range of Numbers**
You want to restrict the values in a certain column to only allow whole numbers between 10 and 100.
   
Steps:
1. Select the column or cells where you want to apply the rule.
2. Go to **Data** > **Data Validation**.
3. In the **Settings** tab, choose **Whole Number** from the **Allow** drop-down.
4. Set the **Minimum** to 10 and the **Maximum** to 100.
5. Optionally, create an input message or error alert to inform the user of the valid range.

**Creating a Drop-Down List**
You want users to select from a list of predefined options, such as product categories (e.g., "Electronics", "Furniture", "Clothing").
Steps:
1. Select the cells where the drop-down list should appear.
2. Go to **Data** > **Data Validation**.
3. In the **Settings** tab, choose **List** from the **Allow** drop-down.
4. In the **Source** field, type the list items, separated by commas: `Electronics,Furniture,Clothing`.
5. Click **OK**. Now users will see a drop-down arrow in the cells, allowing them to choose from the options.

**Validating Dates**
You want to ensure that users can only enter dates within a specific range, such as between January 1, 2023, and December 31, 2023.
Steps:
1. Select the cells where the dates will be entered.
2. Go to **Data** > **Data Validation**.
3. In the **Settings** tab, choose **Date** from the **Allow** drop-down.
4. Set the **Start Date** to 1/1/2023 and the **End Date** to 12/31/2023.
5. Click **OK**. Now users will only be able to enter dates within the specified range.

**Custom Validation with Formulas**
You want to ensure that users can only enter text starting with the letter "A".
Steps:
1. Select the cells where the validation should apply.
2. Go to **Data** > **Data Validation**.
3. In the **Settings** tab, choose **Custom** from the **Allow** drop-down.
4. In the **Formula** field, enter:
   
```excel
   =LEFT(A1, 1)="A"
   
```
5. Click **OK**. Now users will only be able to enter text starting with the letter "A".


## Google specific

Xlookup

# Explain Different Gradient Descent Algorithms, Their Advantages, And Limitations. {#explain-different-gradient-descent-algorithms-their-advantages-and-limitations}





# Explain The Curse Of Dimensionality {#explain-the-curse-of-dimensionality}


The **curse of dimensionality** refers to the various phenomena that arise when working with data in high-dimensional spaces.

- **Increased Data <mark>Sparsity</mark>:** As the number of dimensions grows, the available data becomes increasingly sparse, making it difficult for algorithms to find <mark>meaningful patterns</mark>. This sparsity can lead to poor generalization performance, as the algorithm might not have enough data points in each region of the input space to learn a robust model.

- <mark>**Distance Metric Issues:**</mark> In high-dimensional spaces, traditional distance metrics like Euclidean distance become less effective, as the relative difference between the nearest and farthest points diminishes. This can make it difficult for algorithms like k-nearest neighbours to identify meaningful neighbours.

- **Difficulty in Visualization:** Visualizing data beyond three dimensions becomes incredibly challenging, making it difficult to gain insights from the data and understand the behaviour of machine learning models.

### Examples of the Curse of Dimensionality

**Vulnerability of [Ngrams](#ngrams) [Language Models](#language-models):** 

Classical n-gram language models in [NLP](#nlp), which rely on counting the occurrences of word sequences, are particularly vulnerable to the curse of dimensionality. As the vocabulary size and the value of 'n' increase, the number of possible n-grams grows exponentially, making it impossible to observe most of them in even a massive training set.
### Addressing the Curse of Dimensionality

While the curse of dimensionality presents significant challenges, there are techniques to mitigate its effects:

- **[Dimensionality Reduction](#dimensionality-reduction):** Techniques like Principal Components Analysis (PCA), Factor Analysis, and [Multidimensional Scaling](#multidimensional-scaling) (MDS) can reduce the number of features while retaining essential information, making it easier to visualize and analyze data and train machine learning models.

- **[Feature Selection](#feature-selection):** Identifying and selecting the most relevant features for a given task can improve model performance and reduce computational complexity.

- **Distributed Representations:** Using distributed representations, where information is encoded across multiple features rather than a single one, can help overcome the limitations of one-hot encodings in high-dimensional spaces.

- **[Regularisation](#regularisation):** Techniques like weight decay in neural networks can help prevent overfitting and improve generalization performance, particularly in high-dimensional settings

- **[Manifold Learning](#manifold-learning):** Manifold learning methods assume that the data lies on a lower-dimensional manifold embedded in a high-dimensional space. By learning this manifold structure, these methods can reduce dimensionality while preserving nonlinear relationships in the data.




# Exploration Vs. Exploitation {#exploration-vs-exploitation}

One of the major challenges in [Reinforcement learning](#reinforcement-learning) is balancing exploration (trying new actions) and exploitation (choosing the best-known actions). 

The <mark>epsilon-greedy strategy</mark> is commonly used, where a small probability (epsilon) allows for exploration while primarily exploiting the best-known actions.

# Exploration {#exploration}









# Fabric {#fabric}


Fabric is a unified analytics platform that operates in the cloud, eliminating the need for local installations. It provides an integrated environment for data analysis, similar to how [Microsoft](#microsoft) Office serves as a suite for productivity tasks.

## Key Features

- Unified Platform: Combines various data tools and services into a single platform, streamlining data analysis and management.
- Cloud-Based: Operates entirely in the cloud, ensuring accessibility and scalability without the need for local installations.
- Integrated Environment: Offers a cohesive environment for data analysis, integrating tools like [Data Factory](#data-factory), [Synapse](#synapse), and [PowerBI](#powerbi).

## Components

- Data Factory: Facilitates data integration and transformation.
- Synapse: Acts as a [Relational Database](#relational-database) and [Data Warehouse](#data-warehouse), supporting [T-SQL](#t-sql) for data management.
- [Data Lake](#data-lake): Fabric provides open data storage solutions, allowing for efficient data management and analysis.
- OneLake: Offers workspaces for different departments, enabling data sharing and referencing without duplication.

## Technologies

- Programming Languages: Supports [Scala](#scala) and [PySpark](#pyspark) for data processing within the [Data Lake](#data-lake).
- PowerBI Integration: Enhances data visualization and querying capabilities within Fabric, offering faster insights.

## Advantages

- Data as Fuel: Recognizes data as the essential component powering AI and analytics.
- No ETL Required: Mirrors data sources, eliminating the need for [ETL](#etl) processes when source data is edited.
- Cross-Workspace Shortcuts: Allows departments to reference data across workspaces without creating copies.
- Copilot with PowerBI: Integrates AI-driven insights and automation within PowerBI for enhanced data analysis.

# Fact Table {#fact-table}

A fact table is a central component of a star [Database Schema|schema](#database-schemaschema) or snowflake schema in a [data warehouse](#data-warehouse), it stores [Facts](#facts).

Essential for [data analysis](#data-analysis) in a data warehouse, providing the numerical data that can be analyzed in conjunction with the descriptive data stored in dimension tables.

1. **Measures**: Fact tables contain measurable, quantitative data known as "facts" or "measures." Examples include sales revenue, quantity sold, profit, or any other numeric data that can be aggregated.

2. **Foreign Keys**: Fact tables include foreign keys that reference [Dimension Table](#dimension-table)s. These keys link the fact table to related dimensions, allowing for contextual analysis. For example, a sales fact table might include foreign keys for dimensions such as time, product, and customer.

3. **Granularity**: The [granularity](#granularity) of a fact table refers to the level of detail stored in the table. For example, a sales fact table might store data at the transaction level (each sale) or at a higher level (daily sales totals).

4. **Large Size**: Fact tables can become quite large, as they often store a significant amount of transactional data over time. This is in contrast to dimension tables, which are usually smaller and contain descriptive attributes.

5. **Aggregation**: Fact tables are often used for aggregation and analysis, allowing users to perform operations such as summing, averaging, or counting the measures.

Example:
  - Columns: `DateKey`, `ProductKey`, `RegionKey`, `SalesAmount`, `UnitsSold`



[Fact Table](#fact-table)
   **Tags**: #data_modeling, #data_warehouse

# Factor Analysis {#factor-analysis}


Factor Analysis (FA) is a statistical method used for:
- [dimensionality reduction](#dimensionality-reduction),
- [EDA](#eda)
- or latent variable detection

It identifies underlying relationships between observed variables by modeling them as linear combinations of a smaller number of <mark>unobserved latent factors</mark>.

In simpler terms, it helps reduce a large number of variables into fewer factors while retaining the core information and structure of the data. It assumes that observed variables are influenced by some common latent factors and unique errors.

### Key Features of Factor Analysis:

1. Latent Factors: These are unobserved variables that capture the shared variance among observed variables.
2. Variance Decomposition: FA splits the total variance of observed variables into:
    - Common variance: Shared by latent factors.
    - Unique variance: Specific to each observed variable.

In [ML_Tools](#ml_tools) see: [Factor_Analysis.py](#factor_analysispy)

### Next Steps:

1. Would you like to visualize the factors to understand how the data clusters in the new latent space?
2. Should we explore the relationships between the factors and target classes (e.g., species in the Iris dataset)?

# Factor_Analysis.Py {#factor_analysispy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Preprocess/Factor_Analysis.py
### **1. Factor Loadings Table**

This table shows how strongly each feature (e.g., `sepal length (cm)`) is correlated with the two extracted factors (Factor 1 and Factor 2).

- **Rows**: Represent the extracted factors (`Factor 1` and `Factor 2`).
- **Columns**: Represent the original features of the Iris dataset.
- **Values**: Represent the "loading" or contribution of each feature to the factor. Higher absolute values indicate a stronger relationship between a feature and a factor.

#### Interpretation:

- **Factor 1 (Row 0)**:
    - Strongly influenced by `petal length (cm)` (loading = 1.757902) and `petal width (cm)` (loading = 0.731005).
    - Moderately influenced by `sepal length (cm)` (loading = 0.727461).
    - Weak negative contribution from `sepal width (cm)` (loading = -0.180852).
    - This factor might represent the size of petals and sepals.
      
- **Factor 2 (Row 1)**:
    - Weak contributions from all features, with slightly negative contributions from `sepal length (cm)` and `sepal width (cm)`.
    - This factor might capture a subtle relationship or orthogonal variance not well-defined by the dataset.

---

### **2. Explained Variance**

The explained variance values indicate how much of the dataset's total variance is captured by each factor.
- **Factor 1 (0.9988)**: This factor explains ~99.88% of the variance among the features.
- **Factor 2 (0.9039)**: This factor explains ~90.39% of the variance among the features.

#### Combined Variance:
The two factors together capture a large portion of the total variance in the dataset. This suggests that most of the information in the dataset can be reduced to two latent factors, simplifying its structure while retaining the core relationships.

### **Overall Interpretation**

1. **Dimensionality Reduction**: The dataset with four features can effectively be reduced to two latent factors while retaining most of its variance.
2. **Factor 1 Dominates**: Factor 1 has strong contributions from `petal length`, `petal width`, and `sepal length`. This factor likely represents size-related characteristics.
3. **Factor 2 is Subtle**: Factor 2 shows weaker relationships with the features, potentially capturing noise or orthogonal variance.

### Next Steps:

1. Would you like to visualize the factors to understand how the data clusters in the new latent space?
2. Should we explore the relationships between the factors and target classes (e.g., species in the Iris dataset)?

### Breakdown of Extensions:

1. **Visualization**:  
    After performing factor analysis, we visualize how the data clusters in the new latent space (Factor 1 vs. Factor 2). The points are colored based on the species (target classes), which helps us see if the factors capture any clustering patterns related to species.
    
    - **Plot Details**:
        - The x-axis represents **Factor 1**.
        - The y-axis represents **Factor 2**.
        - Each species is plotted in different colors to visualize possible separations.
2. **Exploring Factor-Target Relationships**:  
    We compute the **average factor values** for each species. This shows how the latent factors (Factor 1 and Factor 2) relate to the different species in the dataset.
    
    - **Interpretation**:
        - If any species tends to cluster around specific values of Factor 1 and Factor 2, it suggests that the extracted factors capture some species-specific variance.

### Next Steps:

- The plot should give a clear idea of whether the latent factors allow for a meaningful separation of species.
- The summary table of average factor values will help understand how the factors relate to the target variable.

# Facts {#facts}

Facts are quantitative data points that are typically stored in the [Fact Table](#fact-table).

They represent measurable events or metrics, such as sales revenue or quantities sold.

# Faiss {#faiss}


FAISS (Facebook AI [Similarity Search](#similarity-search)) is a library developed by Facebook AI Research that enables efficient similarity search and [clustering](#clustering) of dense vectors. It is especially well-suited for applications involving high-dimensional vector data, such as [NLP](#nlp)

Related terms:
- [Vector Embedding](#vector-embedding)
### Overview

FAISS is optimized for:
- **Fast retrieval** from large collections of vectors (millions or more).
- **Approximate nearest neighbor (ANN)** search, which trades off accuracy for speed.
- **Exact search**, depending on the chosen index type.
- **GPU acceleration** for very large-scale search tasks.

### Core Concept

At its core, FAISS takes a large number of **high-dimensional vectors** (e.g., sentence or document embeddings), and enables fast **similarity search** to retrieve the most similar vectors to a given [Querying|query](#queryingquery).

For example, in an NLP [Memory|context](#memorycontext):
- Documents or notes are embedded into vector space using a model like SBERT.
- These embeddings are stored in a FAISS index.
- Given a query, its embedding is computed, and FAISS returns the nearest neighbors (i.e., most semantically similar notes).
### Index Types

FAISS offers different types of indices depending on use case:
- `IndexFlatL2`: exact search using L2 (Euclidean) distance.
- `IndexIVFFlat`: approximate search using inverted files.
- `IndexHNSW`: Hierarchical Navigable Small World graph-based index (good for high recall).
- `IndexPQ`: product quantization for memory-efficient indexing.




# Fastapi {#fastapi}

**FastAPI** is a modern web framework for building APIs with Python. It is designed to be fast and easy to use, leveraging Python's type hints to provide features like:

1. Automatic generation of OpenAPI documentation.
2. Input data validation based on Python's type annotations.
3. Asynchronous request handling with native support for `asyncio`.
4. High performance, as it is built on Starlette and [Pydantic](#pydantic).
### Key Features

- **Automatic validation:** Based on type hints and Pydantic models.
- **Interactive API docs:** Automatically generated Swagger UI and ReDoc.
- **Asynchronous support:** Full support for async functions.
- **Dependency injection:** Built-in support for dependencies.

### How to Run

<mark>In [ML_Tools](#ml_tools) see: [FastAPI_Example.py](#fastapi_examplepy)</mark> <- see this

1. Save the script as `main.py`.
2. Install FastAPI and Uvicorn:
    `pip install fastapi uvicorn`
    
3. Run the server:  
	
```cmd
	ren FastAPI_Example.py main.py # possibly change to
	uvicorn FastAPI_Example:app --reload
	
```
    
4. Open the browser and navigate to:
    - **API documentation (Swagger UI):** `http://127.0.0.1:8000/docs`
    - **ReDoc documentation:** `http://127.0.0.1:8000/redoc`

# Fastapi_Example.Py {#fastapi_examplepy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Deployment/FastAPI_Example.py
### Explanation of New Features

1. **Path and Query Parameter Metadata**: Added descriptions and constraints for better validation and autogenerated documentation.
2. **Nested Models**: Demonstrated hierarchical data validation with the `User` model that includes a list of `Item` instances.
3. **Partial Updates**: Introduced a `PATCH` endpoint to allow partial updates of fields using the `Body` method.
4. **Static Data**: Provided a status endpoint that returns static information about the API.
5. **Returning Key-Value Data**: Added a summary endpoint to showcase mock data.

# Main

### **What the Script Does**

1. **Starts the FastAPI Application**  
    When you run the script, it starts a web server using Uvicorn. This makes your FastAPI app accessible via the browser or API clients like Postman.
    
2. **Default Endpoint (`GET /`)**  
    The browser sends a `GET` request to the root path (`http://127.0.0.1:8000/`).  
    The root route (`@app.get("/")`) is defined in the script to return:
    
    
```python
    {"message": "Welcome to the expanded FastAPI example!"}
    
```
    
    This is why you see that message in the browser or the API response.
    
3. **404 for `/favicon.ico`**  
    The browser automatically tries to load a favicon (a small icon displayed in the browser tab). Since no favicon is defined in the script, a `404 Not Found` is returned, which is normal behavior.
    

---

### **What the Script Offers Beyond the Root Endpoint**

The script defines several API endpoints that are not automatically accessed unless you explicitly call them. Here’s a summary of the key routes:

|**Endpoint**|**HTTP Method**|**Description**|
|---|---|---|
|`/`|`GET`|Returns a welcome message.|
|`/items/{item_id}`|`GET`|Fetches an item by its `item_id`, with an optional query parameter.|
|`/search/`|`GET`|Searches items using `limit` and `offset` query parameters for pagination.|
|`/items/`|`POST`|Creates a new item using the `Item` model for validation.|
|`/items/{item_id}`|`PUT`|Updates an item by its `item_id` with a new `Item` object.|
|`/items/{item_id}`|`DELETE`|Deletes an item by its `item_id`.|
|`/users/`|`POST`|Creates a new user with optional nested items using the `User` model.|
|`/items/{item_id}`|`PATCH`|Partially updates fields (like `price` or `on_offer`) of an item by its ID.|
|`/status/`|`GET`|Returns static data, such as the API's status and version.|
|`/summary/`|`GET`|Returns a dictionary with some mock data, such as total items and users.|

---

### **Testing the Script**

You need to explicitly visit or call other endpoints to explore more features of the script. For example:

1. <mark>**Accessing the Swagger UI** Go to [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) in your browser.</mark>  
    This interactive interface shows all the available endpoints and lets you test them directly.
    
2. **Calling Specific Endpoints** You can call the endpoints via:
    - **Browser** (e.g., `http://127.0.0.1:8000/items/123?q=test`).
    - **API Clients** like Postman or Curl.

---

### **Why You See Only the Welcome Message**

You’ve accessed only the root endpoint (`/`). The other features of the script (like creating, updating, or searching for items) require you to call the respective endpoints explicitly.

# Calling endpoints

Here's how to call the respective endpoints of the script using various tools like **browser**, **Curl**, or **Postman**. Each example demonstrates an endpoint and its purpose.
### **1. Welcome Endpoint (`GET /`)**

- **Purpose:** Displays a welcome message.
- **Call:**
    - **Browser:** Open `http://127.0.0.1:8000/`.
    - **Curl:**
        
```bash
        curl -X GET http://127.0.0.1:8000/
        
```
    - **Response:**
        
```json
        {"message": "Welcome to the expanded FastAPI example!"}
        
```
### **2. Retrieve an Item (`GET /items/{item_id}`)**

- **Purpose:** Fetches item details by its `item_id` with an optional query parameter `q`.
- **Example:** Retrieve item `3` with query `test`.
- **Call:**
    - **Browser:** Open `http://127.0.0.1:8000/items/3?q=test`.
    - **Curl:**
        
        
```bash
        curl -X GET "http://127.0.0.1:8000/items/3?q=test"
        
```
    - **Response:**
        
```json
        {"item_id": 3, "query": "test"}
        
```

---

### **3. Search Items (`GET /search/`)**

- **Purpose:** Uses `limit` and `offset` query parameters for pagination.
- **Example:** Limit results to 5, skip the first 2.
- **Call:**
    - **Browser:** Open `http://127.0.0.1:8000/search/?limit=5&offset=2`.
    - **Curl:**
        
```bash
        curl -X GET "http://127.0.0.1:8000/search/?limit=5&offset=2"
        
```     
    - **Response:**
        
```json
        {"limit": 5, "offset": 2}
        
```
### **4. Create an Item (`POST /items/`)**

- **Purpose:** Adds a new item.
- **Example:** Create an item named "Laptop" priced at 999.99.
- **Call:**
    - **Curl:**
        
```bash
curl -X POST "http://127.0.0.1:8000/items/" -H "Content-Type: application/json" -d "{\"name\": \"Laptop\", \"price\": 999.99, \"description\": \"High-end laptop\", \"on_offer\": true}"
        
```
    - **Response:**
        
```json
        {
          "message": "Item created successfully",
          "item": {
            "name": "Laptop",
            "price": 999.99,
            "description": "High-end laptop",
            "on_offer": true
          }
        }
        
```
### **5. Update an Item (`PUT /items/{item_id}`)**

- **Purpose:** Updates item details by its `item_id`.
- **Example:** Update item `3` with new data.
- **Call:**
    - **Curl:**
        
        
```bash
        curl -X PUT "http://127.0.0.1:8000/items/3" \
        -H "Content-Type: application/json" \
        -d '{"name": "Smartphone", "price": 499.99, "description": "Updated phone", "on_offer": false}'
        
```

    - **Response:**
        
        
```json
        {
          "item_id": 3,
          "updated_item": {
            "name": "Smartphone",
            "price": 499.99,
            "description": "Updated phone",
            "on_offer": false
          }
        }
        
```

### **6. Delete an Item (`DELETE /items/{item_id}`)**

- **Purpose:** Deletes an item by its `item_id`.
- **Example:** Delete item `2`.
- **Call:**
    - **Curl:**
        
        
```bash
        curl -X DELETE http://127.0.0.1:8000/items/2
        
```
        
    - **Response:**
        
        
```json
        {"message": "Item 2 deleted successfully"}
        
```
        

---

### **7. Create a User (`POST /users/`)**

- **Purpose:** Creates a user, optionally with nested items.
- **Example:** Create a user with items.
- **Call:**
    - **Curl:**
        
```bash
		curl -X POST "http://127.0.0.1:8000/users/" -H "Content-Type: application/json" -d "{\"username\": \"john_doe\", \"email\": \"john@example.com\", \"full_name\": \"John Doe\", \"items\": [{\"name\": \"Tablet\", \"price\": 299.99, \"description\": \"Portable tablet\"}]}"
        
```
        
    - **Response:**
        
        
```json
        {
          "message": "User created successfully",
          "user": {
            "username": "john_doe",
            "email": "john@example.com",
            "full_name": "John Doe",
            "items": [
              {
                "name": "Tablet",
                "price": 299.99,
                "description": "Portable tablet",
                "on_offer": null
              }
            ]
          }
        }
        
```
### **8. Partially Update an Item (`PATCH /items/{item_id}`)**

- **Purpose:** Partially updates an item using optional fields.
- **Example:** Update the price of item `5`.
- **Call:**
    - **Curl:**
        
        
```bash
        curl -X PATCH "http://127.0.0.1:8000/items/5" \
        -H "Content-Type: application/json" \
        -d '{"price": 79.99}'
        
```
        
    - **Response:**
        
        
```json
        {
          "item_id": 5,
          "updates": {
            "price": 79.99
          }
        }
        
```
### **9. Check API Status (`GET /status/`)**

- **Purpose:** Returns static information like API status.
- **Call:**
    - **Browser:** Open `http://127.0.0.1:8000/status/`.
    - **Curl:**
        
        
```bash
        curl -X GET http://127.0.0.1:8000/status/
        
```
        
    - **Response:**
        
        
```json
        {"status": "API is running", "version": "1.0.0"}
        
```
        

### **10. Retrieve a Summary (`GET /summary/`)**

- **Purpose:** Returns a mock summary of items and users.
- **Call:**
    - **Browser:** Open `http://127.0.0.1:8000/summary/`.
    - **Curl:**
        
        
```bash
        curl -X GET http://127.0.0.1:8000/summary/
        
```
        
    - **Response:**
        
        
```json
        {"total_items": 42, "total_users": 5, "recent_activity": "Item purchase"}
        
```
        

---

### **Testing Tips**

- Use **Swagger UI** at `http://127.0.0.1:8000/docs` for interactive testing of all endpoints.
- Use **Postman** for testing more complex requests with nested data.

# New 

### **Explanation of Changes**:

- **`created_items`**: Items that have been created using the `/items/` POST endpoint.
- **`updated_items`**: Items that have been updated using the `/items/{item_id}` PUT endpoint.
- **`deleted_items`**: Items that have been deleted using the `/items/{item_id}` DELETE endpoint.
- **Summary Endpoint**:
    - Returns counts of items created (`created_items_count`), updated (`updated_items_count`), and deleted (`deleted_items_count`).

---
Here's the formatted content for use in `cmd`:

<mark>TEST LATER</mark>
### **Testing with `curl`**:

1. **Create an item**:
    
    
```bash
    curl -X POST "http://127.0.0.1:8000/items/" -H "Content-Type: application/json" -d '{"name": "Tablet", "price": 299.99, "description": "Portable tablet"}'
    
```
    
2. **Update an item**:
    
    
```bash
    curl -X PUT "http://127.0.0.1:8000/items/1" \
    -H "Content-Type: application/json" \
    -d '{"name": "Tablet", "price": 250.00, "description": "Updated portable tablet"}'
    
```
    
3. **Delete an item**:
    
    
```bash
    curl -X DELETE "http://127.0.0.1:8000/items/1"
    
```
    
4. **Get the summary**:
    
    
```bash
    curl -X GET "http://127.0.0.1:8000/summary/"
    
```
    

### **Expected Response**:

```json
{
  "total_items": 0,
  "total_users": 5,
  "recent_activity": "Item purchase",
  "created_items_count": 1,
  "updated_items_count": 1,
  "deleted_items_count": 1
}
```

You can copy and paste these commands directly into `cmd` to test the API.
### **Note**:

Make sure your FastAPI server is running on `http://127.0.0.1:8000` before executing these commands.

# Feature Engineering {#feature-engineering}


Its the term given to the iterative process of building good features for a better model. Its the process that makes relevant features (using formulas and relations between others). 

We use it when we have a refined and optimised model.

What does it involve
- Create new features from existing ones (e.g., ratios, interactions).
- Transform features to better capture non-linear relationships.
- [Dimensionality Reduction](#dimensionality-reduction) if necessary.

The main techniques of feature engineering:
- are selection (picking subset), 
- learning (picking the best), 
- extraction and combination(combining).

Example:
Predicting house prices. Raw features might be square footage, number of bedrooms, and location. Feature engineering could involve: Combining square footage and bedrooms into a "living space" feature.

**Example**:
- Decompose datetime information into separate features for date and time to capture their respective predictive powers.


![C1_W2_Lab07_FeatureEngLecture.png](../content/images/C1_W2_Lab07_FeatureEngLecture.png)

# Feature Evaluation {#feature-evaluation}



# Note

Garbage in garbae out. It is the features that 
# What is involved:

Want to assess the  **usefulness** of chosen features
### **Measuring feature importance:** 

Quantifying the contribution of each feature to the model's predictions. This can be done through various methods like statistical tests, model-specific importance scores, or permutation importance.
### **Analysing feature relationships:** 

Investigating correlations and redundancy,

### **Assessing feature impact on model performance:** 


# Example:



# When are we done:

- **Reaching a stable and satisfactory model performance
- **No further room for improvement
- **Understanding the model's decision-making process



# Feature Extraction {#feature-extraction}


### Summary:

In machine learning, **Feature extraction** is the process of transforming raw data into a set of useful features that can be effectively used by algorithms. It involves identifying and selecting key attributes or characteristics of the data that are most relevant to the problem at hand. Feature extraction helps improve both the performance and efficiency of machine learning models.

Feature extraction simplifies complex data by transforming it into a smaller set of <mark>informative features.</mark> Techniques like [Dimensionality Reduction](#dimensionality-reduction) help retain the most significant information, improving model performance and [interpretability](#interpretability). It is conceptually similar to the way the [Attention mechanism](#attention-mechanism) in [LLM](#llm)s captures relationships between concepts, or how [Activation atlases](#activation-atlases) visualize key patterns in deep learning models.

### Key Concepts of Feature Extraction:

1. **Similarity and Relations**:
   - Feature extraction can be compared to how the [Attention mechanism|Attention mechanism](#attention-mechanismattention-mechanism) works in large language models (LLMs), which identify relationships and similarities between concepts. For example, the analogy "King - Queen ~ Man - Woman" highlights how certain features (gender, royalty) can be extracted to understand the underlying relationship between words.
   
   Similarly, in feature extraction, relationships between data points can be used to capture important aspects of the data, such as patterns or correlations, and transform them into features that a model can learn from.

2. [Dimensionality Reduction](#dimensionality-reduction):
   - One of the key techniques in feature extraction is **[Dimensionality Reduction](#dimensionality-reduction)**, which is used to reduce the number of features while still preserving the important information in the data. This involves compressing the data into a smaller set of features that capture most of its variance. By doing this, you improve the efficiency of the machine learning model and make the analysis more interpretable. Allowing to focus on the most important features while <mark>reducing noise</mark> and redundancy.

3. **Visual Feature Extraction**:
   - In the case of complex data like images, techniques such as [Activation atlases](#activation-atlases) can be used to visualize and understand the features that are being extracted and activated within a neural network. These atlases show how different neurons in a neural network respond to specific features or patterns within the data, giving insights into what the model "sees" as important.




# Feature Importance {#feature-importance}



Feature importance refers to <mark>techniques that assign scores to input features</mark> (predictors) in a machine learning model to <mark>indicate their relative impact on the model's predictions.</mark>

Feature importance is typically assessed <mark>after</mark> [Model Building](#model-building). It involves analyzing the trained model to determine the impact of each feature on the predictions.

Feature importance helps in:

- improving model [interpretability](#interpretability), 
- identifying key predictors, 
- and possibly performing [Feature Selection](#feature-selection) to reduce dimensionality, and refining performance

The <mark>outcome</mark> is a ranking or scoring of features based on their importance.

By understanding which features contribute the most to the predictions, you can focus on the most relevant information in your data and potentially reduce model complexity without sacrificing performance.
### Types of Feature Importance Methods

1. Model-Specific Methods:
    - Tree-based models: Models like Random Forests, Gradient Boosted Trees, and Decision Trees have built-in mechanisms for calculating feature importance. They do so based on the decrease in impurity (e.g., [Gini Impurity](#gini-impurity) in classification tasks or variance in regression tasks) or based on the reduction in error when the feature is used for splitting.
    - Linear models: In models like linear regression or logistic regression, feature importance can be derived from the absolute values of the model coefficients, assuming features are standardized.
   
2. Model-Agnostic Methods:
    - Permutation importance: This method measures the importance of a feature by randomly shuffling its values and observing the impact on the model's performance. The larger the decrease in performance, the more important the feature is.
    - [SHapley Additive exPlanations](#shapley-additive-explanations)
    - [Local Interpretable Model-agnostic Explanations](#local-interpretable-model-agnostic-explanations)
### Code snippets for conducting Feature Importance

[SHapley Additive exPlanations](#shapley-additive-explanations)

[Local Interpretable Model-agnostic Explanations](#local-interpretable-model-agnostic-explanations)

Tree-based algorithms like [Random Forests](#random-forests) or [XGBoost](#xgboost) automatically calculate feature importance. 

In Python, for example, after training a Random Forest model, you can access the feature importance scores using:

```python
from sklearn.ensemble import RandomForestClassifier

# Train a RandomForest model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Get feature importance scores
importances = model.feature_importances_
```

This method uses the decrease in node impurity as a measure of feature importance.




# Feature Scaling {#feature-scaling}


Used in preparing data for machine learning models. 

Feature Scaling is a [preprocessing](#preprocessing) step in machine learning that involves adjusting the range and distribution of feature values. 

This ensures that all features contribute equally to the model's performance, especially when they are measured on different scales, which is particularly important for distance-based algorithms, [Principal Component Analysis](#principal-component-analysis), and optimization techniques like [Gradient Descent](#gradient-descent). 

By using methods like normalization and standardization, you can enhance the performance and accuracy of your models.

See sklearn.preprocessing

Examples of algorithms not affected by feature scaling are [Naive Bayes](#naive-bayes), [Decision Tree](#decision-tree), and [Linear Discriminant Analysis](#linear-discriminant-analysis).
### Why Use Feature Scaling?
Feature scaling is important for several reasons:

1. Distance-Based Algorithms: Algorithms like k-nearest neighbors (KNN) rely on distance measures (e.g., Euclidean distance). If features are on different scales, those with larger magnitudes will disproportionately influence the distance calculations. Scaling ensures that all features weigh equally.

2. Principal Component Analysis (PCA): PCA aims to identify the directions (principal components) that maximize variance in the data. If features have high magnitudes, they will dominate the variance calculation, skewing the results. Scaling helps to mitigate this issue.

3. Gradient Descent Optimization: In optimization algorithms like gradient descent, features with larger ranges can cause inefficient convergence. Scaling ensures that all features are on a similar scale, allowing for faster and more stable convergence to the optimal solution.

### Common Scaling Methods
[Normalisation](#normalisation)

[Standardisation](#standardisation)

Min-Max Scaling:  Scales features to a fixed range (e.g., $[0, 1]$), preserving relative distances.
### Example of Scaling
Here’s how you can scale a DataFrame using the `scale` function from `sklearn`:

```python
from sklearn import preprocessing
df_scaled = preprocessing.scale(df)  # Scales each variable (column) with respect to itself
```

This returns an array where each feature is standardized.

**Note:**

- Scaling is done when one feature is at a significantly different scale.
- For each data point, subtract the mean and divide by the range (max-min).

![Pasted image 20241224083928.png](../content/images/Pasted%20image%2020241224083928.png)

# Feature Selection And Creation {#feature-selection-and-creation}

[Feature Selection](#feature-selection)

[Feature Engineering](#feature-engineering)

After the data is ready.

Which features have the best value, which play the biggest role.

Combining features to simplify the

How to select features.
- Correlation between each two (poor)
- Stepwise regression
- Lasso and ridge regression

When selecting features we ask:
- Can we control it/select it?
- Can we control it easily what do we gain from it
- is it a sensible variable?

# Feature Selection Vs Feature Importance {#feature-selection-vs-feature-importance}


### Summary

- [Feature Selection](#feature-selection) is about choosing which features to include in the model <mark>before training</mark>, aiming to improve model performance and efficiency.
- [Feature Importance](#feature-importance) is about understanding the role and impact of each feature <mark>after the model has been trained,</mark> providing insights into the model's decision-making process.

Use for [interpretability](#interpretability) of the model, but they are applied at different stages and serve different purposes.





# Feature Selection {#feature-selection}


Purpose: The primary goal of feature selection is to identify and retain the most relevant features for model training while <mark>removing irrelevant or redundant ones</mark>. This helps in simplifying the model, reducing overfitting, and improving computational efficiency.

Process: Feature selection is typically performed before model training. It involves evaluating features based on certain criteria or algorithms to decide which features to keep or discard.

Through an iterative process, feature selection experiments with different methods, adjusts parameters, and evaluates model performance until an optimal set of features is found.

See [Feature Selection vs Feature Importance](#feature-selection-vs-feature-importance)
### Methods

The choice of feature selection method depends on factors like the size of your dataset, the number of features, and the complexity of your model. It's often a balance between computational cost and performance improvement.

- [Filter Methods](#filter-methods): Select features based on statistical properties, independent of any machine learning algorithm. (Separate stage to training)
- [Wrapper Methods](#wrapper-methods): Involve training multiple models with different subsets of features and selecting the subset that yields the best performance. (Separate stage to training)
- [Embedded Methods](#embedded-methods): Perform feature selection as part of the model training process. (Part of training)

After selecting features, it's essential to evaluate your model's performance ([Model Evaluation](#model-evaluation)) with the chosen subset. Sometimes, feature selection can inadvertently remove important information.

### Detecting Noisy or Redundant Features

- [Correlation](#correlation) Analysis: Use a [Heatmap](#heatmap) or [Clustering](#clustering). Features with low correlation to the target or high correlation with other features may be candidates for removal.

- [Dimensionality Reduction](#dimensionality-reduction) Techniques: Techniques like [Principal Component Analysis](#principal-component-analysis) or Singular Value Decomposition ([SVD](#svd)) can transform the features into a lower-dimensional space while preserving as much variance as possible. Features with low contribution to the principal components can be considered for removal.

- Visualizations: Plotting pairwise scatter plots or [Heatmap](#heatmap) of feature [Correlation](#correlation) can provide visual insights into redundant features. Clusters of highly correlated features or scatter plots showing no discernible pattern with the target variable can indicate noisy or redundant features.
### Investigating Features

1. Variance Thresholding: Check the [Variance](#variance) & [Distributions](#distributions) of each feature. Features with very low variance (close to zero) contribute little information and may be considered noisy. Removing such features can help simplify the model without sacrificing much predictive power.

1. Univariate Feature Selection: Use statistical tests like chi-square for categorical variables or [ANOVA](#anova) for numerical variables to assess the relationship between each feature and the target variable. Features with low test scores or high p-values may be less relevant and can be pruned.

# Feature_Distribution.Py {#feature_distributionpy}



# Feed Forward Neural Network {#feed-forward-neural-network}


A **Feedforward Neural Network (FFNN)** is the simplest type of [Neural network](#neural-network). In this model, connections between neurons do not form a cycle, allowing data to flow in one direction—from the input layer, through the hidden layers, to the output layer—without any loops or backward connections. This straightforward design is primarily used for [supervised learning](#supervised-learning) tasks.

### Structure
- Information flows in one direction: input → hidden layers → output.
- During **[forward propagation](#forward-propagation)**, input data is passed through the network, processed by each layer, and an output is produced.
- Unlike [recurrent neural networks](#recurrent-neural-networks) (RNNs), FFNNs do not share information or weights between layers, meaning the model does not maintain memory of past inputs.

### Learning
- FFNNs learn by adjusting weights and biases during training to minimize the [Loss function](#loss-function).

### Limitations
- **Shallow vs. Deep Networks:** Simple feedforward networks with only a few hidden layers (shallow networks) may struggle to learn complex, hierarchical representations of data. Deeper networks (deep feedforward networks) with many layers can model more complex patterns but require more data and computational resources to train.
- **[Overfitting](#overfitting):** FFNNs can overfit on the training data, especially if they have many parameters and not enough regularization (e.g., dropout, [Ridge|L2](#ridgel2) regularization).
- **No Temporal Understanding:** Unlike [Recurrent Neural Networks](#recurrent-neural-networks) or transformers, FFNNs cannot model sequential dependencies in data. They are better suited for static, non-sequential tasks.

# Feedback Template {#feedback-template}

- Praise: I really appreciate your work on this
    
    - _add here_
        
- FYI: It's really not a big deal, but I'm letting you know just in case.
    
    - _add here_
        
- Suggestion: I’m fairly confident this would help, but I can live without it
    
    - _add here_
        
- Recommendation: This could be holding you back
    
    - _add here_
        
- Plea: It’s almost at the breaking point if it’s not already there.
    
    - _add here_

# Filter Method {#filter-method}



# Filter Methods {#filter-methods}



For [Feature Selection](#feature-selection)

1. **Pearson [Correlation](#correlation) Coefficient**:
   - Measures the linear correlation between two continuous variables.
   - Features with low correlation with the target variable are considered less relevant.
   - Features with high correlation among themselves might be redundant.

2. **Mutual Information**:
   - Measures the amount of information obtained about one variable through another variable.
   - High mutual information indicates strong dependency between features and the target variable.
   - Can handle both continuous and categorical variables.
   - used to rank or score features based on their relevance to the target variable.
   - joint probability distribution
   - information theory,

3. **[ANOVA](#anova) (Analysis of Variance)**:
   - Assesses the differences in means among groups of a categorical variable.
   - Calculates the F-statistic and p-value to determine if there are significant differences in the means of the target variable across different levels of a categorical feature.
   - Useful for selecting features with significant impact on the target variable in classification tasks.

4. **Chi-Squared Test**:
   - Tests the independence between two [categorical](#categorical) variables.
   - Calculates the chi-squared statistic and p-value to determine if the observed frequency distribution differs from the expected distribution.
   - Helpful for [Feature Selection](#feature-selection) in classification tasks with categorical variables.


# Firebase {#firebase}

Googles version of [AWS](#aws)

[Setup basics](https://www.youtube.com/watch?v=XC4Y1KLNLzI&list=WL&index=6)

Project idea: Set up a basic emailer app.

# Fishbone Diagram {#fishbone-diagram}

Fishbone diagram
[Documentation & Meetings](#documentation--meetings)
Root cause analysis: [Documentation & Meetings](#documentation--meetings)
- 5 Y's
- Fishbone diagram: start at issue at head
- ![Pasted image 20250312162034.png](../content/images/Pasted%20image%2020250312162034.png)
- People and ownership: Who is entering the data: the source data

# Fitting Weights And Biases Of A Neural Network {#fitting-weights-and-biases-of-a-neural-network}

For a neural network model, fitting weights and biases involves optimizing these [Model Parameters](#model-parameters) so the model learns to map input features ($X$) to target outputs ($y$) effectively. This is achieved through the training process, which minimizes the error between predictions and actual values.

Best Practices
- Use appropriate weight initializations like He or Xavier.
- Choose a suitable [loss function](#loss-function) for the task.
- Optimize using advanced optimizers like Adam or RMSprop.
- Experiment with batch sizes, epochs, and learning rates.
- Apply regularization (L2, [Dropout](#dropout)) to prevent overfitting.
- Monitor validation metrics and use early stopping.

In [ML_Tools](#ml_tools) see: Neural_Net_Weights_Biases.py

## Initialization of Weights and Biases

Initializing all weights randomly. The weights are assigned randomly by initializing them very close to 0. It gives better accuracy to the model since every neuron performs different computations.

Proper initialization is critical for training to converge efficiently. Poor initialization can lead to slow convergence or getting stuck in local minima. By starting with well-chosen initial values, the network can learn more effectively and avoid issues like vanishing or exploding gradients.

Weights:
- Use small random values (e.g., drawn from Gaussian or uniform [distributions](#distributions)) to break symmetry and ensure that neurons learn different features.
- Initialization techniques like He initialization (for ReLU activations) or Xavier initialization (for sigmoid/tanh activations) are commonly used because they help maintain the scale of gradients across layers, promoting stable and faster convergence.

```python
from tensorflow.keras.layers import Dense
from tensorflow.keras.initializers import HeNormal

# Example of He initialization for ReLU activation
Dense(25, activation="relu", kernel_initializer=HeNormal())
```
Biases:
- Start with zeros (`0`) to ensure symmetry-breaking during optimization. This allows the network to learn offsets for the activations without introducing bias in the initial learning phase.

## [Forward Propagation](#forward-propagation)

During forward propagation, the network computes activations using the current weights and biases, and passes these activations to subsequent layers to generate predictions. This step is crucial as it determines how well the network can map inputs to outputs based on its current parameters.

## Loss Function

The loss function quantifies the difference between predicted outputs and true labels. It serves as the objective function that the network aims to minimize during training. Choosing the right loss function is essential as it directly impacts the learning process and the network's ability to generalize.

- Binary Cross-Entropy: For [Binary Classification](#binary-classification).
- Categorical Cross-Entropy: For multi-class classification.
- Mean Squared Error (MSE): For regression tasks.

Example:
```python
from tensorflow.keras.losses import BinaryCrossentropy
loss_fn = BinaryCrossentropy()
```
## [Backpropagation](#backpropagation)

Backpropagation computes the gradients of the loss function with respect to weights and biases using the chain rule. This process is fundamental for learning, as it provides the necessary information to update the parameters in a way that reduces the loss.

## [Gradient Descent](#gradient-descent) Optimization

Gradients from backpropagation are used to update weights and biases iteratively. Optimization algorithms like Adam, RMSprop, and [Stochastic Gradient Descent|SGD](#stochastic-gradient-descentsgd) with momentum are crucial as they determine the efficiency and speed of convergence, especially in large datasets and complex models.

Example:
```python
from tensorflow.keras.optimizers import Adam
optimizer = Adam(learning_rate=0.001)
```

## Batch Training

Weights and biases are updated after processing a batch of data. Batch training helps in stabilizing the learning process and can lead to faster convergence compared to updating after each sample. The choice of batch size and number of epochs affects the trade-off between computational efficiency and the quality of the learned model.

## [Regularisation](#regularisation) Techniques

Prevent overfitting by penalizing large weights. Regularization is essential for improving the generalization of the model, ensuring it performs well on unseen data.

[Ridge](#ridge)
[Dropout](#dropout)
## Learning Rate Tuning

Learning rate impacts convergence. It is a [hyperparameter](#hyperparameter) that determines the step size during optimization. A poorly chosen learning rate can lead to divergence or slow convergence.

Techniques:
- [Learning Rate](#learning-rate) Scheduling: Reduce learning rate as training progresses to fine-tune the learning process.
- Adaptive Learning Rates: Optimizers [Optimisation techniques](#optimisation-techniques)

#software 

web app framework for writing web pages

uses decorators

![Pasted image 20240922202938.png](../content/images/Pasted%20image%2020240922202938.png)

# [Flask](#flask) {#flaskflask}
## Flask app example

https://www.youtube.com/watch?v=wBCEDCiQh3Q&list=PLcWfeUsAys2my8yUlOa6jEWB1-QbkNSUl

You can run a flask app in google colab and then share it publicly with ngrok.

flask app saved on github.

# Folder Tree Diagram {#folder-tree-diagram}


## Links 

Simple method
https://www.digitalcitizen.life/how-export-directory-tree-folder-windows/

More general

https://superuser.com/questions/272699/how-do-i-draw-a-tree-file-structure

[Treeviz](randelshofer.ch/treeviz/)
[Graphviz](https://graphviz.org/)


tree /a /f >output.doc



# Forecasting_Autoarima.Py {#forecasting_autoarimapy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/Forecasting_AutoArima.py
## Tools and Resources

- AutoARIMA: Automatically selects the best [ARIMA](#arima) [model parameters](#model-parameters). Available in the statsforecast library by [Nixtla](https://www.linkedin.com/company/nixtlainc/).
- Implementation Example: See `TS_AutoArima.py` in [ML_Tools](#ml_tools) for practical implementation.
## Performance Insights

- [Evaluation Metrics](#evaluation-metrics): Marginal differences in evaluation metrics across ARIMA models may occur due to the volatile nature of the data.


# Forecasting_Baseline.Py {#forecasting_baselinepy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/Forecasting_Baseline.py

Baseline methods are essential for establishing a performance benchmark. They provide insights into the data's underlying patterns and help in assessing the effectiveness of more sophisticated forecasting models. By comparing advanced models against these baselines, you can determine if the added complexity is justified by improved accuracy.

**Methods Implemented:**
    - **Mean Forecasting:** Uses the average of all past values as the forecast for future periods.
    - **Naive Forecasting:** The last observed value is used as the forecast for all future periods.
    - **Seasonal Naive Forecasting:** Uses the value from the previous seasonal period to forecast the future.
    - **Drift Method:** Predicts future values based on the trend between the first and last observations in the training data.

# Forecasting_Exponential_Smoothing.Py {#forecasting_exponential_smoothingpy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/TimeSeries/Forecasting_Exponential_Smoothing.py

Exponential smoothing models are a set of [Time Series Forecasting](#time-series-forecasting) techniques that apply weighted averages of past observations, with the weights decaying exponentially over time. These methods are useful for capturing different components of time series data, such as level, trend, and seasonality.

However, their effectiveness depends on the nature of the data. For [Datasets](#datasets) with simple patterns, these models can be quite effective, but for more complex series, alternative methods may be necessary.

## Methods Implemented

Simple Exponential Smoothing (SES):
   - Description: Suitable for forecasting data without trends or seasonality. It applies a constant smoothing factor to past observations.
   - Use Case: Best for stationary series where the data fluctuates around a constant mean.

Double Exponential Smoothing (Holt's Linear Trend):
   - Description: Extends SES by adding a trend component, allowing it to model data with a linear trend.
   - Use Case: Ideal for series with a consistent upward or downward trend but no seasonality.

 Triple Exponential Smoothing (Holt-Winters):
   - Description: Incorporates both trend and seasonal components, making it suitable for data with both linear trends and seasonal patterns.
   - Use Case: Effective for series with regular seasonal fluctuations.

Advanced Alternatives: For complex datasets like stock prices, advanced models such as [Forecasting_AutoArima.py](#forecasting_autoarimapy) may be more appropriate to capture the intricacies of the data.






# Foreign Key {#foreign-key}

A foreign key is a field in one table that uniquely identifies a row in another table, linking to the primary key of that table.

For example, `DepartmentID` in the <mark>Employees</mark> table links to `DepartmentID` in the Departments table. 

Foreign keys establish relationships between tables and maintain referential integrity by ensuring valid connections between records.

**Departments Table**

| DepartmentID | DepartmentName      |
|--------------|----------------------|
| 1            | Human Resources       |
| 2            | IT                   |
| 3            | Marketing            |

**Employees Table**

| EmployeeID | EmployeeName | DepartmentID |
|------------|--------------|---------------|
| 101        | Alice        | 1             |
| 102        | Bob          | 2             |
| 103        | Charlie      | 1             |
| 104        | Dana         | 3             |

# Functional Programming {#functional-programming}



Functional Programming is a style of building functions that threaten computation as a mathematical function that avoids changing state and mutable data. It is a declarative programming paradigm, which means programming expressive and [declarative](term/declarative.md) as opposed to imperative. It's getting more popular with the rise of [Functional Data Engineering](term/functional%20data%20engineering.md).

See also [Programming Languages](programming%20languages.md).

# Fuzzywuzzy {#fuzzywuzzy}

Tool used for correcting spelling with pandas.

[Data Cleansing](#data-cleansing)



# Gaussian Distribution {#gaussian-distribution}


Common assumption for a [Distributions](#distributions).

# Gaussian Mixture Models {#gaussian-mixture-models}



Gaussian Mixture Models (GMMs) represent data as a mixture of multiple Gaussian [distributions](#distributions), with each cluster corresponding to a different Gaussian component. GMMs are more effective than [K-means](#k-means) because they consider the distributions of the data rather than relying solely on distance metrics.

Soft [Clustering](#clustering) technique.

In [ML_Tools](#ml_tools) see: [Gaussian_Mixture_Model_Implementation.py](#gaussian_mixture_model_implementationpy)

[Kmeans vs GMM](#kmeans-vs-gmm)

GMMs can have difference [Covariance Structures](#covariance-structures)
## Key Concepts

- **Gaussian Components**: Each Gaussian distribution is characterized by its mean and [Covariance](#covariance).
- **Likelihood**: The likelihood of a data point belonging to a cluster is given by the formula:
  $$P(X | C_k) = \pi_k \cdot \mathcal{N}(X | \mu_k, \Sigma_k)$$
  where $P(X | C_k)$ is the probability of data point $X$ given cluster $C_k$, $\pi_k$ is the prior probability of cluster $C_k$, and $\mathcal{N}$ is the Gaussian distribution.
- **Expectation-Maximization (EM) Algorithm**: GMMs utilize the EM algorithm to iteratively optimize the parameters of the Gaussian components.

## Advantages of GMMs

- **Complex Data Distributions**: GMMs can capture complex data distributions, unlike [K-means](#k-means), which only considers distance metrics.
- **Probabilistic Framework**: GMMs provide a probabilistic framework for clustering, allowing for soft assignments of data points to clusters.
- **Modeling Elliptical Clusters**: The use of covariance matrices enables GMMs to model elliptical clusters, enhancing clustering performance.

## Applications

- **[Anomaly Detection](#anomaly-detection)**: GMMs are widely used in various applications, including anomaly detection.

## Important Considerations

- **Covariance Types**: The choice of covariance types (full, tied, diagonal, spherical) can significantly impact the performance of GMMs.

## Follow-up Questions

- How do GMMs compare to other clustering algorithms in terms of scalability and computational efficiency?
- What are the implications of choosing different covariance types in GMMs?

![Pasted image 20250126135722.png|500](../content/images/Pasted%20image%2020250126135722.png|500)

# Gaussian Model {#gaussian-model}

### Gaussian Model 

(Univariate)

- **Formula:**  
    $p(x; \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2 \sigma^2}\right)$
- **Steps:**
    - Estimate $\mu$ and $\sigma^2$ from the data.
    - Compute the probability density for each data point.
    - Points with low probabilities (below a threshold $\epsilon$) are considered anomalies.

![Pasted image 20241230202826.png|500](../content/images/Pasted%20image%2020241230202826.png|500)

### **5. Multivariate Gaussian Distribution**

- **Steps:**
    - Extend the Gaussian model to include covariance across features.
    - Fit the multivariate Gaussian model:  
        $p(x; \mu, \Sigma) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu)\right)$
        - $\mu$: Mean vector
        - $\Sigma$: Covariance matrix
    - Threshold low-probability examples to identify anomalies.

# Gaussian_Mixture_Model_Implementation.Py {#gaussian_mixture_model_implementationpy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/Clustering/Gaussian_Mixture_Model_Implementation.py

Follow-Up Questions

- How do GMMs compare to other clustering algorithms in terms of scalability and computational efficiency?
- What are the implications of choosing different covariance types in GMMs?


# General Linear Regression {#general-linear-regression}


[Linear Regression](#linear-regression)

[t-test](#t-test) - to compare means between two populations.

[ANOVA](#anova) - tests 




# Generative Adversarial Networks {#generative-adversarial-networks}


   Composed of two neural networks, a generator, and a discriminator, that compete against each other. GANs are used for tasks like generating realistic images or videos.

# Generative AI From Theory to Practice {#generative-ai-from-theory-to-practice}

### Objective:

 How do LLMs work and operate.  
 Enabling [LLM](#llm)'s at scale:
 Explore recent AI and Generative AI language models 


### Steps

Math on words: Turn words into coordinates.
Statistics on words: Given context what is the probability of whats next.
Vectors on words. Cosine similarity
How train: Use [Markov chain](#markov-chain) for prediction of the next [Tokenisation](#tokenisation)


Tokeniser: map from token to number

1. Pre-training: tokenise input using [NLP](#nlp) techqinues
2. [LLM](#llm) looks at context: nearby tokens, in order to predict

different implmentationg for differnet languages. Differnet tokenisers or translating after.

Journey to scale:

1. Demos, POC (plan to scale): understand limitations
2. Beyond experiments and before production: 
3. Enterprise level: translate terms so they can use governess techniques.

Building:

![Pasted image 20240524130607.png](../content/images/Pasted%20image%2020240524130607.png)

### [Software Development Life Cycle](#software-development-life-cycle)

For GenAI: Building an applicaiton with GenAi features

1. Plan: use case: prompts : archtecture: cloud or on site
2. Build: vector database
3. Test: Quality and responsible ai. 

### [call summarisation](#call-summarisation)

take transcript - > summariser -> summarise

Source: human labeled transcripts to check summariser. 

![Pasted image 20240524131311.png|500](../content/images/Pasted%20image%2020240524131311.png|500)

[Ngrams](#ngrams) analysis - when specific words realy matter


### [RAG](#rag)

Use relvant data to make response better:

![Pasted image 20240524131603.png](../content/images/Pasted%20image%2020240524131603.png)

## [GAN](#gan)

For image models.

Examples: midjourney,stable diffusion,dall-e 3

image model techniques:
- text to image
- image to image
## Notes: 

Use [LLM](#llm)'s to get short info, then cluster.
Going round training data : called a Epochs












# Generative Ai {#generative-ai}



# Get Data {#get-data}


# What is involved:

`df = pd.read_csv('Categorical.csv')`

- Gather relevant data from appropriate sources, addressing any quality or privacy concerns.

```python
## Get textbook data using for example:

import re
def read_file(filename):
    with open(filename, "r", encoding='UTF-8') as file:
        contents = file.read().replace('\n\n',' ').replace('[edit]', '').replace('\ufeff', '').replace('\n', ' ').replace('\u3000', ' ')
    return contents
text = read_file('Data various/Monte_Cristo.txt')

text_start = [m.start() for m in re.finditer('VOLUME ONE', text)]
text_end = [m.start() for m in re.finditer('End of Project Gutenberg', text)]
text = text[text_start[1]:text_end[0]]
```


How would you approach a colleague who is hesitant to share their data?
?
- explain the purpose and benefits
- ensure confidentiality (GDPR) with data masking.
- and finding common ground to address any concerns or objections.
- build trust.
- make agreements of terms of use/ownership/document the data accessing process.

How would you go about obtaining the necessary permissions for a dataset?
?
- establishing clear communication channels within the organsisation.
- obtaining necessary approvals
- emphasizing the value of collaboration.

How would you gather sensitive data?;; Get consent. Ensure anonyminaty (follow regularions)

How to you ensure data is unbiased and representative.
?
- Stratified sampling, (group then randomly sample).
- Examine the data sources.

# Gini Impurity Vs Cross Entropy {#gini-impurity-vs-cross-entropy}

When working with decision trees, both [Gini Impurity](#gini-impurity) and [Cross Entropy](#cross-entropy) are metrics used to evaluate the quality of a split. They help determine how well a feature separates the classes in a dataset.

### Gini Impurity

- **Definition**: Gini impurity measures the probability of incorrectly classifying a randomly chosen element if it was randomly labeled according to the distribution of labels in the node.
- **Computation**: Generally faster to compute than cross-entropy because it does not involve logarithms.
- **Use Case**: Often used in the CART (Classification and Regression Trees) algorithm. It is a good default choice for classification tasks due to its simplicity and efficiency.

### Cross Entropy (More refined than impurity)

- **Definition**: Cross-entropy measures the amount of information needed to encode the class distribution of the node. It quantifies the expected amount of information required to classify a new instance.
- **Computation**: Involves logarithmic calculations, which can be computationally more intensive than Gini impurity.
- **Use Case**: Often used in algorithms like ID3 and C4.5. It can be more informative in cases where the class [Distributions](#distributions) is skewed or when you need a more nuanced measure of impurity.

### Choosing Between Gini Impurity and Cross Entropy

- **Performance**: In practice, both metrics often lead to similar results in terms of the structure and performance of the decision tree. The choice between them may not significantly affect the final model.
- **Efficiency**: If computational efficiency is a concern, Gini impurity might be preferred due to its simpler calculation.
- **Interpretability**: Cross-entropy provides a more information-theoretic perspective, which might be preferred if you are interested in the information gain aspect of the splits.

# Gini Impurity {#gini-impurity}

Gini impurity is a metric used in decision trees to measure the degree or probability of misclassification in a dataset. It is associated with the leaves of a [Decision Tree](#decision-tree) and helps determine the best split at each node.
## Calculation

- Mathematical Formula: Gini impurity is calculated as the probability of incorrectly classifying a randomly chosen element if it were randomly labelled according to the distribution of labels in the subset.
- Formula: 

  $$ \text{Gini Impurity} = 1 - \sum_{i=1}^{n} p_i^2 $$
  where $p_i$ is the probability of an element being classified into a particular class.

## Usage

- Decision Trees: Gini impurity is commonly used in decision trees to evaluate splits. A lower Gini impurity indicates a better split, as it means the data is more homogeneously classified.
- [Classification](#classification) Tasks: It is particularly useful in classification tasks where the goal is to minimize misclassification.

## Relationship to Other Metrics

- Gini impurity is one of several [Regression Metrics](#regression-metrics) used to evaluate the performance of decision trees, alongside others like entropy.

## Example

Suppose you have a dataset with a binary classification problem, where the target variable can be either "Yes" or "No". You have a node in your decision tree with the following distribution of classes:

- 10 samples labeled "Yes"
- 5 samples labeled "No"

### Gini Impurity Calculation

The formula for Gini impurity is:

{% raw %}
$$ \text{Gini impurity} = 1 - \sum (p_i^2) $$
{% endraw %}

where$p_i$ is the proportion of class$i$ in the node.

#### Step-by-Step Calculation

1. **Calculate the proportion of each class**:
   - Total samples = 10 (Yes) + 5 (No) = 15
   - Proportion of "Yes" =$\frac{10}{15} = 0.67$
   - Proportion of "No" =$\frac{5}{15} = 0.33$

2. **Calculate the squared proportions**:
   - $(0.67)^2 = 0.4489$
   - $(0.33)^2 = 0.1089$

3. **Sum the squared proportions**:
   - Sum =$0.4489 + 0.1089 = 0.5578$

4. **Calculate the Gini impurity**:
   - Gini impurity =$1 - 0.5578 = 0.4422$


A Gini impurity of 0.4422 indicates the level of impurity in this node. A Gini impurity of 0 would mean the node is pure (all samples belong to one class), while a higher value indicates more impurity or mixed classes.

This calculation helps in deciding whether to split the node further or not. The goal is to choose splits that <mark>minimize the Gini impurity</mark>, leading to more homogeneous branches.

# Gis {#gis}

Geographic information system.

File formats: 

The Web Map Tile Service (WMTS) and Web Feature Server (WFS) are both specifications used in the field of Geographic Information Systems (GIS) to serve different types of geographic data over the web. The primary differences between them lie in the type of data they serve and how they serve it.

	[Web Map Tile Service (WMTS)](#web-map-tile-service-wmts)
	[Web Feature Server (WFS)](#web-feature-server-wfs)
	[Key Differences of Web Feature Server (WFS) and Web Feature Server (WFS)](#key-differences-of-web-feature-server-wfs-and-web-feature-server-wfs)

[shapefile](#shapefile)

There are free GIS softwares




# Git {#git}



tags:
  - software

Do git bash here.

git status

git add . (adds all)

git status

git commit -m ""

git push

## Notes

https://www.youtube.com/watch?v=xnR0dlOqNVE

[Git Fork vs. Git Clone](https://www.youtube.com/watch?v=6YQxkxw8nhE)

[How to do git commit messages properly](#how-to-do-git-commit-messages-properly)

## Examples


# Git: Common Issues and Fixes

Git can be frustrating, especially when things go wrong. This guide provides practical solutions to common Git mistakes, explained in simple terms.

https://ohshitgit.com/

### how to remove something from a git history, if i forgot to add it to the gitignore, but now have

2. Remove the file from the Git index
This tells Git to stop tracking the file.

git rm --cached path/to/file
For a folder:
git rm -r --cached path/to/folder

3. Commit this change
This saves the removal from the index.

bash
Copy code
git commit -m "Stop tracking path/to/file and add to .gitignore"


## Undoing Mistakes

### I messed up badly! Can I go back in time?

Yes! Use Git’s reflog to find a previous state:

```bash
git reflog
# Find the index of the state before things broke
git reset HEAD@{index}
```

_This is useful for recovering deleted commits, undoing bad merges, or rolling back to a working state._

## Commit Fixes

### I committed but forgot a small change!

```bash
# Make the change
git add .
git commit --amend --no-edit
```

⚠ Warning: Never amend a commit that has already been pushed!

### I need to change the last commit message!

```bash
git commit --amend
```

This will open an editor where you can modify the commit message.



## 🔀 Branching Issues

### I committed to `master` but wanted a new branch!

```bash
# Create a new branch from the current state
git branch new-branch
# Remove the commit from master
git reset HEAD~ --hard
git checkout new-branch
```

⚠ Warning: If you’ve already pushed the commit, additional steps are needed.

### I committed to the wrong branch!

```bash
# Undo the last commit but keep the changes
git reset HEAD~ --soft
git stash
git checkout correct-branch
git stash pop
git add .
git commit -m "Moved commit to correct branch"
```

Alternative:

```bash
git checkout correct-branch
git cherry-pick master  # Moves last commit to correct branch
git checkout master
git reset HEAD~ --hard  # Removes the commit from master
```



## 🔍 Diff and Reset

### I ran `git diff`, but it showed nothing!

If your changes are staged, use:

```bash
git diff --staged
```

This shows differences between the last commit and staged files.

### I need to undo a commit from 5 commits ago!

```bash
git log  # Find the commit hash
git revert [commit-hash]
```

This creates a new commit that undoes the changes.



## 🗑️ Undoing Changes

### I need to undo changes to a file!

```bash
git log  # Find a commit before the changes
git checkout [commit-hash] -- path/to/file
git commit -m "Reverted file to previous version"
```

### I want to reset my repo to match the remote!

⚠ _Destructive action—this cannot be undone!_

```bash
git fetch origin
git checkout master
git reset --hard origin/master
git clean -d --force  # Removes untracked files
```



## 🤯 Last Resort

If everything is completely broken, nuke the repo and reclone:

```bash
cd ..
sudo rm -r repo-folder
git clone https://github.com/user/repo.git
cd repo-folder
```



# Gitlab Ci.Yml {#gitlab-ciyml}

The purpose of a `gitlab-ci.yml` file is to define and configure the **GitLab CI/CD pipeline** for automating tasks such as building, testing, and deploying your code. It is the core configuration file that GitLab uses to orchestrate and execute CI/CD workflows in a repository.

### Key Purposes:

1. **Automation of Workflows:**
    - Automates repetitive tasks like running tests, building applications, linting code, and deploying updates.
      
2. **Pipeline Definition:**
    - Specifies the **stages** (e.g., `build`, `test`, `deploy`) and their sequence.
    - Defines the **jobs** within each stage and their respective commands.
      
3. **Consistency and Reliability:**
    - Ensures consistent execution of tasks across environments, reducing errors caused by manual intervention.
      
4. **Integration with GitLab:**
    - Automatically triggers pipelines in response to events such as code pushes, merge requests, or scheduled runs.
      
5. **Environment Management:**
    - Manages deployments to various environments (e.g., development, staging, production) with variables, conditions, and manual approvals.
      
6. **Feedback and Reporting:**
    - Provides immediate feedback on the status of tasks (e.g., whether tests passed) directly in the GitLab interface.
    - Supports artifact generation and uploads (e.g., logs, reports, or compiled binaries).

### Benefits:

- Improves development velocity by automating workflows.
- Increases code quality through consistent testing and linting.
- Simplifies deployments to various environments.
- Enables team collaboration with clear and visible pipeline progress.

### Example 

```yaml
# Define the stages of the pipeline in the order they will be executed
stages:
  - build    # The stage where the application is built
  - test     # The stage where tests are executed
  - deploy   # The stage where the application is deployed

# Job to build the project
build_job:
  stage: build           # Assign this job to the 'build' stage
  script:                # Commands to execute during this job
    - echo "Building the project" # Example build command (replace with actual build steps)
  artifacts:             # Files or directories to save for use in subsequent jobs
    paths:
      - build/           # Save the 'build' directory as an artifact for later stages

# Job to test the project
test_job:
  stage: test            # Assign this job to the 'test' stage
  script:                # Commands to execute during this job
    - echo "Running tests" # Example test command (replace with actual test steps)

# Job to deploy the project
deploy_job:
  stage: deploy          # Assign this job to the 'deploy' stage
  script:                # Commands to execute during this job
    - echo "Deploying the application" # Example deployment command (replace with actual deployment steps)
  only:                  # Specify when this job should run
    - main               # Only run this job for commits to the 'main' branch

```

# Gitlab {#gitlab}

[GitLab CI CD Tutorial for Beginners Crash Course](https://www.youtube.com/watch?v=qP8kir2GUgo)

  - Provides managed runners to execute [CI-CD](#ci-cd) pipelines.
  - Integrates with version control systems to automate the CI/CD process.




# Google Cloud Platform {#google-cloud-platform}

Google Cloud Platform is a suite of cloud computing services offered by Google. It provides a range of services including computing, storage, and application development that run on Google hardware.

Resources:
 [Introduction to Google Cloud](https://www.youtube.com/watch?v=IeMYQqJeK4)
### Compute Engine

 Description: GCP's Infrastructure as a Service (IaaS) offering, allowing users to run virtual machines on Google's infrastructure.
 
 Features:
   Custom Machine Types: Create VMs with custom configurations.
   Preemptible VMs: Costeffective, shortlived instances for batch jobs and faulttolerant workloads.
   Sustained Use Discounts: Automatic discounts for prolonged usage.
   Persecond Billing: Charges calculated per second for cost savings.
   
 Use Cases: Suitable for web hosting, data processing, and largescale applications.
 Integration: Works seamlessly with other GCP services like Google [Kubernetes](#kubernetes) Engine, Cloud Storage, and [BigQuery](#bigquery).

### Bigtable
 A scalable [NoSQL](#nosql) database service for large analytical and operational workloads.

### App Engine
 A platform for building scalable web applications and mobile backends.

### [BigQuery](#bigquery)
 A fullymanaged, serverless data warehouse for largescale data analytics.

### Cloud Storage
 Object storage service for storing and accessing data on Google's infrastructure.

### Cloud [SQL](#sql)
 Managed relational database service for [MySQL](#mysql), PostgreSQL, and SQL Server.

### CI/CD
 Tools and services for continuous integration and continuous delivery.

### [standardised/Firebase](#standardisedfirebase)
 A platform for building mobile and web applications with realtime databases, authentication, and more.

## Notes:
 Consider setting up a personal GCP example for handson experience.
 Explore the generic repository for additional resources and examples.


# Google My Maps Data Extraction {#google-my-maps-data-extraction}


### Summary:

This guide covers the key workflows and tools for managing and processing location data in Google Sheets and Google My Maps. Suppose we have marks on Google My Maps. In order to extract the location of markers to a google sheet.

1. **Export Marker Data** from Google My Maps as KML/CSV.
2. Convert KML to CSV
3. **Use Apps Script in Google Sheets** to extract data like addresses or postal codes from coordinates.

### **Extract Data from Google My Maps**  
   - **Export Custom Markers:**
     1. Open Google My Maps.
     2. Use the menu (three dots) to select **Export to KML**.
     3. The exported file will contain marker names, descriptions.

### Extract KML data to google sheets
   
- Rename KML file to XML.
- Open XML in excel.
- Extract marker and coordinate data.
- Paste data into google sheets.

### **Extracting Information from Coordinates in Google Sheets**  

Use [**Google Apps Script**](#google-apps-script) to extract additional information like addresses or postal codes from geographic coordinates.

- **Get Address from Coordinates**:
  
```javascript
  function getAddress(lat, lng) {
    var response = Maps.newGeocoder().reverseGeocode(lat, lng);
    var result = response.results[0];
    if (result) {
      return result.formatted_address;
    } else {
      return 'No address found';
    }
  }
  
```
- **Get Postal Code from Coordinates**:
  
```javascript
  function getPostalCode(lat, lng) {
    var response = Maps.newGeocoder().reverseGeocode(lat, lng);
    var result = response.results[0];
    if (result) {
      for (var i = 0; i < result.address_components.length; i++) {
        var component = result.address_components[i];
        if (component.types.indexOf('postal_code') !== -1) {
          return component.long_name;
        }
      }
      return 'Postal code not found';
    } else {
      return 'No results found';
    }
  }
  
```
- Use `getAddress()` with `getPostalCode()`.
- =getPostalCode(56.033139, -3.4182519)




# Gradient Boosting Regressor {#gradient-boosting-regressor}




https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor

[Boosting](#boosting)

The `GradientBoostingRegressor` from the `sklearn.ensemble` module is a model used for regression tasks. It builds an [Model Ensemble](#model-ensemble) of [Decision Tree](#decision-tree) in a sequential manner, where each tree tries to correct the errors made <mark>by the previous ones</mark>. Here’s a breakdown of the key parameters:

1. **loss**: Specifies the loss function to optimize. Default is `'squared_error'`, which is the least-squares loss function. Other options like `'absolute_error'` can be used for robustness against outliers.

2. **learning_rate**: Controls the contribution of each tree to the final prediction. A smaller value (e.g., 0.01) makes the model learn more slowly, but it can lead to better generalization. Default is 0.1.

3. **n_estimators**: The number of boosting stages (i.e., trees). More trees can improve performance but also increase the risk of overfitting. Default is 100.

4. **subsample**: The fraction of samples to be used for fitting each tree. Setting this to a value less than 1.0 can help reduce overfitting, at the cost of a slight increase in bias. Default is 1.0 (use all samples).

5. **criterion**: The function used to measure the quality of a split. `'friedman_mse'` is the default, which is an improved version of mean squared error for decision trees. Other options include `'mse'` and `'mae'`.

6. **max_depth**: The maximum depth of the individual trees. This parameter controls the complexity of each tree. Default is 3, which typically works well for most tasks.

7. **min_samples_split**: The minimum number of samples required to split an internal node. Default is 2, meaning any node can be split as long as there are at least 2 samples.

8. **min_samples_leaf**: The minimum number of samples required to be at a leaf node. This helps control overfitting by requiring more data points at each leaf. Default is 1.

9. **alpha**: The quantile used for the loss function in cases of robust regression. This is useful when dealing with data that includes outliers. Default is 0.9.

10. **validation_fraction**: The fraction of training data to set aside for validation to monitor performance during training. Default is 0.1.

11. **n_iter_no_change**: The number of iterations with no improvement on the validation score to wait before stopping the training early. Default is `None`, meaning no early stopping.

12. **ccp_alpha**: Complexity parameter used for pruning the trees. A larger value leads to more pruning (simplifying the model), which can help prevent overfitting.


# Gradient Boosting {#gradient-boosting}


Gradient Boosting is a technique used for building predictive models [Model Building](#model-building), particularly in tasks like regression and classification. It combines the concepts of [Boosting](#boosting) and [Gradient Descent](#gradient-descent) to create strong models by sequentially combining multiple [Weak Learners](#weak-learners) ([Decision Tree](#decision-tree). 

Key Idea: Instead of fitting a single strong model, Gradient Boosting builds multiple weak learners sequentially. Each new model focuses on <mark>correcting the mistakes made by the previous ones</mark> by fitting to the residuals (differences between observed and predicted values).

Gradient Boosting builds an ensemble of [Weak Learners](#weak-learners) (usually [Decision Tree](#decision-tree)) sequentially. Each new model focuses on the errors of the previous ones, aiming to minimize the residual errors.

Final Prediction: The final prediction is made by aggregating the predictions of all the weak models, usually through a weighted sum.

High Performance: Known for its high performance and efficiency in terms of speed and memory usage.

[Watch Video Explanation](https://www.youtube.com/watch?v=3CC4N4z3GJc)

[Model Ensemble](#model-ensemble)
### Key Components

- [Weak Learners](#weak-learners): Typically decision trees used in the ensemble.
- [Loss Function](#loss-function): Measures how well the model fits the data.
- [learning rate](#learning-rate): Controls the contribution of each weak learner to the final model.

### Examples

- [LightGBM](#lightgbm)
- [XGBoost](#xgboost)
- [CatBoost](#catboost)
### Benefits

- Predictive Accuracy: Often outperforms other [Machine Learning Algorithms](#machine-learning-algorithms).
- Feature Handling: Effectively manages [heterogeneous features](#heterogeneous-features) and automatically selects relevant ones.
- [Overfitting](#overfitting): Less prone to overfitting compared to other complex models.

# Gradient Descent {#gradient-descent}


Gradient descent is an [Optimisation function](#optimisation-function) used to minimize errors in a model by adjusting its parameters iteratively. It works by moving in the direction of the steepest decrease of the [Loss function](#loss-function).

Uses the difference quotient.

The step size is important between derivatives (small then slow) (if large then might miss minimum).

With Stochastic method we can don't need to the entire data set again, we can just add the new information to get improvement.

Gradient descent uses the entire data set.

Used to find the min/max of [Cost Function](#cost-function).

Given any point on the cost function surfaces. Then ask, "In what direction should I go to make the biggest change downhill or up hill, i.e. gradient descent"

![Obsidian_EPIqLAto5w.png|500](../content/images/Obsidian_EPIqLAto5w.png|500)

![Obsidian_FEGflF5RKQ.png|500](../content/images/Obsidian_FEGflF5RKQ.png|500)

How do you implement Gradient descent? You update the (direction) parameter by the small step by the [learning rate](#learning-rate).

![Obsidian_M4mzGSAx7d.png|500](../content/images/Obsidian_M4mzGSAx7d.png|500)

### [Stochastic Gradient Descent](#stochastic-gradient-descent)
Stochastic uses random entries to get derivative instead of the full dataset
Why do we use [Stochastic Gradient Descent](#stochastic-gradient-descent)?;; To find the derivative of discrete data so we can determine a straight line with the Least Square Error (LSE).
What is [Stochastic Gradient Descent](#stochastic-gradient-descent)?;; updates the model parameters based on the gradient of a single randomly chosen data point. 
### [Batch gradient descent](#batch-gradient-descent)
What is [Batch gradient descent](#batch-gradient-descent)?;; computes the gradient of the entire dataset,
### [Mini-batch gradient descent](#mini-batch-gradient-descent)
Stochastic Mini-batched descent is the fastest way (groups then does randomly).
What is [Mini-batch gradient descent](#mini-batch-gradient-descent)?;; Is a compromise of [Batch gradient descent](#batch-gradient-descent) and [Stochastic Gradient Descent](#stochastic-gradient-descent).

**What is the difference between batch gradient descent and stochastic gradient descent?**;; Batch gradient descent computes the gradient of the cost function using the entire training dataset in each iteration, while stochastic gradient descent updates the model's parameters based on the gradient of the cost function with respect to one training example at a time. Mini-batch gradient descent is a compromise, using a subset of the training data in each iteration.
# [Gradient Descent](#gradient-descent)

Gradient descent is commonly used in:
- **Deep Learning**: Frameworks like TensorFlow and PyTorch use variations of gradient descent for training.
- **Custom Implementations**: If you write logistic regression from scratch, gradient descent is a straightforward optimization method.

### **How Gradient Descent Works**

Gradient Descent, a common [Optimisation techniques](#optimisation-techniques), iteratively updates the [Model Parameters](#model-parameters) by computing the gradient of the [loss function](#loss-function) with respect to the parameters. The update formula is:

$\theta = \theta - \alpha \nabla_{\theta} \text{Cost}(\theta)$

Where:
- $\theta$ are the parameters (intercept and coefficients).
- $\alpha$ is the learning rate (step size for updates).
- $\nabla_{\theta} \text{Cost}(\theta)$ is the gradient of the [cost function](#cost-function) with respect to the parameters $\theta$.

#### Process:

1. Calculate the gradient of the loss function.
2. Adjust the parameters in the direction of the negative gradient (to reduce loss).
3. Repeat until either:
    - The loss function converges (minimal change between updates), or
    - The maximum number of iterations is reached.

[Gradient Descent](#gradient-descent)

[Cost Function](#cost-function) value versus number of iterations of [Gradient Descent](#gradient-descent) should decrease

Can use contour plots to show [Gradient Descent#](#gradient-descent) moving towards minima.
![Pasted image 20241224082847.png](../content/images/Pasted%20image%2020241224082847.png)

# Gradio {#gradio}

Gradio is an open-source platform that simplifies the process of <mark>creating user interfaces</mark> for machine learning models. 

It allows users to quickly build interactive demos and applications for their models without extensive front-end development knowledge. 

Main uses:

- **Interactive Interfaces**: Gradio provides a simple way to create web-based interfaces where users can interact with machine learning models by uploading files, entering text, or adjusting sliders.
- **Rapid Prototyping**: It enables quick prototyping and sharing of machine learning models, making it easier to demonstrate model capabilities to stakeholders or gather user feedback.
- **Ease of Integration**: Gradio can be easily integrated with popular machine learning frameworks like TensorFlow, PyTorch, and Hugging Face Transformers, allowing seamless deployment of models.

### Related content

[Video Link](https://www.youtube.com/watch?v=eE7CamOE-PA&list=PLcWfeUsAys2my8yUlOa6jEWB1-QbkNSUl&index=2)
https://www.gradio.app/

[Overview](#overview)

# Grain {#grain}

Grain
   - Definition: The level of detail or [granularity](#granularity) of the data stored in the fact table.
   - Importance: Defining the grain is crucial as it determines what each record in the fact table represents (e.g., individual transactions, daily summaries).

# Grammar Method {#grammar-method}

can understand the Grammar as a method for acceptable sentences.

# Granularity {#granularity}


Definition of Grain in [Dimensional Modelling](#dimensional-modelling)
   - The grain of a [Fact Table](#fact-table) defines what a single row in the table represents. It is the level of detail captured by the fact table.
   - Declaring the grain is essential because it sets the foundation for the entire dimensional model. It determines how detailed the data will be.

Importance of Grain Declaration:
   - The grain must be established before selecting [Dimensions](#dimensions) and [Facts](#facts) because all dimensions and facts must align with the grain.
   - This alignment ensures consistency across the data model, which is critical for the performance and usability of [business intelligence](#business-intelligence) applications.

Balancing Granularity:
   - In the transformation layer, you need to decide the level of aggregation. For instance, you might aggregate hourly data into daily data to save storage space.
   - Adding dimensions increases the number of rows exponentially, so it's important to carefully choose which dimensions to include.

Semantic Layer:
   - A [semantic layer](#semantic-layer) sits on top of transformed data in a data warehouse, providing flexibility and enabling ad-hoc analysis without needing to store every possible data representation.
   - This is akin to [OLAP](#olap) cubes, where you can perform complex queries (slice-and-dice) on large datasets without pre-storing all combinations.

## Choosing the level of granularity

Granularity, or grain, refers to the <mark>level of detail</mark> represented by a single row in a fact table within a data warehouse. 

The choice of granularity depends on the business requirements and the types of analyses you want to support. Finer granularity (e.g., transaction-level) provides more detailed insights but requires more storage and processing power. Coarser granularity (e.g., monthly product-level) reduces storage needs and can improve query performance but may limit the depth of analysis.

By clearly defining the grain, you ensure that all dimensions and facts in the data model are consistent and aligned with the intended analytical use cases.

### Example: Retail Sales Data

Imagine you are designing a data warehouse for a retail company that tracks sales transactions. You need to decide the granularity of the sales fact table. Here are a few possible options:

1. Transaction-Level Granularity:
   - Grain: Each row represents a single sales transaction.
   - Example: A row might include details such as transaction ID, date and time of sale, store location, product sold, quantity, and total sale amount.
   - Use Case: This level of granularity is useful for detailed analysis, such as examining individual customer purchases or identifying specific transaction patterns.

2. Daily Store-Level Granularity:
   - Grain: Each row represents the total sales for a specific store on a specific day.
   - Example: A row might include the store ID, date, total sales amount, and total number of transactions for that day.
   - Use Case: This granularity is suitable for analyzing daily sales trends across different stores, comparing store performance, or identifying peak sales days.

3. Monthly Product-Level Granularity:
   - Grain: Each row represents the total sales for a specific product across all stores for a specific month.
   - Example: A row might include the product ID, month, total sales amount, and total units sold.
   - Use Case: This level is ideal for tracking product performance over time, identifying best-selling products, or planning inventory and supply chain logistics.



# Graph Analysis Plugin {#graph-analysis-plugin}



# Graph Neural Network {#graph-neural-network}


Resources:
- [How Graph Neural Networks Are Transforming Industries](https://www.youtube.com/watch?v=9QH6jnwqrAk&list=PLcWfeUsAys2kC31F4_ED1JXlkdmu6tlrm&index=6)

Use cases:
- [Recommender systems](#recommender-systems) i.e. Uber, Pinterest (PinSage)
- Traffic Prediction - Deepmind in google maps
- Weather forecasting - GraphCast - Deepmind
- Data Mining - Relational Deep Learning
- Material Science - Deepmind - GNome - Density function theory.
- Drug Discovery - MIT - antibiotic activities

# Graph Theory Community {#graph-theory-community}

In graph theory, a community (also known as a cluster or module) is a group of nodes that are more densely connected to each other than to the rest of the network.

#graph_analysis #clustering 

https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.louvain.louvain_communities.html

### Intuition
Communities often represent:
- Functional units in biological networks (e.g., protein complexes)
- Groups of friends or followers in social networks
- Topical clusters in knowledge graphs or citation networks

They capture meso-scale structure—between the local (node/edge) and global (graph-level) scale.

### Formal Definition
There is no single universal definition, but communities typically exhibit:

- High intra-community density: lots of edges within the group
- Low inter-community density: few edges connecting to other groups

Mathematically, a common goal is to maximize modularity, a measure that quantifies the density of links inside communities compared to links between them.

### Community Detection Algorithms
Some widely used algorithms:

| Algorithm | Description |
|-|-|
| Louvain | Fast and widely used; optimizes modularity |
| Girvan–Newman | Based on removing high-betweenness edges |
| Label Propagation | Propagates labels through the network |
| Leiden | Improved version of Louvain for better quality and performance |


# Graph Theory {#graph-theory}


[Graph Theory Community](#graph-theory-community)

[Page Rank](#page-rank)

[PyGraphviz](#pygraphviz)

[networkx](#networkx)

[Plotly](#plotly) for graphs
https://plotly.com/python/network-graphs/

# Graphrag {#graphrag}


[GraphRAG](#graphrag) is a [RAG](#rag) framework that utilizes [Knowledge Graph](#knowledge-graph)s to enhance information retrieval and processing. A significant aspect of this framework is the use of large language models (LLMs) for [Named Entity Recognition](#named-entity-recognition) (NER) within [Neo4j](#neo4j).

[Graph Neural Network](#graph-neural-network)
### Related Terms
- [How to search within a graph](#how-to-search-within-a-graph)
- **[Text2Cypher](#text2cypher)**: This feature allows users to interact with the graph in a user-friendly manner, converting natural language queries into Cypher queries.
- How to move datasets into a graph database.
- Graphrag patterns.
- The role of [interpretability](#interpretability) in understanding graph-based retrieval.

### Implementation

I discovered an insightful LinkedIn post discussing the potential of knowledge graphs:
This specific graph is called a "Lexical Graph with Extracted Entities".
[LinkedIn Post](https://www.linkedin.com/posts/rani-baghezza-69b154b8_thats-why-im-bullish-on-knowledge-graphs-activity-7287474722039033857-BXyN?utm_source=share&utm_medium=member_desktop)
In [ML_Tools](#ml_tools) see: [Wikipedia_API.py](#wikipedia_apipy)
### Resources

- [GraphRAG Site](https://graphrag.com/concepts/intro-to-graphrag/)
- [Neo4j: Building Better GenAI: Your Intro to RAG & Graphs](https://www.youtube.com/watch?v=OuyTENdRcNs)


# Grep {#grep}


![grep.png](../content/images/grep.png)



# Gridseachcv {#gridseachcv}

Used [GridSeachCv](#gridseachcv) to search through the [Hyperparameter](#hyperparameter) space

```python
rf_regressor = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error')
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_

# Model Training with best hyperparameters
rf_regressor = RandomForestRegressor(**best_params, random_state=42)
rf_regressor.fit(X_train, y_train)
```

Given a parameter grid of [Hyperparameter](#hyperparameter), a model, then you model it on the hypers, then gives you the best hypers, that gives the highest cross validation performance.

![Pasted image 20240128194244.png|500](../content/images/Pasted%20image%2020240128194244.png|500)

# Groupby Vs Crosstab {#groupby-vs-crosstab}

In pandas, [Groupby](#groupby) and [Crosstab](#crosstab) serve related but distinct purposes for data <mark>aggregation</mark> and summarization.

- groupby is more flexible for aggregation and transformations,
- whereas `crosstab` is specifically designed for creating frequency tables and exploring the relationship between categorical variables.


In [DE_Tools](#de_tools) see:
- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Transformation/reshaping.ipynb
### Key Differences

1. **Purpose**:
   - `groupby`: Used for performing aggregate functions (sum, mean, count, etc.) on grouped data.
   - `crosstab`: Used for generating frequency tables or contingency tables.

2. **Output**:
   - `groupby`: Returns a DataFrame with aggregated values.
   - `crosstab`: Returns a DataFrame with counts or specified aggregation functions applied across two or more columns.

3. **Usage**:
   - `groupby`: Can be used with multiple aggregation functions and complex groupings.
   - `crosstab`: Typically used for counting occurrences and exploring the relationship between two categorical variables.





# Groupby {#groupby}


Groupby is a versatile method in pandas used to group data based on one or more columns, and then perform aggregate functions on the grouped data. 

Related:
- [Groupby vs Crosstab](#groupby-vs-crosstab)

In [DE_Tools](#de_tools) see:
- https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Investigating/Transformation/group_by.ipynb
### Implementation
```python
# Sample DataFrame
df = pd.DataFrame({'Category': ['A', 'B', 'A', 'B', 'A'],'Values': [10, 20, 30, 40, 50]})
# Group by 'Category' and calculate the sum of 'Values'
grouped = df.groupby('Category').sum()
print(grouped)
```
Output:
```
          Values
Category        
A              90
B              60
```


![Pasted image 20250323081619.png](../content/images/Pasted%20image%2020250323081619.png)


# Grouped Plots {#grouped-plots}

Related:
- [Data Visualisation](#data-visualisation)
- pairplots

```python
import seaborn as sns
import matplotlib.pyplot as plt

# Load example dataset
tips = sns.load_dataset("tips")

# Facet Grid Example
g = sns.FacetGrid(tips, col="sex", row="time")
g.map_dataframe(sns.histplot, x="total_bill", bins=20)

plt.show()
```

![Pasted image 20250402212849.png](../content/images/Pasted%20image%2020250402212849.png)

# Gru {#gru}



# Gsheets {#gsheets}


Useful functions:
- [QUERY GSheets](#query-gsheets)
- ARRAYFORMULA
- Indirect

Accessing google sheets from a script:
https://www.youtube.com/watch?v=zCEJurLGFRk

# Guardrails {#guardrails}


Controlling a [Generative AI](#generative-ai) in business through the use of [Guardrails](#guardrails) ensures that the AI remains aligned with specific business goals and avoids unintended or harmful outputs. Guardrails are essential for maintaining security, compliance, and reliability in AI systems. Here's an outline based on your notes:

### 1. Input Guardrails

   - Prompt Injection Control: [Prompting](#prompting) To prevent users from prompting the AI in ways that could result in harmful or inappropriate responses, filtering or validating inputs can be essential. This reduces the risk of the model being "jailbroken" (i.e., forced to generate outputs outside its intended use case).
   - Topic Restriction: Limit the AI’s inputs to specific business-relevant topics. For instance, if the AI is designed for customer support, it should ignore inputs about unrelated topics (e.g., entertainment or politics).
   - User Authentication: Depending on business needs, certain input guardrails can restrict access to specific features or sensitive information based on user credentials or roles.

### 2. Output Guardrails

   - Content Moderation: Post-processing can be applied to outputs to ensure they align with business values, compliance regulations, or safety standards. For example, any harmful or offensive language can be filtered out.
   - Pre-defined Boundaries: Limit the AI’s responses to fall within specific domains. For instance, when the AI is asked questions outside its scope, it can respond with a predefined message, such as "I am not programmed to handle that topic."
   - Compliance and Ethical Constraints: Outputs can be regulated to ensure the model adheres to legal, ethical, and regulatory constraints, which is especially important in industries like finance or healthcare.

### 3. Jailbreaking Concerns

   - Jailbreaking occurs when a user manipulates the system to bypass these guardrails, leading to undesirable outputs. This depends on the business context—some may tolerate more flexible AI behavior, while others, like legal or healthcare firms, need strict controls.

### 4. Business-Specific Use Cases

   - Tailor the AI to address specific business needs. For example, a generative AI for a legal firm should stick to legal advice and documentation, whereas a customer service chatbot should handle predefined topics like returns and product support.
   - [Data Observability|monitoring](#data-observabilitymonitoring) / Monitoring and Logging: Keep track of input and output interactions to ensure that the AI’s performance remains within its intended boundaries.

# Hadoop {#hadoop}


   Hadoop provides the backbone for distributed storage and computation. It uses HDFS (Hadoop Distributed File System) to split large datasets across clusters of servers, while MapReduce enables parallel processing. It’s well-suited for [Batch Processing](#batch-processing)asks, though newer tools like [Apache Spark|Spark](#apache-sparkspark) often outperform Hadoop in terms of speed and ease of use.

1. **Architecture**:
   - **Open-Source Framework**: Hadoop is an open-source framework for distributed storage and processing of large datasets using clusters of commodity hardware.
   - **Distributed File System**: The Hadoop Distributed File System (HDFS) stores data across multiple machines, providing high throughput access to data.
   - **MapReduce**: Originally designed for [Batch Processing](#batch-processing) using the MapReduce programming model, though newer frameworks like Apache Spark are often used now.

2. **Data Storage**:
   - **Unstructured, Semi-Structured, and Structured Data**: Hadoop can handle a wide variety of data formats, including unstructured, semi-structured, and structured data.
   - **Scalable Storage**: HDFS can store vast amounts of data by adding more nodes to the cluster.

3. **Management**:
   - **Complex Management**: Requires more administrative effort to manage and maintain the infrastructure, including handling failures, load balancing, and tuning.

4. **Performance**:
   - **[Batch Processing](#batch-processing)**: Hadoop is optimized for batch processing of large datasets, though it can be less efficient for real-time processing compared to other systems.
   - **Latency**: Higher latency for query processing compared to Snowflake, particularly for complex analytical queries.

5. **Use Cases**:
   - **[Big Data](#big-data) Processing**: Ideal for large-scale data processing tasks, including ETL (Extract, Transform, Load), data mining, and large-scale machine learning.
   - **[Data Lake](#data-lake)**: Commonly used as a data lake to store vast amounts of raw data.



# Handling Different Distributions {#handling-different-distributions}

Handling different [distributions](#distributions) is needed for developing robust, fair, and accurate machine learning models that can adapt to a wide range of data environments.

## Importance of Handling Different Distributions

1. [Model Robustness](#model-robustness): Ensures models generalize well to new, unseen data.
2. Bias Mitigation: Prevents bias in predictions by accommodating diverse data types.
3. Improved [Accuracy](#accuracy): Fine-tunes models for better accuracy across varied [Datasets](#datasets).
4. Maintains model effectiveness across different data sources.
5. Decision Making: Informs [Preprocessing](#preprocessing), [model selection](#model-selection), and evaluation strategies.

## Resources

Video: [Training and Testing on Different Distributions](https://www.youtube.com/watch?v=sfk5h0yC67o&list=PLkDaE6sCZn6E7jZ9sN_xHwSHOdjUxUW_b&index=16)

## Example Scenario

High-resolution photos (many) vs. amateur photos (small number) exhibit different distributions.

## Strategy for Handling Distributions

Code Example: See `Handling_Different_Distributions.py` in [ML_Tools](#ml_tools)

In this script:
- **Data Generation:** Creates two mock datasets with different distributions.
- **Data Splitting:** Combines and splits the data into train, dev, and test sets.
- **Model Tuning:** Uses `GridSearchCV` to find the best hyperparameters for a RandomForest model.
- **Model Training and Evaluation:** Trains the model on the training set and evaluates it on the dev and test sets.
- **Visualization:** Uses `matplotlib` to plot the distribution of a feature from both datasets and the model's accuracy on the dev and test sets.

### Follow up questions

How best to combine the datasets?
How should we shuffle and split based on the distributions?
How do we pick the dev set?

1. **Combining Datasets:**
    - The script combines two datasets (`dataset1` and `dataset2`) that may have different distributions. This step ensures that the model is exposed to a variety of data during training.
    
1. **Random Shuffling and Splitting:**
    - By shuffling and splitting the combined dataset into train, dev, and test sets, the script ensures that each set contains a mix of data from both distributions. This helps the model learn from the diversity in the data.

1. **Model Tuning with Diverse Data:**
    - The model tuning process uses the dev set, which contains data from both distributions. This helps in finding hyperparameters that work well across different data characteristics.

## Related Topics
- [Preprocessing](#preprocessing)

# Handling_Missing_Data.Ipynb {#handling_missing_dataipynb}

https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Investigating/Cleaning/Handling_Missing_Data.ipynb

# Handling_Missing_Data_Basic.Ipynb {#handling_missing_data_basicipynb}

https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/Investigating/Cleaning/Handling_Missing_Data_Basic.ipynb

# Handwritten Digit Classification {#handwritten-digit-classification}

![Pasted image 20241006124356.png|800](../content/images/Pasted%20image%2020241006124356.png|800)

# Hash {#hash}

A hash is a fixed-size string of characters that is generated from input data of any size using a hash function. 

Hashes are used to ensure [data integrity](#data-integrity) by providing a unique representation of the data, making it easy to detect any changes or alterations.

### Key Characteristics of Hashes:

1. **Deterministic**: The same input will always produce the same hash output.
2. **Fixed Size**: Regardless of the size of the input data, the output hash will always be of a fixed length (e.g., SHA-256 produces a 256-bit hash).
3. **Fast Computation**: Hash functions are designed to compute the hash value quickly.
4. **Pre-image Resistance**: It should be computationally infeasible to reverse-engineer the original input from its hash.
5. **Collision Resistance**: It should be difficult to find two different inputs that produce the same hash output.

### How Hashes are Used in Data Integrity:

6. **Data Verification**: When data is stored or transmitted, a hash of the data is generated and stored or sent along with it. When the data is later accessed, the hash is recalculated and compared to the original hash. If they match, the data is considered intact; if not, it indicates potential corruption or tampering.

7. **Digital Signatures**: Hashes are often used in digital signatures to ensure the authenticity and integrity of a message or document.

8. **Password Storage**: Instead of storing passwords in plain text, systems often store the hash of the password. When a user logs in, the system hashes the entered password and compares it to the stored hash.

### Example of Hashing:

For example, if we take the string "Hello, World!" and apply a hash function like SHA-256, it will produce a unique hash value:

- Input: "Hello, World!"
- Hash (SHA-256): `a591a6d40bf420404a011733cfb7b190d62c65bf0bcda190f4b6c3f0f3c3b8a`

If the input data changes even slightly (e.g., "Hello, World"), the hash will be completely different, making it easy to detect any alterations.



# Heatmap {#heatmap}


### Description

A **heatmap** is a two-dimensional graphical representation of data where individual values are represented by colors. It is particularly useful for visualizing numerical data organized in a table-like format. 

A heatmap is a graphical representation of data where individual values are represented by colors. It is useful for visualizing numerical data and analyzing the correlation between features.

A heatmap is a  visualization tool for analyzing the [Correlation](#correlation) between features in a dataset. In the context of correlation analysis, a heatmap can display the correlation coefficients between different features in a dataset.

By using a heatmap, you can easily identify [Multicollinearity](#multicollinearity) and make informed decisions about which features to retain or remove, ultimately enhancing the performance and interpretability of your machine learning models.
### Correlation Coefficients
The correlation coefficients range from -1 to 1:
- **-1**: Indicates a perfect negative correlation; if one attribute is present, the other is almost certainly absent.
- **0**: Indicates no correlation; there is no dependence between the attributes.
- **1**: Indicates a perfect positive correlation; if one attribute is present, the other is also certainly present.
### Implementation in Python

In [ML_Tools](#ml_tools) see: [Heatmaps_Dendrograms.py](#heatmaps_dendrogramspy)



# Heatmaps_Dendrograms.Py {#heatmaps_dendrogramspy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations\Preprocess\Correlation\Heatmaps_Dendrograms.py

See:
 - [Heatmap](#heatmap)
 - [Dendrograms](#dendrograms)


# Heterogeneous Features {#heterogeneous-features}


## Description

In machine learning, heterogeneous features refer to a situation where the input data contains a variety of different types of features. Let's break it down:

### 1. **Features:**
   - Features are the individual measurable properties or characteristics of the data used for making predictions in a machine learning model.
   - For example, in a dataset about houses, features could include the number of bedrooms, square footage, location, and whether it has a garden.

### 2. **Homogeneous vs. Heterogeneous:**
   - **Homogeneous Features:** In some datasets, all features are of the same type, such as numerical or categorical. For instance, a dataset containing only numerical features like age, income, and temperature is homogeneous.
   - **Heterogeneous Features:** In contrast, heterogeneous features refer to datasets where features are of different types. This means the dataset may contain a mix of numerical, categorical, text, image, or other types of data.

### 3. **Examples of Heterogeneous Features:**
   - **Numerical Features:** Represented by continuous values like age, income, or temperature.
   - **Categorical Features:** Represented by discrete values such as gender, city, or type of car.
   - **Text Features:** Textual data like product descriptions, customer reviews, or email content.
   - **Image Features:** Visual data represented by pixels in an image, used in tasks like image recognition or object detection.

### 4. **Challenges and Considerations:**
   - Handling heterogeneous features requires specialized techniques in [Preprocessing](#preprocessing) and model building.
   - Different types of features may need different preprocessing steps, such as encoding categorical variables, scaling numerical features, or extracting features from text or images.
   - Models need to be capable of handling diverse data types, either through feature engineering or using algorithms specifically designed for heterogeneous data.

### 5. **Applications:**
   - Heterogeneous features are common in many real-world applications, such as e-commerce (combining text descriptions with numerical features), healthcare (integrating medical records with images or text), and social media analysis (analyzing text, images, and user profiles).

### 6. **Resources for Further Learning:**
   - Feature Engineering for Machine Learning: [https://www.datacamp.com/community/tutorials/feature-engineering-kaggle](https://www.datacamp.com/community/tutorials/feature-engineering-kaggle)
   - Handling Text Data in Machine Learning: [https://towardsdatascience.com/handling-text-data-in-machine-learning-projects-b52bbc9531d7](https://towardsdatascience.com/handling-text-data-in-machine-learning-projects-b52bbc9531d7)
   - Image Feature Extraction Techniques: [https://towardsdatascience.com/image-feature-extraction-techniques-91e8625616f1](https://towardsdatascience.com/image-feature-extraction-techniques-91e8625616f1)

Understanding how to work with heterogeneous features is essential for building effective machine learning models that can handle diverse types of data and extract meaningful insights from them.
