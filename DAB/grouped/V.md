# V

## Table of Contents
* [Vacuum](#vacuum)
* [Variance](#variance)
* [Vector Database](#)
* [Vector Embedding](#vector-embedding)
* [Vector_Embedding.py](#vector_embeddingpy)
* [Vectorisation](#vectorisation)
* [Vectorized Engine](#)
* [Vercel](#vercel)
* [View Use Case](#)
* [Views](#views)
* [Violin plot](#violin-plot)
* [Virtual environments](#virtual-environments)
* [vanishing and exploding gradients problem](#vanishing-and-exploding-gradients-problem)



# Vacuum {#vacuum}



# Variance {#variance}

Variance in a dataset is a statistical measure that represents the degree of spread or dispersion of the data points around the mean (average) of the dataset. 

It quantifies how much the individual data points differ from the mean value. 

A higher variance indicates that the data points are more spread out from the mean, while a lower variance indicates that they are closer to the mean. 

Variance is calculated as the average of the squared differences between each data point and the mean.

See also:
[Boxplot](#boxplot)
[Distributions](#distributions)

**Variance**:
- Measures how much a single variable deviates from its mean.
- For variable $X$, variance is: 
$$
\text{Var}(X) = \frac{1}{n}\sum_{i=1}^n (x_i - \mu_X)^2
$$
- Variance determines the **spread** of data along a particular dimension.

## Overview

Vector databases are specialized systems designed to handle and manage [Vector Embedding](#vector-embedding). 

As most real-world data is unstructured, such as text, images, and audio, vector databases play a  role in organizing and querying this data effectively.

Why use them:
- data is [unstructured data](#unstructured-data) i.e image
- 

## Key Features

- **Vector Embeddings**: At the core, vector databases store embeddings generated by machine learning models. These embeddings transform complex data into fixed-size vectors that encapsulate semantic information.
- **Similarity Search**: By leveraging the geometric properties of vector spaces, vector databases can quickly identify similar items. This is achieved by measuring distances (e.g., cosine similarity, Euclidean distance) between vectors.
- **Indexing Methods**: Various indexing techniques, such as HNSW (Hierarchical Navigable Small World) graphs, IVF (Inverted File), and PQ (Product Quantization), are employed to optimize search speed and accuracy. Allows faster searching.

## Querying Vectors

To query vectors, users typically specify a target vector and a similarity metric. 

The database then retrieves vectors that are closest to the target, based on the chosen metric. This process is crucial for applications like recommendation systems, where finding similar items is essential.

## Use Cases
1. **Long-term Memory for [LLM](#llm)**: Vector databases can store vast amounts of contextual information, enhancing the memory and retrieval capabilities of large language models (LLMs). Implemented using [Langchain](#langchain).
2. Rank and Recommendation system using nearest neighbours.
3. **Semantic Search**: Unlike traditional keyword-based search, semantic search understands the context and meaning, providing more relevant results. This is particularly useful in natural language processing (NLP) applications.
4. **Similarity Search**: Beyond text, vector databases support similarity searches for multimedia data, enabling applications in image recognition, audio analysis, and video retrieval.

## Options
Several vector database solutions are available, each with unique features and optimizations:
- **Pincone**: Known for its scalability and ease of integration with machine learning workflows.
- **Weaviate**: Offers a semantic graph database with built-in vector search capabilities.
- **Chroma**: Focuses on simplicity and performance for embedding-based applications.
- **Redis**: Provides vector search capabilities through its modules, suitable for real-time applications.
- **Qdrant**: Designed for high-performance vector search with a focus on scalability.
- **Milvus**: An open-source solution optimized for handling large-scale vector data.
- **Vespa**: Combines vector search with traditional search capabilities, ideal for complex applications.

## Related Concepts
- **[standardised/Vector Embedding](#standardisedvector-embedding)**: The process of converting data into vector form, capturing its semantic essence.
- **[standardised/Vector Embedding](#standardisedvector-embedding)**: A specific type of vector embedding used in NLP to represent words in a continuous vector space.
- **[Semantic Relationships](#semantic-relationships)**: A search technique that leverages the meaning and context of queries and data to deliver more relevant results.
- [Cosine Similarity](#cosine-similarity)
### Resources

[Vector Databases simply explained! (Embeddings & Indexes)](https://www.youtube.com/watch?v=dN0lsF2cvm4&list=PLcWfeUsAys2kC31F4_ED1JXlkdmu6tlrm)



# Vector Embedding {#vector-embedding}


Vector Embedding is a technique used in machine learning and [NLP](#nlp) to represent data in a continuous vector space. This representation captures the [Semantic Relationships](#semantic-relationships) of data, such as words or sentences, allowing similar items to be positioned close to each other in the vector space.

### Key Concepts

- Data Compression: Embeddings compress data into a lower-dimensional space, making it easier to process and analyze. This is particularly useful for high-dimensional data like text or images.
  
- Semantic Similarity: In the embedding space, similar items are positioned close to each other. This proximity reflects semantic similarity, meaning that items with similar meanings or characteristics have similar vector representations.

1. [Dimensionality Reduction](#dimensionality-reduction): Words are represented in a lower-dimensional space compared to traditional methods like one-hot encoding, resulting in more efficient computations.

2. [Semantic Relationships](#semantic-relationships): Words with similar meanings or contexts are located close to each other in the vector space. For example, "king" and "queen" might be closer to each other than "king" and "apple."

![Pasted image 20241015211934.png](../content/images/Pasted%20image%2020241015211934.png)

4. Contextual Understanding: (Vector) Word embeddings capture the context in which words appear, allowing models to understand nuances and relationships in language.

Popular methods for generating vector (word) embeddings include:
- [Word2vec](#word2vec),
- GloVe, 
- FastText.
- [spaCy](#spacy)

### Types of Similarity Measures

- Euclidean Distance
- [Cosine Similarity](#cosine-similarity)

### Applications

- [Language Models](#language-models): Vector embeddings are widely used in language models to represent words, phrases, or sentences, enabling models to understand and generate human language more effectively.
- [Attention mechanism](#attention-mechanism): Embeddings are often used with attention mechanisms to enhance model performance in tasks like translation, summarization, and question answering.

### Example Use Cases

- Word Embeddings: Techniques like Word2Vec and GloVe create word embeddings that capture semantic relationships between words, enabling tasks like word similarity and analogy solving.
- Sentence Embeddings: Models like [BERT](#bert) and Sentence Transformers generate embeddings for entire sentences, facilitating tasks like sentiment analysis and semantic search.

### Visualizations

- [t-SNE](#t-sne): A technique for visualizing high-dimensional data, often used to display word embeddings in a two-dimensional space.




![Pasted image 20241015211844.png](../content/images/Pasted%20image%2020241015211844.png)


## Implementation

How to do vector embeddings in [PyTorch](#pytorch) that show [Semantic Relationships](#semantic-relationships) between terms.

In [ML_Tools](#ml_tools) see: [Vector_Embedding.py](#vector_embeddingpy)

## Related Terms

[How to search within a graph](#how-to-search-within-a-graph)
[How would you decide between using TF-IDF and Word2Vec for text vectorization](#how-would-you-decide-between-using-tf-idf-and-word2vec-for-text-vectorization)
[embeddings for OOV words](#embeddings-for-oov-words)



# Vector_Embedding.Py {#vector_embeddingpy}

https://github.com/rhyslwells/ML_Tools/blob/main/Explorations/Build/NLP/Vector_Embedding.py

### Explanation of the Script

1. **Vocabulary and Embedding Layer**:
    - Terms are mapped to indices using a dictionary.
    - The embedding layer learns continuous vector representations for these terms.
      
2. **Cosine Similarity**:
    - The cosine similarity function measures how similar two terms are in the embedding space. Higher values indicate closer relationships.
      
3. **Visualization**:
    - Embeddings are plotted in a 2D space to show semantic relationships. Terms with similar meanings (e.g., "king" and "queen") are expected to cluster together.
      
4. **t-SNE for Dimensionality Reduction**:
    - If the embedding dimension is higher than 2, t-SNE can reduce it to 2D for visualization while preserving semantic relationships.

### Outputs

1. **Cosine Similarities**:
    - Pairwise similarity scores between terms to quantify their semantic closeness.
      
2. **Visualization**:
    - A scatter plot showing the positions of terms in the embedding space.

# Vectorisation {#vectorisation}




[Link](https://www.youtube.com/watch?v=uvTL1N02f04&list=PLkDaE6sCZn6FNC6YRfRQc_FbeQrF8BwGI&index=24)

![Pasted image 20241217204829.png|500](../content/images/Pasted%20image%2020241217204829.png|500)
Numpy dot is better than for loop and summing.
Why does it run faster?
A: Designed to run in parallel 

Sequentially versus simultaneously in parallel.

Related concepts:
- [Numpy](#numpy)
- [gpu](#gpu)

## Vectorized Engine

A modern database query execution engine designed to optimize data processing by leveraging vectorized operations and SIMD (Single Instruction, Multiple Data) capabilities of modern CPUs. Vectorized engines, such as  [DuckDB](#duckdb), process data in large blocks or batches using SIMD instructions, allowing for improved parallelism, cache locality, and reduced overhead compared to traditional row-at-a-time processing engines, using [Columnar Storage](#columnar-storage).

# Vercel {#vercel}




## View Use Case

### Scenario

A company wants to generate monthly performance reports for its employees. The performance data is spread across multiple tables, including `employees`, `departments`, and `performance_reviews`. Instead of writing complex queries every time a report is needed, the company can create a view that simplifies data retrieval.

### Step 1: Define the Tables

Assume we have the following tables:

- **employees**: Contains employee details.
  - `employee_id`
  - `name`
  - `department_id`

- **departments**: Contains department details.
  - `department_id`
  - `department_name`

- **performance_reviews**: Contains performance review data.
  - `review_id`
  - `employee_id`
  - `review_score`
  - `review_date`

### Step 2: Create a View

To simplify the reporting process, we create a view that joins these tables and aggregates the performance scores:

```sql
CREATE VIEW employee_performance AS
SELECT 
    e.employee_id,
    e.name,
    d.department_name,
    AVG(pr.review_score) AS average_score
FROM 
    employees e
JOIN 
    departments d ON e.department_id = d.department_id
JOIN 
    performance_reviews pr ON e.employee_id = pr.employee_id
GROUP BY 
    e.employee_id, e.name, d.department_name;
```

### Step 3: Query the View

Now, whenever the HR department needs to generate a performance report, they can simply query the `employee_performance` view:

```sql
SELECT * FROM employee_performance WHERE average_score >= 4.0;
```

This query retrieves all employees with an average performance score of 4.0 or higher, making it easy to identify top performers.

### Benefits of Using Views in This Use Case

1. **Simplification**: The view encapsulates complex joins and aggregations, allowing HR to retrieve performance data without needing to understand the underlying table structure.

2. **Reusability**: The view can be reused for different reports, such as quarterly reviews or department-specific performance assessments.

3. **Maintainability**: If the logic for calculating performance scores changes, the HR team only needs to update the view definition, not every individual query.

4. **Data Consistency**: All reports generated from the view will be consistent, as they rely on the same underlying logic for calculating average scores.

5. **Security**: If sensitive employee data needs to be protected, the view can be designed to exclude certain columns, ensuring that only necessary information is accessible.

# Views {#views}


Views are virtual tables defined by SQL [Querying|Query](#queryingquery) that <mark>simplify complex data representation.</mark> They can remove unnecessary columns, aggregate results, partition data, and secure sensitive information.


In [DE_Tools](#de_tools) see:
https://github.com/rhyslwells/DE_Tools/blob/main/Explorations/SQLite/Viewing/Viewing.ipynb

Basic Usage:
- Simplification
- Aggregation (using GROUP)
- [Common Table Expression](#common-table-expression)
- Securing data: can give all values in a field the same value.

Advanced Usage
- Temporary Views: Exist only for the <mark>duration of the database connection.</mark>
- [Common Table Expression](#common-table-expression): Serve as temporary views for a single query.
- [Soft Deletion](#soft-deletion): Use views and triggers to mark records as deleted without physically removing them from the table.

Related topics:
- [View Use Case](#view-use-case)

## Why Use Views?

1. **Simplification and Abstraction**:
   - Views encapsulate complex queries, allowing users to interact with data without needing to understand the underlying structure. This simplifies data retrieval by hiding complexity.

2. **Security**:
   - Views restrict access to specific data by granting users access to views instead of underlying tables, which can help protect sensitive information. Note: Access controls may vary by database system (e.g., not available in [SQLite](#sqlite)).

3. **Reusability and Maintainability**:
   - Define complex queries once in a view and reuse them across multiple applications, simplifying maintenance when logic changes.

4. **Data Consistency and Integrity**:
   - Ensure consistent data presentation across applications and users by encapsulating business logic for uniform calculations.

5. **Performance Optimization**:
   - While regular views do not inherently improve performance, materialized views can enhance performance by storing precomputed results.

6. **Logical Data Independence**:
   - Provide a layer of abstraction between physical data storage and access methods, allowing [Database Schema|schema](#database-schemaschema) changes without affecting view users.

7. **Aggregation and Partitioning**:
   - Views can be used to calculate and store aggregated results (e.g., average ratings) and organize data by specific criteria (e.g., years or categories).

# Violin Plot {#violin-plot}


An extension of a [Boxplot](#boxplot) showing the data distribution. Useful when comparing distributions, skewness.

```python
data = [...]  # Your data
sns.violinplot(data=data, color="purple", fill="lightblue", scale="area")
plt.show()
```



# Virtual Environments {#virtual-environments}


[Setting up virtual env](https://www.youtube.com/watch?v=yG9kmBQAtW4)

For windows (need to not be in a venv before del)
```cmd
rmdir /s /q venv
python -m venv venv
venv\Scripts\activate
```

pip freeze > requirements.txt

.gitignore 
https://www.youtube.com/watch?v=_vejzukmn4s

Remember to set python interpreter

Related terms:
- [Poetry](#poetry)

# Vanishing And Exploding Gradients Problem {#vanishing-and-exploding-gradients-problem}


[Recurrent Neural Networks|RNN](#recurrent-neural-networksrnn)



[vanishing and exploding gradients problem](#vanishing-and-exploding-gradients-problem)
In standard RNNs, the difficulty lies in retaining useful information over long sequences due to the exponential decrease in the gradient values, which results in poor learning of long-term dependencies.